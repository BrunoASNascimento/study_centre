# Introduction to Data Analytics

- **Data Generation**: In our modern world, data generation is increasing rapidly due to various activities like social media posts, financial transactions, and interactions with technology.
- **Data as Insights**: The data generated reflects our behavior, habits, and preferences, providing valuable insights into our world.
- **Exponential Growth**: The amount of data is growing exponentially, thanks to advancements in technology and connected devices such as phones, smartwatches, car computers, and smart home systems.

**Importance of Data**

- **Decision Making**: Proper use of data can help make better decisions.
- **Need for Data Analysts**: There is a high demand for data analysts to make sense of the data and derive insights.

**Applications of Data Analytics**

- **Broad Applications**: Data analytics is essential for business owners, marketers, and individuals wanting to understand the world.
- **Simple to Complex Data**: Data can range from simple handwritten records to complex terabytes of data collected through the Internet and advanced machines.
- **Insights from Data**: When structured and analyzed correctly, even simple data can provide meaningful insights for better decision-making.

**Role of Data Analysts**

- **Data Collection**: Collecting relevant data.
- **Data Cleaning and Organizing**: Cleaning and organizing the data.
- **Data Analysis and Visualization**: Analyzing and visualizing data to uncover insights and inform decisions.

**Benefits of Understanding Data Analytics**

- **Informed Decisions**: Ability to make informed decisions.
- **Pattern Recognition**: Identifying patterns and trends.
- **Clear Communication**: Communicating data clearly and concisely.

**What is Data Analytics?**

- **Definition**: Data analytics is the process of collecting, cleaning, organizing, analyzing, and interpreting data to uncover insights and make informed decisions.

**Components of Data Analytics**

1. **Collecting Data**: Gathering data from various sources such as databases, spreadsheets, and online platforms. Data can come in different formats and structures.
2. **Cleaning Data**: Removing duplicates, inconsistencies, or errors to ensure data accuracy.
3. **Organizing Data**: Sorting and categorizing data into meaningful groups to identify trends and patterns.
4. **Analyzing Data**: Using statistical and mathematical methods to uncover insights and relationships within the data. This includes creating charts, graphs, and tables.
5. **Interpreting Data**: Presenting data in an understandable way through visualization techniques like charts, graphs, and dashboards. Storytelling is key to conveying the data's story and making decisions.

**Example: Anna's Clothing Boutique**

- **Collecting Data**: Anna collects sales data from her store and online platform using software that captures sale price, location, product details, and more.
- **Cleaning Data**: She finds inconsistencies in size information and standardizes sizes to ensure consistency.
- **Organizing Data**: Anna sorts and categorizes data to identify trends and patterns, such as which brands are most profitable.
- **Analyzing Data**: Using software with built-in statistical models, Anna analyzes sales trends to predict future demand.
- **Interpreting Data**: Anna creates charts and summaries to communicate her insights to her associates, helping them make informed decisions for the next season's inventory.

**Importance of Data Analytics**

- **Informed Decisions**: Enables better decision-making based on data.
- **Pattern Recognition**: Helps identify trends and patterns in data.
- **Clear Communication**: Facilitates the clear and concise presentation of data insights.

**Role of a Data Analyst**

- **Definition**: A data analyst is a professional who uses data analytics to uncover insights and make informed decisions.
- **Responsibilities**: Collecting, cleaning, organizing, analyzing, and visualizing data to provide valuable insights for organizations.

**Importance of Data Analysts**

- **Data-Driven Decisions**: Data analysts help organizations make data-driven decisions, impacting the bottom line.
- **Growing Demand**: The increasing availability and value of data make the role of data analysts more critical and in high demand.

**Typical Tasks of a Data Analyst**

1. **Collecting Data**: Gathering data from various sources such as databases, spreadsheets, and online platforms.
2. **Cleaning Data**: Removing duplicates, inconsistencies, and errors (data scrubbing).
3. **Organizing Data**: Sorting data into meaningful groups to identify trends and patterns.
4. **Analyzing Data**: Using statistical and mathematical methods to uncover insights and relationships within the data.
5. **Interpreting Data**: Visualizing data and telling a story based on the analysis. Creating presentations to communicate insights and findings to others in the organization.

**Applications of Data Analytics in Different Fields**

- **Healthcare**: Identifying trends in patient health data to make recommendations for improving patient outcomes. For example, a health insurance company might use data analytics to identify patients with asthma and suggest strategies like regular cardio exercise to improve their health.
- **Finance**: Understanding market trends and making informed investment decisions. Data analysts might analyze stock prices, economic indicators, and other financial data to help investors.
- **Marketing**: Understanding consumer behavior and preferences. Collecting and analyzing data on buying patterns, website traffic, and social media activity to develop targeted marketing campaigns. For instance, recommending a shift in advertising budget based on predicted effectiveness of different marketing channels.

**Career Opportunities**

- **Growing Demand**: There is an increasing demand for data analysts across various industries.
- **Career Growth**: Numerous opportunities for career growth and advancement in data analytics.
- **Industry Impact**: Data analysts play a critical role in solving real-world problems and making data-driven decisions.

**Data Analytics vs. Data Science**

As you learn about data, you'll encounter various data-related terms. Two often-confused terms are data analysts and data scientists. While both roles work with data to gather insights, their approaches and responsibilities differ.

**Data Analysts**

- **Role**: Work with structured data to identify patterns, build visualizations, and extract insights to aid decision-making.
- **Responsibilities**:
  - Maintaining databases
  - Interpreting data sets
  - Creating reports that present data trends, patterns, and predictions
  - Gathering data from various sources
  - Cleaning and organizing data
  - Presenting findings through visualizations
- **Skills and Tools**:
  - Foundational mathematics and statistics
  - Analytical thinking and data visualization
  - Basic fluency in R, Python, and SQL
  - Proficiency in SAS, Excel, and business intelligence software
- **Background**:
  - Experience in mathematics and statistics
  - Degrees in mathematics, statistics, computer science, or finance

**Data Scientists**

- **Role**: Work with both structured and unstructured data using advanced techniques like machine learning and predictive modeling to design processes, develop models, and extract insights.
- **Responsibilities**:
  - Arranging undefined datasets
  - Writing algorithms
  - Building automation systems and statistical models
  - Gathering and cleaning raw data
  - Creating data visualization tools, dashboards, and reports
  - Developing code to automate data collection and processing
- **Skills and Tools**:
  - Advanced statistics and predictive analytics
  - Machine learning and data modeling
  - High-level, object-oriented programming
  - Proficiency in Hadoop, MySQL, TensorFlow, and Spark
- **Background**:

  - Experience in computer science
  - Master's or doctoral degree in data science, information technology, mathematics, or statistics

  **Introduction to the OSEMN Framework**

  In your career as a data analyst, you'll encounter various data analysis projects with different data sources, methods, and business goals. A framework can help structure your approach, and the OSEMN framework is one such tool.

  **OSEMN Framework Overview**

  - **Definition**: The OSEMN framework, pronounced as "awesome," was described by Hilary Mason and Chris Wiggins. It stands for Obtain, Scrub, Explore, Model, and Interpret.
  - **Purpose**: Helps break down data analytics projects into manageable stages.

  **Stages of the OSEMN Framework**

  1. **Obtain**:
     - **Description**: Gathering data relevant to your business question. Sources can include internal data like sales records or external sources like government databases or financial market data.
     - **Considerations**: Sometimes obvious, sometimes requiring creativity and research. You may need to explore if the data exists or generate your own through interviews or observations.
  2. **Scrub**:
     - **Description**: Cleaning the data to make it usable. This involves correcting data formats and dealing with missing data points.
     - **Importance**: Proper data scrubbing helps avoid problems later in the analysis.
  3. **Explore**:
     - **Description**: Searching for patterns and statistics in the data. Categorizing and visualizing data with basic charts helps understand what the data is telling you.
     - **Role**: Acts as a detective phase, looking for trends, patterns, or anomalies.
  4. **Model**:
     - **Description**: Using statistical or mathematical methods to generate predictions and insights. This stage involves deeper analysis to understand patterns in the data.
     - **Application**: Many models are easy for data analysts to apply and can provide detailed insights.
  5. **Interpret**:
     - **Description**: Building a better understanding of the situation by visualizing data and creating stories and presentations.
     - **Goal**: Helps others understand the results of your analysis through clear communication.

  **Conclusion**

  - **Flexibility**: The OSEMN framework is simple and flexible, making it a great choice for data analytics projects.
  - **Future Learning**: The course will delve deeper into each stage of the OSEMN framework in the following weeks.

  The OSEMN framework helps structure data analysis projects effectively, ensuring each step is manageable and systematic.

  **Setting Business Goals for Analytics Projects**

  Before starting an analytics project, it's essential to understand the business goal related to your analytics task. A clear goal is crucial for planning and measuring progress.

  **SMART Goals**

  - **Specific**: The goal should be clear and understandable. Example: sell 100 products, get 200 new subscribers, or generate 20 qualified leads.
  - **Measurable**: The goal should be quantifiable. Example: number of sales, revenue, or website visitors.
  - **Achievable**: The goal should be realistic. If a goal is too ambitious, break it into smaller, achievable sub-goals or milestones.
  - **Relevant**: The goal should align with the broader business objectives and strategy.
  - **Time-bound**: The goal should have a start and end date to track progress and evaluate outcomes.

  **Example of a SMART Goal**

  - **Scenario**: A business launching its website to transition from physical store success to online sales.
  - **Goal**: Get 10,000 website visits during the month of May.
    - **Specific**: Clearly states the objective.
    - **Measurable**: Can track the number of visits using website analytics.
    - **Achievable**: Challenging but realistic given the business's physical store success.
    - **Relevant**: Supports the intention to sell products online.
    - **Time-bound**: Deadline set for the end of May.

  **Formulating Objectives for Analysis**

  - **Defining the Objective**: Once a SMART goal is set, it's easier to formulate the objective for your analysis, making it clear what you are contributing to.
  - **Determining Questions**: Identify the questions to answer to achieve the goal. Answering these questions becomes the analysis objective.
  - **Evaluating Goals**: Often, analysis centers around evaluating whether a goal was achieved.
  - **Isolating Metrics**: Identify key performance indicators (KPIs) that best indicate whether the goal was reached.

  **Next Steps**

  - **KPIs**: Later in the course, there will be a detailed explanation of KPIs and how to use them.

  A clear understanding of business goals and the formulation of SMART goals are fundamental steps in ensuring the success of your analytics projects.

  **Key Performance Indicators (KPIs)**

  Before diving deeper into using the OSEMN framework, it's important to understand Key Performance Indicators (KPIs). KPIs are measurable values that help track progress toward a goal.

  **Example of a SMART Goal and KPI**

  - **SMART Goal**: "By the end of this year, I want to run the Honolulu Marathon in under four hours."
    - **Specific**: Running the Honolulu Marathon.
    - **Measurable**: Completing the marathon (26.2 miles) in under four hours.
    - **Achievable**: Realistic with proper training.
    - **Relevant**: Relates to overall fitness focus.
    - **Time-bound**: Deadline is December.
  - **KPI Example**: Using Strava to track running progress.
    - **Important Metric**: Pace (11 minutes and 49 seconds per mile).
    - **Goal**: Run at approximately nine minutes per mile to complete the marathon in under four hours.
    - **Pace**: A key metric to assess progress.

  **Characteristics of KPIs**

  - **Quantitative**: KPIs are measurable.
  - **Directional**: KPIs increase or decrease over time.
  - **Direct Relationship**: KPIs should have a direct connection to the goal.

  **Determining KPIs**

  - **Example**: Calla & Ivy, a flower business in Amsterdam.
    - **SMART Goal**: "Get 10,000 website visits during the month of May."
    - **Primary KPI**: Number of monthly website visitors.
      - **Measurable**: Number of visitors is quantifiable.
      - **Directional**: Visitor count can increase or decrease.
      - **Direct Relationship**: Directly related to the goal of increasing website traffic.
    - **Secondary KPIs**: Metrics like brand awareness, number of pages viewed, average time spent per visit, and number of transactions. These are related but do not directly measure the success of the primary goal.

  **Importance of KPIs**

  - **Prevent Overwhelm**: With a lot of data available, KPIs help focus on what's important.
  - **Set Ahead of Time**: Setting KPIs related to the goal ahead of time protects against getting lost in the data.
  - **Ready for Analysis**: With goals and KPIs set, you're prepared to start your analysis.

  In the following sections of the course, you will see examples of how to apply the OSEMN framework using KPIs to ensure effective data analysis.

  **Using the OSEMN Framework in Practice**

  Reflecting on SMART goals and understanding KPIs is crucial for setting objectives for your analytics project. The OSEMN framework can make a significant difference in achieving accurate results.

  **Example: Calla & Ivy's Marketing Campaign**

  - **Business Goal**: Imra, the owner, aims to get 10,000 website visits during May through a social media advertising campaign.
  - **Campaign Plan**: $5,000 budget for Instagram ads, daily Instagram stories, and TikTok videos.

  **OSEMN Framework Steps**

  1. **Obtain**:
     - **Data Sources**:
       - Google Analytics 360 for website visit data.
       - Meta's Ads Manager for Instagram ad performance.
       - Instagram and TikTok accounts for stories and video performance.
     - **Data Collection**: Download data from January to May, covering website visits, ad performance, and social media engagement.
  2. **Scrub**:
     - **Cleaning Data**:
       - Add clear labels to data.
       - Remove rows with broken URLs.
       - Reformat dates to ensure consistency across data sources.
     - **Preparation**: Ensure data is accurate and ready for analysis.
  3. **Explore**:
     - **Trend Analysis**: Create a chart showing website visits by month.
     - **KPI Achievement**: Verify that the campaign exceeded the goal with 13,457 visits in May.
     - **Marketing Activity Effectiveness**:
       - Analyze clicks from Instagram ads, TikTok videos, and Instagram stories.
       - Identify that TikTok videos generated significant clicks at a lower cost.
     - **New Insights**: Consider shifting some budget to TikTok ads.
  4. **Model**:
     - **Prediction**: Build a statistical model with the help of a teacher to simulate reallocating the budget ($3,000 on Instagram ads and $2,000 on TikTok ads).
     - **Result**: Predict a 35% increase in website visits for June.
  5. **Interpret**:
     - **Presentation**:
       - Report KPI trends and marketing performance.
       - Highlight the success of TikTok videos and potential benefits of reallocating budget.
       - Explain model predictions for increased website visits.
     - **Impact**: Imra decides to advertise on TikTok based on Sheila's data-driven insights.

  **Conclusion**

  - **Success**: Sheila's work demonstrates the practical application of the OSEMN framework and influences decision-making at Calla & Ivy.
  - **Future Learning**: The course will delve deeper into each OSEMN step, equipping you with the tools to tackle various data analytics challenges.

  **Introduction: Obtaining and Scrubbing Data**

  In the second week, the focus is on obtaining and scrubbing data, which are the first two steps of the OSEMN framework. This week covers where to find data and how to clean it for analysis.

  **Obtaining Data**

  - **Types of Data Sources**:
    - **Freely Accessible, Open-Source Databases**: Examples include Eurostat, OECD, World Bank, United Nations, and US Census Bureau. These sources offer a wealth of reliable information on various socioeconomic topics.
    - **Company-Specific Data**: This includes internal data like sales records or data collected using software like Google Analytics. This data is not publicly accessible and is often proprietary.
    - **Intentionally Collected Data**: When required data isn't available, it can be collected through surveys, interviews, or observations using tools like SurveyMonkey or Google Forms.
  - **Data Formats**:
    - **Numeric Data**: Quantitative data stored in tables or spreadsheets, often in .CSV format.
    - **Text Data**: Unstructured data from sources like social media posts, emails, and customer reviews. Analyzed using natural language processing (NLP).
    - **Visual Data**: Data in visual formats like images and videos, useful for quality control, machine learning in self-driving cars, and medical research.
  - **Sampled Data**:
    - Used when analyzing the entire population is impractical.
    - Important considerations include sample size, representativeness, and generalizability.
  - **First vs. Third Party Data**:
    - **First Party Data**: Collected directly by a business from its customers or internal sources.
    - **Third Party Data**: Collected by outside parties, such as market research firms or government agencies.

  **Evaluating Data Quality**

  - **Source Credibility**: Check the authorship, credentials, and publication date.
  - **Methodology**: Ensure the sample size is adequate and the sampling method is unbiased.
  - **Objectivity**: Avoid data with biases or conflicts of interest.
  - **Accuracy**: Cross-check data with other reputable sources and look for obvious errors.
  - **Relevance**: Ensure data is relevant to the question at hand and presented in a meaningful context.

  **Checklist Example**

  - **Example**: Evaluating data on climate change effects on polar ice caps.
    - Check source credibility: Verify the credentials and affiliations.
    - Check methodology: Ensure sample size and sampling techniques are clear.
    - Check objectivity: Look for biases or conflicts of interest.
    - Check accuracy: Compare with data from reputable sources.
    - Check relevance: Ensure data fits the research scope and is well-documented.

  **Free Data Sources**

  - **Google Public Dataset Search**: Access to millions of datasets on public websites.
  - **United States Census Bureau**: Data on US population, economy, and geography.
  - **Pew Research Center**: Insights on social, political, and technological issues.
  - **Eurostat**: Economic, social, and environmental data from the European Union.
  - **OECD**: Comparative data on global economic and social matters.
  - **Kaggle Datasets**: Public datasets from various industries.
  - **National Centers for Environmental Information (NCEI)**: Data on climate change and environmental information.
  - **World Bank Open Data**: Global indicators on economic, social, and environmental trends.

  By understanding these aspects of data collection and evaluation, you will be better equipped to obtain and scrub data effectively for your analytics projects.

  **Introduction: Obtaining and Scrubbing Data**

  In this section, we focus on the first two steps of the OSEMN framework: obtaining and scrubbing data.

  **Obtaining Data**

  - **Overview**: The obtain phase involves gathering data from various sources for analysis.
  - **Data Sources**:
    - **Freely Accessible, Open-Source Databases**: Examples include Eurostat, OECD, World Bank, United Nations, and US Census Bureau.
    - **Company-Specific Data**: Internal data such as sales records or data collected using tools like Google Analytics.
    - **Intentionally Collected Data**: Data gathered through surveys, interviews, or observations using tools like SurveyMonkey or Google Forms.
  - **Data Formats**:
    - **Numeric Data**: Quantitative data stored in tables or spreadsheets, often in .CSV format.
    - **Text Data**: Unstructured data from sources like social media posts, emails, and customer reviews.
    - **Visual Data**: Data in visual formats like images and videos, useful for various analyses.
  - **Sampled Data**:
    - Used when analyzing the entire population is impractical.
    - Considerations include sample size, representativeness, and generalizability.
  - **First vs. Third Party Data**:
    - **First Party Data**: Collected directly by a business.
    - **Third Party Data**: Collected by outside parties, such as market research firms or government agencies.

  **Scrubbing Data**

  - **Importance**: Cleaning data is essential to ensure it's usable for analysis and to avoid errors that could lead to incorrect conclusions.
  - **Four Main Tasks**:
    1. **Removing Duplicates**: Identifying and eliminating duplicate records to ensure each record is unique.
    2. **Formatting Records**: Ensuring consistent data formatting to avoid confusion and errors in analysis.
    3. **Solving for Missing Values**: Addressing missing values by either filling them in with placeholders (like "unknown" or "N/A") or removing records with crucial missing information.
    4. **Checking for Mistakes or Wrong Values**: Identifying and correcting obviously wrong values, such as unrealistic numbers or errors.

  **Example: Calla & Ivy's Marketing Campaign**

  - **Business Goal**: Imra, the owner, aims to get 10,000 website visits during May through a social media advertising campaign.
  - **Campaign Plan**: $5,000 budget for Instagram ads, daily Instagram stories, and TikTok videos.
  - **Data Sources**:
    - Google Analytics 360 for website visit data.
    - Meta's Ads Manager for Instagram ad performance.
    - Instagram and TikTok accounts for stories and video performance.

  **Example: Inu and Neko's Subscription Service**

  - **Objective**: Carlos wants to launch a subscription meal service for cats and dogs with a goal of 500 subscribers in the first year.
  - **Data Source**: E-commerce sales data from the company's website.
  - **Scrubbing Process**:
    1. **Remove Duplicates**: No duplicates found in the dataset.
    2. **Format Records**: Standardize ZIP codes and remove unnecessary columns (phone numbers and size).
    3. **Solve for Missing Values**: Fill missing sales totals and remove records with missing customer IDs.
    4. **Check for Mistakes**: Remove entries with obviously incorrect product prices.

  **Conclusion**

  By obtaining and scrubbing data effectively, you ensure that your dataset is clean and ready for analysis. This prepares you for the next steps in the OSEMN framework: exploring, modeling, and interpreting your data.

  **Exploring and Modeling Data**

  In the third week, we focus on the exploring and modeling phases of the OSEMN framework. These stages help you uncover patterns and predict future outcomes using your data.

  **Exploring Data**

  - **Overview**: The explore phase is about getting familiar with the data, identifying patterns using statistics and visualizations.
  - **Iterative Process**: Revisiting earlier stages (obtain and scrub) is normal as you discover more about your data.

  **Steps in Data Exploration**

  1. **Understanding Your Data**:
     - **Data Sources**: Identify and combine data from different sources.
     - **File Sizes**: Note the size of each file to gauge the amount of data.
     - **Data Structure**: Count rows and columns to understand the dataset's structure.
  2. **Data Types**:
     - **Numerical Data**: Quantitative data expressed in numbers.
     - **Categorical Data**: Data that falls into distinct groups.
  3. **Summary Statistics**:
     - **Categorical Data**: Count the frequency of each category.
     - **Numerical Data**: Calculate minimum, maximum, median, mode, mean, and standard deviation.
     - **Raw Data Review**: Look at the first and last few rows or take a random sample.
  4. **Data Visualizations**:
     - **Importance**: Visualizations help understand complex data and communicate insights effectively.
     - **Key Elements**: Titles, axes, legends, labels, and visual attributes.
     - **Types of Visualizations**: Bar charts (compare categories), line charts (show trends over time), scatter plots (show relationships between variables).
  5. **Data Distributions**:
     - **Histograms**: Visualize numerical data by grouping values into bins.
     - **Common Distributions**: Normal, bimodal, log-normal, exponential, and uniform.
     - **Insights**: Identify minimum, maximum, mode, and standard deviation from histograms.
  6. **Variable Relationships**:
     - **Correlation**: Measure the relationship between variables using scatter plots and line charts.
     - **Correlation Coefficients**: Quantify correlation (ranging from -1 to 1).
     - **Correlation vs. Causation**: Distinguish between correlation (relationship) and causation (one variable causes the other).
  7. **Feature Engineering**:
     - **Definition**: Creating new features or modifying existing ones to better understand data.
     - **Techniques**: Extracting information from dates, creating calculated features, and categorizing data.
     - **Domain Expertise**: Understanding your field to identify valuable features.

  **Explore Checklist**

  1. **Inspect Your Data**:
     - Read through your data for interesting information.
     - Use summary statistics to evaluate your data.
     - Inspect a random sample if the dataset is too large.
  2. **Visualizing Data**:
     - Use bar charts, line charts, and scatter plots to examine hidden information.
  3. **Examine Variable Distributions**:
     - Categorize and plot the data.
     - Evaluate minimum, maximum, mode, and standard deviation.
  4. **Examine Variable Relationships**:
     - Visualize variables to understand their correlation.
     - Calculate the correlation coefficient.
  5. **Feature Engineering**:
     - Create new features or modify existing ones for better insights.

  By exploring your data using these steps, you will gain a deeper understanding and be prepared to move on to the modeling phase, where you can make predictions and derive actionable insights.

  **Modeling Data**

  **Make Predictions from Your Data with Modeling**

  So far, we've gone over the first three stages of the awesome cycle. We've obtained the data we needed, scrubbed it, and explored it thoroughly. Now, we're ready for stage 4 "Modeling". This phase is about using our data to make predictions with mathematical models. These models can be anything from simple linear regressions to advanced machine learning algorithms depending on the project. Although there are many different models, they all work by discovering hidden patterns in data and using it to make predictions on any new data we give the model. For example, you might build a model to predict how many conversions you expect a campaign to deliver. You would do that by using data from the past to predict the future. Our discussion of modeling is going to be broken down into the following 3 sections: What are models, how the models work, and the types of models? All of the steps of the OSEMN process are important, but modeling is a central piece of data analysis. This is the stage when we start getting answers to our key questions. So let's get started!

**What Are Models and Why Use Them?**

You might not know it, but we live in a world full of models. In marketing, for instance, models are used for a number of different tasks, including projecting sales, predicting whether an individual is likely to buy a product, splitting audiences into subgroups, and finding new potential customers. In fact, it's hard to find an area where models aren't used. You'll hear about economic predictions and weather forecasts if you listen to the news. If you go on your favorite social media apps, your feed is full of content that you might like, and you're shown advertisements based on the groups you are perceived to be in. These are all examples of models. So what are models? Models are mathematical tools that can be used to recognize hidden patterns in data that often aren't obvious to humans. Models can be used to try and apply those patterns to different data. Their most common use is making predictions about the future based on data from the past. Creating some models can be really simple. For instance, calculating the average of a set of data and then predicting our future data will be average is a simple model. It's often not a very good one, but it is a model. Creating models can also be very complex, like machine learning models that try and predict stock prices, or models that generate amazing text and images. The good news is that you often don't need to know how to build them to use them! Analysts will often work very closely with a team of data scientists or with a specialized firm that builds the models for them. So, we'll focus on understanding how to use them, so that you can pick the right model for the job and use it correctly in your work as a data analyst. Let's look at a type of model you learn to use called a Linear Regression. Linear regression sounds fancy, but it just means a line that can be used to predict the value of one variable using the value of another variable. This particular linear regression predicts the price of a house based on the square footage. The line and shaded region represents our model and its predictions, while the dots are the examples the model examined from the data set we trained it on. In other words, the dots represent the data we gave to the machine to learn more about house prices, and the machine then used that learning to make predictions about other prices. This model gives price predictions for a given amount of square footage. For instance, to get the prediction for a 3000-square-foot house, we find the x-coordinate (Square feet) and move up until we hit that prediction line. Then we take the Y coordinate on the line as the corresponding prediction! We can see that the model predicts that the higher the square footage, the higher the price of the house-- this makes sense. Remember that I mentioned predictions will not be perfectly accurate. This is what the shaded area represents-- the margin of error with 95% confidence, meaning that we're 95% sure that the actual value will land somewhere in the shaded area. The ability to quantify uncertainty like this is what allows us to make smart business decisions about the predictions our models make and how much we should trust a given prediction.

As the statistician, George Box famously said, "All models are wrong, but some are useful." We must understand that the predictions our models make won't be perfectly accurate. However, with the right data, they'll be accurate enough to generate insights that have business value. The predictions that models make are never 100% certain, but importantly, we can quantify the uncertainty for each model. This allows us to understand how far off the average prediction might be.

​
**How Do Models Work?**

Now that we know what models are, let's dive deeper into how they actually work. Remember, models are mathematical tools that can be used to recognize hidden patterns in data that often aren't obvious to humans. But how do we get from a set of raw data to a useful model that can make predictions? It all starts with model training. The basic idea is that we feed in a set of known data and use an algorithm to create a mathematical representation of the relationship between the input and output variables we're trying to predict. You can think of an algorithm as a recipe. It describes a set of procedures to be carried out. The algorithms used by models examine data sets and use math to learn the underlying relationships in the data. In the case of linear regression, an algorithm is used to create the equation for a line that approximates the input data. If you've taken algebra, you might remember the equation y is mx+b. This is the general equation for a line. Y is the output of the equation, x is the input value, and m is the slope of the line, and b is the y intercept. The algorithm creates a linear equation with m and b values that best fit the input data. You can then put any x value into the equation and predict y. Essentially, all models attempt to do something similar. They mathematically encode a relationship. In linear regression, it's the m and b values. In a more complicated model, like a neural network, the model's algorithm might be determining billions of value in a complex network of relationships. In practice, different models will require the data to be in a specific format. For example, some algorithms will require all of your data to be numerical. Other algorithms can't handle missing values. It's up to you to manipulate the data before applying an algorithm. This is one of the reasons why the scrubbing stage of the OSEMN process is so important. Another key feature of models is what type of data they output. There are special names for different models based on the type of data they output. We will discuss this more when we talk about the types of models. Many models also need to go through a training and testing phase. The training phase is what we just discussed, where the model attempts to learn a relationship. The testing phase is where an analyst uses a second dataset that the model was not trained on to check and make sure that the model did not learn something that was only true for the training data. If a model returns good results for another dataset, its results are said to be generalizable to that data. The testing data is fed into the model and the algorithm predicts the output variable for each data point. We can then compare these predictions to the actual output values we know from the testing data. This allows us to measure how well the model can make accurate predictions. If you look at our linear regression model from before, we can see that the model fits very well for the blue data the model was trained on, but does not fit very well with the red data we tested it with. In this case, its because we trained on data from one city and test it on data from another city. Apparently, housing prices can be quite different between cities. This leads us to an important lesson about models. Always make sure that your model is trained on data that is as similar as possible to the business case you care about. In this case, we only care about predicting housing prices in the blue city, so our model will work just fine. It's often best practice to split a single dataset into testing and training data to ensure the data is similar. Sometimes it makes sense to split the data randomly, but often it's best to train on older data and test against the most recent data if the data is time-based.

Generating models can be an art as much as it is a science, so practice is key to learning to make good models. Keep up the great work. I'll see you in the next video.

**Different Types of Models**

Models are fundamental to understanding and interpreting patterns within data, and they can be used across many fields. So far, we've discussed basic models, such as linear regression, but let's look deeper at the types of models you might encounter. A good place to start discussing models is, what types of questions do you want your model to be able to answer? Regression models answer questions that start with how much, or how many. They're used for answering questions that are numerical in nature. As an example in marketing, they're often used for forecasting numerical metrics like sales and click-through rates. As another example, regression models could be used in finance to predict the price of a stock based on various factors like the company's earnings, the state of the economy, and historical trends. Classification models predict the group or "classes" as answers. The data they output is categorical. These classes can be binary, like in "True" or "False". Or they can be one of many in a multiclass problem, like when predicting if an image is of a dog, cat, or human. For instance, in healthcare, classification models can help identify what disease a patient might have based on their symptoms. They can be used in marketing to answer questions like, is this a customer that's going to leave me for a competitor or not? Clustering algorithms split data into groups or segments with similar characteristics. Clustering algorithms are often used to make a customer base and divide them into smaller niche audiences, which can lead to more focused advertising for each of these specialized groups. In sociology, clustering algorithms are used to group people based on common attributes or behaviors. Using a clustering algorithm on customer sales data might reveal that most of the customer base comes from distinct segments that we can translate into marketing insights. For example, a clustering algorithm could show distinct customer segments, like "sports fans aged 18 to 25," or "new mothers in urban communities." Each model is defined by its algorithm, each having its own strengths and weaknesses. Although there are many algorithms, let's focus on three prominent ones, linear regression, decision trees, and neural networks. Each of these models brings a different approach to data analysis, suited for different complexities and types of problems. Linear regression, a form of a regression model, identifies linear relationships between input and output variables. They offer a simple, less data-intensive model ideal for simpler problems. You often don't need a large dataset to perform linear regressions, but the trade-off is they only provide answers to simple questions. Decision trees are models that use a tree-like structure to segment data based on a series of binary decisions. For instance, in credit scoring, decision trees might be used to determine whether an individual is likely to default on a loan based on factors such as income, employment status, and credit history. Individual trees are usually combined into a model known as a random forest. These models are probably best known as classification models, but can also be used to create regression models. Decision trees are well suited to help with a wide range of analysis problems and tend to require less computing power than neural networks. Neural networks use complex networks of "neurons" to make predictions. They're great at learning complex relationships involving numerous variables and patterns. They're used in diverse fields like real estate, autonomous driving, and art generation, and often require substantial data and computing power, but the results can be very useful.

The key to success is selecting the right model for the problem you're trying to solve. Some models are more effective than others when working with certain types of problems. Understanding the strengths and limitations of each type of model will help guide you in making the best choice for your specific project. It's important to understand that models primarily help us to analyze data so that we can interpret it and predict what might happen in the future. We will go into this in more detail later. See you then.

**Example: Exploring and Modeling Data**

**A Real World Example**

In previous videos, we followed Keira, a data analysts working with Carlos, the owner of Inu and Neko, a dog and cat care company. Carlos had approached Keira to help and launch a new subscription meal service for cats and dogs and wanted to select the 10 best products to offer as part of the subscription. Keira started by obtaining the necessary data from their e-commerce software. She downloaded last year's sales data into Google Sheets to analyze which products were most popular and purchased repeatedly. She obtained the data properly by making sure it was credible, collected accurately, objective, accurate, and relevant to Carlos' questions. Keira proceeded to scrub the dataset, checking for duplicates, inconsistent formatting in the zip codes, and missing values like phone numbers and sales totals. She standardized zip codes for consistency and remove the phone number column because it was not crucial for analysis. Keira filled in missing sales stores by calculating product price multiplied by quantity. Some records lacked customer IDs, preventing the assessment of repeat buyers. So Keira deemed them unhelpful and removed them from the dataset. Lastly, she identified inaccurate information such as negative sales amounts and unusually high prices, recognizing them as glitches in Inu+Neko’s system and excluded them from consideration to maintain data accuracy for future analysis. Now that Keira has completed scrubbing the data successfully while addressing duplicates, inconsistencies, proportionate gaps, and inaccuracies, she's ready to dive into our next topic; explore and model. In the explore and model stages of the OSEMN Framework, Kiera is now tasked with uncovering patterns and trends within the data and creating a model to predict subscription bundle preferences for Inu and Neko's customers. During the explorer stage, Keira will perform various exploratory data analysis techniques to gain insights. This might involve using charts and visualizations to identify correlations between different product categories or demographic segments. She'll also dive deeper into customer behavior by analyzing purchase frequency, average basket sizes, and seasonal trends. Once Keira has explored the data thoroughly. She can move on to modeling. Modeling involves using algorithms to create a predictive model based on historical data. Keira will develop models to help Carlos meet his goal of hitting 500 subscriptions. In the upcoming videos, we'll follow Keira through the explore and model stages of the OSEMN Framework! Let's get started!

**Case Study - Exploring Data**

Kira now has a clean set of data that she considers relevant for the question she got from Carlos at Inu+Neko: Which products should he include in the new dog and cat food subscription product to help him reach 500 subscribers by the end of the year? Kira gets started with Step 3 of the OSEMN cycle, she starts to explore the data. She thinks that a good place to start is to see what types of data are in each column. She sees that she has both categorical and numerical data. There's a date column and text columns like order numbers and customer IDs. She notices that the order number and customer ID columns are very long and take up a lot of screen space and are hard to type. So, she decides to map each unique value to a number in a process called encoding. Encoding is the process of turning a string of data into numerical data by mapping each unique string to a unique number. Encoding can be used to reduce the amount of memory needed to work with data, or to make large, complex strings easier to work with. Can also be used to turn text data into numbers when a model needs numerical inputs instead of text inputs. In this example, Kira's using encoding to make the data easier to view and understand. She also notices that there are quite a few columns that contain redundant information. A column is redundant if you could look at one column and always guess what's in the other column. In this case, the customer ID and name are redundant. They might not always be, but for this product she knows it's safe to assume, so she hides customer name. She also notices that SKU and product name are redundant as well, so she hides the SKU. She also decides that it is unlikely that this project will look at data at the street address level, so she only keeps the state and zip code. Great, now she can see all of the useful columns at a glance. And not having a large text columns even helps her computer run more smoothly. Next, she decided to run some summary statistics for the remaining columns. She looks at things like the most common value, how many times it appears, and what percentage of the rows it shows up in. She also looks at the minimum, maximum, range, and mean of the numerical values. From this, she sees that the data spans 899 days from the spring of 2019 through the summer of 2021. She sees that Texas is the most popular state, which makes sense because it is quite populous. There are also some numerical columns that she thinks could come in handy, like price and quantity. Kira decides to add up the total sales for each product and use a bar chart to look at the distribution of these total sales. This gives her exactly what she was looking for, the top selling products. She can also see from this chart that cat products tend to sell more than dog products. She realizes that it's great to know what top selling products are, but it might also be important to know what causes those products to sell more. To answer that question, she creates a few scatter plots to observe the relationships between the quantities sold and different variables. One relationship that stands out to her is quantity sold and price. She notices that if she looks at just cat or dog products in isolation, the quantity sold is low for low and high-priced items, but high for items in the middle. This means that the two variables have a positive correlation for low prices, and a negative correlation for high values. This is great information to pass on to Carlos. Maybe adding more medium-priced items will help him find additional products for his subscription service. Kira now knows the top selling products and a possible reason for why those products are the top sellers. Kira's ready to move on to the next stage in her analysis, the model stage.

**Case Study - Modeling Data**

Kira is making her way through the OSEMN cycle. She has obtained the right data, scrubbed the data, and she learned quite a bit in the exploration stage to help Carlos at Inu+Neko decide on the product mix for the new subscription service. Now, it's time to create a model. Kira wants to create a model so she can use the data from historical purchases to predict the future. Of course, there are a few complexities here. The historical data doesn't show any information on subscriptions, because the subscription model doesn't exist yet. But Kira believes that it is fair to assume that if a subscription model exists, the people who buy almost every month are likely to subscribe. She doesn't know exactly how likely though, so she decides to play it safe and assume that 50% of the people who buy eight or more times a year will subscribe. Kira talks to her friend who is a data scientist. She tells him that, from her exploration of the data, she learned that this is the ranking of the products among repeat buyers. She thinks that the top five cat and dog products in this list should be the ten items included in the package. She refers to this package as "Package A". She also tells him that she thinks there's a 50% chance that a repeat buyer would become a subscriber. Her friend says that using this information he can create a custom algorithm to train a regression model that will predict the number of subscriptions Kira can expect in a year. At this point, she doesn't have to understand the details of the model, but it basically finds every repeat buyer in the database that bought one of the ten products in the proposed package. And then assumes that 50% of them will become subscribers. The model shows that if we use the 10- product mix Kira suggested in Package A, we can expect 475 subscriptions. Kira knows that isn't quite enough to reach Carlos's goal of 500 subscribers. She knows that Carlos told her that he wants to keep the products offered in the subscription limited to 10 products, but she's curious to see what would happen if she put 12 products in the package instead. So she asks her friend to run the model again, but now with package B, which includes the top six dog and cat items purchased by repeat buyers. After they run the model, they find that the estimated number of subscribers for the year with package B is 517. That helps to reach Carlos's goal! Kira is happy with the predictions she has from the model based on the different packages. She isn't sure whether Carlos will be interested in adding two more products to the package, but at least she can now provide him with a convincing argument for why he might want to consider it.

**Weekly Conclusion**

That brings us to the end of another week in this course. You're doing great. We have now covered four of the five steps in the OSEMN framework, and you're well on your way to becoming a data analyst! You know how to explore your data by examining data distributions and relationships. You also know the different types of models and how they can be used to make predictions from your data. And you've seen the explore and model phases applied to a real-world example. Now, we will move on to the final step of the OSEMN framework INterpreting the data. See you there!

**Interpreting Data**
**Introduction: Interpreting Data**

Welcome to the last week of our Intro to Data Analytics course, great work so far. You now know what's involved in obtaining and scrubbing data, as well as exploring and modeling it. So this week, we will focus on the last step in the awesome Framework, interpreting your analysis. Generating insights from your data is very powerful, and giving an interpretation to those findings, is where you can help the data tell a story. I think this is where a lot of the magic happens. Telling a story and convincing other people of your points is great. But if you can do that, backed up with data, it just makes you feel more powerful. This week, we'll highlight some of the ways in which you can do this. In the first lesson, we will take a look at what's involved in this interpretation phase, and how you can approach drawing conclusions from your analysis. Then in the second lesson, we will focus on how you can let your data tell a compelling story, a skill that you'll find very useful as a data analyst. And after that, we'll put all we learned about applying the awesome Framework together, in a real life example. We'll hear from Jules on how he used the awesome Framework, for a research project, to try and influence policymakers. You're in the final stretch for this course, so let's get right to it.

**Answer Your Business Question with Your Data**

The interprets stage of the OSEMN process is where everything comes together. So far, you've seen the first four stages of the OSEMN framework, obtain, scrub, explore and model. Each stage gives you a better understanding of the business problem you're trying to solve. For example, you could be predicting the number of responses for an email campaign or you could be trying to find new potential customers to grow the business. In either case, you start with the obtain stage where you collect the data to answer the question. Then you move on to the scrubs stage where you clean the data. Remember cleaning data includes removing duplicates, handling missing values, and ensuring the data has a consistent format. After scrubbing, you then explore the data. When exploring, you'll apply statistics to find interesting patterns and trends. For example, you might notice that some email campaigns have higher response rates than others, and after exploring, you'll apply models to the data. These models will allow you to generate predictions. For example, based on past email campaigns, how many responses can be we expect for this campaign? Models also give you more insights. A model that predicts email response rates will also tell you what makes it more likely for people to respond to emails. Knowing what influences people to respond can help improve marketing campaigns. The last stage of the OSEMN process is interpret. The interpret stage is where you interpret your analysis. It's arguably the most important. Without it, all we would have is data and statistics. The interprets stage translates your analytical findings back to a business context. After successful modeling stage, you'll have a new tool like a regression model that can be used to generate predictions. The answers generated from these sorts of models are very specific and usually aren't immediately interpretable or understandable by non-technical team members. During the interprets stage, our goal is to close the loop of the OSEMN cycle by using the models and insights we generated during the exploration and modeling phases to try and answer the business question driving the entire project. In other words here you look back at your objective for your analysis. Your goal here is twofold. First and foremost you need to understand the results of your model and all the insights it can provide. These might be the actual predictions the model makes, like forecasting the results of sales from a campaign or they might be information contained within your model like an insight that shows that mailing lists sign-ups are strong predictors of people spending more money with your company. Second, you need to be able to explain your findings to a non-technical audience in a clear concise way. Simply understanding the implications of your model isn't enough. You need to be able to make others understand it and trust your results. Remember, analytics projects are about generating actionable insights or information that can be used to make better decisions that help the company.

**Understand the Results of Your Model**

The first thing you'll do during the interpret stage is to try and answer some basic questions. Some questions to ask yourself in the interpret stage include, what was the objective of this analysis? It's important to go back to your starting point because that will remind you of the questions you set out to answer. It's quite easy to get lost in the data during the model and explore stages and lose sight of your initial question. Then ask yourself, how does the data answer my questions? Maybe the data shows you that the business goal you set is currently unattainable, or maybe it gives you a plan that you could use to move forward. Another question to ask is, what other learnings do I have? In the process of answering one business question, you will often find new pieces of potentially useful information that help solve the problem at hand in a different way. Or maybe they open up new potential business objectives you can address in later analysis. How can I apply this to a business context? Gaining new knowledge is great, but it is important to focus on information that's actionable and moves your business forward in meaningful ways. It will often be someone else that takes action based on your information, so think about how that will happen. Perhaps the most important question to ask is, how confident should I be in my results? If you see an improvement in a business metric, was it due to the changes you made or was it due to random chance? Many data analysts are overconfident in their results, and when they implement what they have learned, they quickly discover that something was wrong with their analysis. That brings us to the topic of, how do you know if you should be confident in the results of your model? Earlier, during the modeling stage, we briefly discussed using a separate set of test data to check your trained model against. The testing process is all about ensuring you have the right amount of confidence in your model. By running your test data through your model, you can answer questions like, how wrong is the model on average? If the model predicts something, how likely is it to be correct or incorrect? Are there particular scenarios that cause the model to be incorrect? Even the best models have limitations, so it's important to know what they are. On top of those basic questions, you can also use a tool called statistical testing to quantify how confident you should be. Statistical tests are mathematical methods of ensuring that differences are not caused by random chance. Sometimes this is called the significance of the results. For example, you're trying to improve an email campaign. Your model recommends a change that you implement, and you see a 5% increase in sales. Great, you just made the company 5% more money, right? Well, not so fast, it's possible that the increase in sales was random or because of some other factor. How can you know? You might run a statistical test and see that you should be 80% confident that the change in revenue was due to your new improved emails. It's then up to you or your organization to decide how confident you need to be to take action. Sometimes organizations want to be between 90 and 95% confident to take action, other times, they're happy with greater than 50%. It often depends on how risky it is for your business to be wrong. This is why statistical tests are so useful to businesses. So how do statistical tests work? To be honest, there's a lot of complicated math involved that we won't get into here. But there are some things they generally measure, including the differences in the averages of the datasets. If the averages are very different, the difference is less likely to be caused by randomness, the size of the dataset. The more data you have, the more confident you should be that the difference in averages isn't random, even if it's small. And the distributions of the datasets, this is often measured using standard deviation. A high standard deviation indicates high variability or that data values on average fall far from the mean. And a low standard deviation would mean that data in general is closer to the mean. If your data sets have high standard deviations, even large differences in their averages can simply be due to randomness. It's important to note here that none of these metrics in isolation provide a quantifiable measure of confidence. Only by combining them using statistical tests can you measure confidence.

The interpret phase of the awesome framework is crucial because it's where data driven insights are evaluated and communicated. You must revisit your initial analysis objectives, understand how the data answers your questions, and uncover any additional findings. Moreover, it's vital to ensure these insights are actionable within your business context. Tools like statistical testing play a key role, helping quantify your confidence in the results. And ultimately, the goal of the interpret phase and data analysis overall is not just to gain insights, but to make informed decisions that propel your business forward.

**Explain Your Findings**
Let's continue our discussion of the interprets stage of the OSEMN process. This is where all the data explorations and number crunching finally pay off. This is also where we explain our findings and generate concrete recommendations for our organizations. Now that we've explored the entire OSEMN process to understand our model, we want to make the hard numbers tell a story. It needs to be a story that anyone in our team, not just the data experts can understand and act upon. This is a crucial step because it's not just about having insights, it's about communicating them effectively. An essential aspect of effective data storytelling is choosing the right medium to communicate your findings, and there are many different mediums you could use. Some examples could be a slide presentation or an interactive notebook, or an in-depth report for an executive review. Each of these comes with its own strengths and challenges, and being able to adapt your story-telling approach to fit each medium is an important skill in data analytics. For the purpose of our discussion today, we will concentrate on slide presentations, but these lessons largely apply to any other medium you use. Slide presentations are universally relevant across industries and job roles and provide a structured, visual and engaging way to take your audience through your findings and recommendations. Your presentation should recap the original goal, review how you went through the steps of the OSEMN cycle, visualize critical data points, and importantly, explain your findings and recommendations. Let's talk about the key components of an effective slide presentation. Start your presentation by taking your audience back to the original problem that initiated your analysis. Re-introduce the issue at hand. What were we trying to solve? Why did we consider it necessary to undertake this analysis? Highlight the potential implications of not addressing this issue, illustrating why it was significant enough to warrant such an in-depth investigation. The purpose of this segment is to establish the context, helping your audience comprehend the relevance of the forthcoming findings and recommendations. Now that you've established the context, take the audience through the method you used. In this example you're going to take the audience through the steps of the OSEMN process that was followed. You want to maintain high-level overview, provide enough context for each step so the audience can understand the methods that lead to the insights. Remember, the goal here isn't to dive into highly technical details. You simply want to build a clear picture of the process leading up to your findings. If you're presenting to a more technical audience though, you might want to include more of the technical details. Now, guide your audience through a visual tour of the data, using aesthetically appealing and easy to understand visuals such as graphs, charts, and tables. You should try to encapsulate the core data points that lead to your findings. Make sure these visuals are accessible to a broad audience and designed in a way that even non data analysts can comprehend. The role of visualizations it's not just to display data, but to make the data speak for itself. Highlighting trends, anomalies, patterns, and correlations that underscore your findings. With visuals presented, it's time to describe them for your audience. This is where you act as the translator, decoding the visuals and turning data into a narrative. Elaborate on the key observations drawn from the data, explaining what they signify in the context of the original problem. Try to link the patterns and trends into visuals to the story you're trying to tell. Make sure your explanations are straightforward and relatable so that the audience can easily understand the story coming from the data. As you approach the end of your presentation, present your recommendations based on the findings, what actions should be taken to address the problem identified at the beginning of the presentation. If your data reveals a potential for improvement or modification in certain areas, outline those recommendations clearly. Discuss why you believe these steps would make a difference. Drawing connections between your findings and the recommended actions.

Let's imagine we've been analyzing a recent decline in web traffic for an e-commerce store. We start our presentation with a recap. We remind the audience about the initial problem; a significant drop in website traffic over the past three months, which we noticed during our routine metrics review. Next, reshare the OSEMN process. We obtained website analytics data, scrubbed it to ensure accuracy, then explore that to identify trends and anomalies. Our exploration led us to the modeling phase where we created a model that identifies potential costs. We then present a visualization, a graph illustrating the downward trend in website traffic alongside the increase in page load time, which we discovered was the primary cause for our model output. The clear inverse correlation visually substantiates our key finding. We can use the graph to explain what happens and when the problem started. As the page load time increased, the web traffic decreased significantly. We're able to tell the story of how our users started leaving our website because of longer low tides causing the website traffic to decline. And finally, we conclude with our recommendation. Given the correlation between page load time and traffic, we suggest optimizing the website's performance in an effort to reduce low time, which should help recover and possibly increase our website traffic. Thus, our data journey comes full circle, offering actionable insights that can be used to improve our business. Interpreting your findings and explaining them effectively to your audience is what turns data into action. It's the crucial final step in the OSEMN cycle, one that bridges the gap between raw data and real-world decisions, and that's the power of data analytics

**Storytelling with Data**

**The Power of Stories**

We can tell stories as a way to entertain, like when we read a mystery novel or go to a movie. But much more than to entertain, we can use stories as a way to teach and share knowledge. Even our novels and movies teaches things. They often inspire us to want to do better in our lives, and they warn us of things like the dangers of war. In this lesson, we'll explore how to use the power of storytelling to make sense of our data and create compelling stories from it. Picture yourself as a master storyteller. As a storyteller, your words create characters, plot twists and intense climaxes. Well, as a data analyst, you're also a storyteller, Adidas storyteller, and you're using data, visuals and a narrative to set up and build your story so that you can inform and persuade your audience. The narrative you create explains the origin of your data and its purpose. It provides a context to your findings and links the data to the answers we need. Using visuals as an aid, your story helps to bring the insights to life and presents a comprehensive picture in a way that's easy for everyone to grasp. Later in this lesson, we'll further explore how to build a narrative from your data and use visuals to craft a compelling and persuasive story for your audience. Let's get started.
