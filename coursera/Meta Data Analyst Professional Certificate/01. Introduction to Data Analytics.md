# Introduction to Data Analytics

- **Data Generation**: In our modern world, data generation is increasing rapidly due to various activities like social media posts, financial transactions, and interactions with technology.
- **Data as Insights**: The data generated reflects our behavior, habits, and preferences, providing valuable insights into our world.
- **Exponential Growth**: The amount of data is growing exponentially, thanks to advancements in technology and connected devices such as phones, smartwatches, car computers, and smart home systems.

**Importance of Data**

- **Decision Making**: Proper use of data can help make better decisions.
- **Need for Data Analysts**: There is a high demand for data analysts to make sense of the data and derive insights.

**Applications of Data Analytics**

- **Broad Applications**: Data analytics is essential for business owners, marketers, and individuals wanting to understand the world.
- **Simple to Complex Data**: Data can range from simple handwritten records to complex terabytes of data collected through the Internet and advanced machines.
- **Insights from Data**: When structured and analyzed correctly, even simple data can provide meaningful insights for better decision-making.

**Role of Data Analysts**

- **Data Collection**: Collecting relevant data.
- **Data Cleaning and Organizing**: Cleaning and organizing the data.
- **Data Analysis and Visualization**: Analyzing and visualizing data to uncover insights and inform decisions.

**Benefits of Understanding Data Analytics**

- **Informed Decisions**: Ability to make informed decisions.
- **Pattern Recognition**: Identifying patterns and trends.
- **Clear Communication**: Communicating data clearly and concisely.

**What is Data Analytics?**

- **Definition**: Data analytics is the process of collecting, cleaning, organizing, analyzing, and interpreting data to uncover insights and make informed decisions.

**Components of Data Analytics**

1. **Collecting Data**: Gathering data from various sources such as databases, spreadsheets, and online platforms. Data can come in different formats and structures.
2. **Cleaning Data**: Removing duplicates, inconsistencies, or errors to ensure data accuracy.
3. **Organizing Data**: Sorting and categorizing data into meaningful groups to identify trends and patterns.
4. **Analyzing Data**: Using statistical and mathematical methods to uncover insights and relationships within the data. This includes creating charts, graphs, and tables.
5. **Interpreting Data**: Presenting data in an understandable way through visualization techniques like charts, graphs, and dashboards. Storytelling is key to conveying the data's story and making decisions.

**Example: Anna's Clothing Boutique**

- **Collecting Data**: Anna collects sales data from her store and online platform using software that captures sale price, location, product details, and more.
- **Cleaning Data**: She finds inconsistencies in size information and standardizes sizes to ensure consistency.
- **Organizing Data**: Anna sorts and categorizes data to identify trends and patterns, such as which brands are most profitable.
- **Analyzing Data**: Using software with built-in statistical models, Anna analyzes sales trends to predict future demand.
- **Interpreting Data**: Anna creates charts and summaries to communicate her insights to her associates, helping them make informed decisions for the next season's inventory.

**Importance of Data Analytics**

- **Informed Decisions**: Enables better decision-making based on data.
- **Pattern Recognition**: Helps identify trends and patterns in data.
- **Clear Communication**: Facilitates the clear and concise presentation of data insights.

**Role of a Data Analyst**

- **Definition**: A data analyst is a professional who uses data analytics to uncover insights and make informed decisions.
- **Responsibilities**: Collecting, cleaning, organizing, analyzing, and visualizing data to provide valuable insights for organizations.

**Importance of Data Analysts**

- **Data-Driven Decisions**: Data analysts help organizations make data-driven decisions, impacting the bottom line.
- **Growing Demand**: The increasing availability and value of data make the role of data analysts more critical and in high demand.

**Typical Tasks of a Data Analyst**

1. **Collecting Data**: Gathering data from various sources such as databases, spreadsheets, and online platforms.
2. **Cleaning Data**: Removing duplicates, inconsistencies, and errors (data scrubbing).
3. **Organizing Data**: Sorting data into meaningful groups to identify trends and patterns.
4. **Analyzing Data**: Using statistical and mathematical methods to uncover insights and relationships within the data.
5. **Interpreting Data**: Visualizing data and telling a story based on the analysis. Creating presentations to communicate insights and findings to others in the organization.

**Applications of Data Analytics in Different Fields**

- **Healthcare**: Identifying trends in patient health data to make recommendations for improving patient outcomes. For example, a health insurance company might use data analytics to identify patients with asthma and suggest strategies like regular cardio exercise to improve their health.
- **Finance**: Understanding market trends and making informed investment decisions. Data analysts might analyze stock prices, economic indicators, and other financial data to help investors.
- **Marketing**: Understanding consumer behavior and preferences. Collecting and analyzing data on buying patterns, website traffic, and social media activity to develop targeted marketing campaigns. For instance, recommending a shift in advertising budget based on predicted effectiveness of different marketing channels.

**Career Opportunities**

- **Growing Demand**: There is an increasing demand for data analysts across various industries.
- **Career Growth**: Numerous opportunities for career growth and advancement in data analytics.
- **Industry Impact**: Data analysts play a critical role in solving real-world problems and making data-driven decisions.

**Data Analytics vs. Data Science**

As you learn about data, you'll encounter various data-related terms. Two often-confused terms are data analysts and data scientists. While both roles work with data to gather insights, their approaches and responsibilities differ.

**Data Analysts**

- **Role**: Work with structured data to identify patterns, build visualizations, and extract insights to aid decision-making.
- **Responsibilities**:
  - Maintaining databases
  - Interpreting data sets
  - Creating reports that present data trends, patterns, and predictions
  - Gathering data from various sources
  - Cleaning and organizing data
  - Presenting findings through visualizations
- **Skills and Tools**:
  - Foundational mathematics and statistics
  - Analytical thinking and data visualization
  - Basic fluency in R, Python, and SQL
  - Proficiency in SAS, Excel, and business intelligence software
- **Background**:
  - Experience in mathematics and statistics
  - Degrees in mathematics, statistics, computer science, or finance

**Data Scientists**

- **Role**: Work with both structured and unstructured data using advanced techniques like machine learning and predictive modeling to design processes, develop models, and extract insights.
- **Responsibilities**:
  - Arranging undefined datasets
  - Writing algorithms
  - Building automation systems and statistical models
  - Gathering and cleaning raw data
  - Creating data visualization tools, dashboards, and reports
  - Developing code to automate data collection and processing
- **Skills and Tools**:
  - Advanced statistics and predictive analytics
  - Machine learning and data modeling
  - High-level, object-oriented programming
  - Proficiency in Hadoop, MySQL, TensorFlow, and Spark
- **Background**:

  - Experience in computer science
  - Master's or doctoral degree in data science, information technology, mathematics, or statistics

  **Introduction to the OSEMN Framework**

  In your career as a data analyst, you'll encounter various data analysis projects with different data sources, methods, and business goals. A framework can help structure your approach, and the OSEMN framework is one such tool.

  **OSEMN Framework Overview**

  - **Definition**: The OSEMN framework, pronounced as "awesome," was described by Hilary Mason and Chris Wiggins. It stands for Obtain, Scrub, Explore, Model, and Interpret.
  - **Purpose**: Helps break down data analytics projects into manageable stages.

  **Stages of the OSEMN Framework**

  1. **Obtain**:
     - **Description**: Gathering data relevant to your business question. Sources can include internal data like sales records or external sources like government databases or financial market data.
     - **Considerations**: Sometimes obvious, sometimes requiring creativity and research. You may need to explore if the data exists or generate your own through interviews or observations.
  2. **Scrub**:
     - **Description**: Cleaning the data to make it usable. This involves correcting data formats and dealing with missing data points.
     - **Importance**: Proper data scrubbing helps avoid problems later in the analysis.
  3. **Explore**:
     - **Description**: Searching for patterns and statistics in the data. Categorizing and visualizing data with basic charts helps understand what the data is telling you.
     - **Role**: Acts as a detective phase, looking for trends, patterns, or anomalies.
  4. **Model**:
     - **Description**: Using statistical or mathematical methods to generate predictions and insights. This stage involves deeper analysis to understand patterns in the data.
     - **Application**: Many models are easy for data analysts to apply and can provide detailed insights.
  5. **Interpret**:
     - **Description**: Building a better understanding of the situation by visualizing data and creating stories and presentations.
     - **Goal**: Helps others understand the results of your analysis through clear communication.

  **Conclusion**

  - **Flexibility**: The OSEMN framework is simple and flexible, making it a great choice for data analytics projects.
  - **Future Learning**: The course will delve deeper into each stage of the OSEMN framework in the following weeks.

  The OSEMN framework helps structure data analysis projects effectively, ensuring each step is manageable and systematic.

  **Setting Business Goals for Analytics Projects**

  Before starting an analytics project, it's essential to understand the business goal related to your analytics task. A clear goal is crucial for planning and measuring progress.

  **SMART Goals**

  - **Specific**: The goal should be clear and understandable. Example: sell 100 products, get 200 new subscribers, or generate 20 qualified leads.
  - **Measurable**: The goal should be quantifiable. Example: number of sales, revenue, or website visitors.
  - **Achievable**: The goal should be realistic. If a goal is too ambitious, break it into smaller, achievable sub-goals or milestones.
  - **Relevant**: The goal should align with the broader business objectives and strategy.
  - **Time-bound**: The goal should have a start and end date to track progress and evaluate outcomes.

  **Example of a SMART Goal**

  - **Scenario**: A business launching its website to transition from physical store success to online sales.
  - **Goal**: Get 10,000 website visits during the month of May.
    - **Specific**: Clearly states the objective.
    - **Measurable**: Can track the number of visits using website analytics.
    - **Achievable**: Challenging but realistic given the business's physical store success.
    - **Relevant**: Supports the intention to sell products online.
    - **Time-bound**: Deadline set for the end of May.

  **Formulating Objectives for Analysis**

  - **Defining the Objective**: Once a SMART goal is set, it's easier to formulate the objective for your analysis, making it clear what you are contributing to.
  - **Determining Questions**: Identify the questions to answer to achieve the goal. Answering these questions becomes the analysis objective.
  - **Evaluating Goals**: Often, analysis centers around evaluating whether a goal was achieved.
  - **Isolating Metrics**: Identify key performance indicators (KPIs) that best indicate whether the goal was reached.

  **Next Steps**

  - **KPIs**: Later in the course, there will be a detailed explanation of KPIs and how to use them.

  A clear understanding of business goals and the formulation of SMART goals are fundamental steps in ensuring the success of your analytics projects.

  **Key Performance Indicators (KPIs)**

  Before diving deeper into using the OSEMN framework, it's important to understand Key Performance Indicators (KPIs). KPIs are measurable values that help track progress toward a goal.

  **Example of a SMART Goal and KPI**

  - **SMART Goal**: "By the end of this year, I want to run the Honolulu Marathon in under four hours."
    - **Specific**: Running the Honolulu Marathon.
    - **Measurable**: Completing the marathon (26.2 miles) in under four hours.
    - **Achievable**: Realistic with proper training.
    - **Relevant**: Relates to overall fitness focus.
    - **Time-bound**: Deadline is December.
  - **KPI Example**: Using Strava to track running progress.
    - **Important Metric**: Pace (11 minutes and 49 seconds per mile).
    - **Goal**: Run at approximately nine minutes per mile to complete the marathon in under four hours.
    - **Pace**: A key metric to assess progress.

  **Characteristics of KPIs**

  - **Quantitative**: KPIs are measurable.
  - **Directional**: KPIs increase or decrease over time.
  - **Direct Relationship**: KPIs should have a direct connection to the goal.

  **Determining KPIs**

  - **Example**: Calla & Ivy, a flower business in Amsterdam.
    - **SMART Goal**: "Get 10,000 website visits during the month of May."
    - **Primary KPI**: Number of monthly website visitors.
      - **Measurable**: Number of visitors is quantifiable.
      - **Directional**: Visitor count can increase or decrease.
      - **Direct Relationship**: Directly related to the goal of increasing website traffic.
    - **Secondary KPIs**: Metrics like brand awareness, number of pages viewed, average time spent per visit, and number of transactions. These are related but do not directly measure the success of the primary goal.

  **Importance of KPIs**

  - **Prevent Overwhelm**: With a lot of data available, KPIs help focus on what's important.
  - **Set Ahead of Time**: Setting KPIs related to the goal ahead of time protects against getting lost in the data.
  - **Ready for Analysis**: With goals and KPIs set, you're prepared to start your analysis.

  In the following sections of the course, you will see examples of how to apply the OSEMN framework using KPIs to ensure effective data analysis.

  **Using the OSEMN Framework in Practice**

  Reflecting on SMART goals and understanding KPIs is crucial for setting objectives for your analytics project. The OSEMN framework can make a significant difference in achieving accurate results.

  **Example: Calla & Ivy's Marketing Campaign**

  - **Business Goal**: Imra, the owner, aims to get 10,000 website visits during May through a social media advertising campaign.
  - **Campaign Plan**: $5,000 budget for Instagram ads, daily Instagram stories, and TikTok videos.

  **OSEMN Framework Steps**

  1. **Obtain**:
     - **Data Sources**:
       - Google Analytics 360 for website visit data.
       - Meta's Ads Manager for Instagram ad performance.
       - Instagram and TikTok accounts for stories and video performance.
     - **Data Collection**: Download data from January to May, covering website visits, ad performance, and social media engagement.
  2. **Scrub**:
     - **Cleaning Data**:
       - Add clear labels to data.
       - Remove rows with broken URLs.
       - Reformat dates to ensure consistency across data sources.
     - **Preparation**: Ensure data is accurate and ready for analysis.
  3. **Explore**:
     - **Trend Analysis**: Create a chart showing website visits by month.
     - **KPI Achievement**: Verify that the campaign exceeded the goal with 13,457 visits in May.
     - **Marketing Activity Effectiveness**:
       - Analyze clicks from Instagram ads, TikTok videos, and Instagram stories.
       - Identify that TikTok videos generated significant clicks at a lower cost.
     - **New Insights**: Consider shifting some budget to TikTok ads.
  4. **Model**:
     - **Prediction**: Build a statistical model with the help of a teacher to simulate reallocating the budget ($3,000 on Instagram ads and $2,000 on TikTok ads).
     - **Result**: Predict a 35% increase in website visits for June.
  5. **Interpret**:
     - **Presentation**:
       - Report KPI trends and marketing performance.
       - Highlight the success of TikTok videos and potential benefits of reallocating budget.
       - Explain model predictions for increased website visits.
     - **Impact**: Imra decides to advertise on TikTok based on Sheila's data-driven insights.

  **Conclusion**

  - **Success**: Sheila's work demonstrates the practical application of the OSEMN framework and influences decision-making at Calla & Ivy.
  - **Future Learning**: The course will delve deeper into each OSEMN step, equipping you with the tools to tackle various data analytics challenges.

  **Introduction: Obtaining and Scrubbing Data**

  In the second week, the focus is on obtaining and scrubbing data, which are the first two steps of the OSEMN framework. This week covers where to find data and how to clean it for analysis.

  **Obtaining Data**

  - **Types of Data Sources**:
    - **Freely Accessible, Open-Source Databases**: Examples include Eurostat, OECD, World Bank, United Nations, and US Census Bureau. These sources offer a wealth of reliable information on various socioeconomic topics.
    - **Company-Specific Data**: This includes internal data like sales records or data collected using software like Google Analytics. This data is not publicly accessible and is often proprietary.
    - **Intentionally Collected Data**: When required data isn't available, it can be collected through surveys, interviews, or observations using tools like SurveyMonkey or Google Forms.
  - **Data Formats**:
    - **Numeric Data**: Quantitative data stored in tables or spreadsheets, often in .CSV format.
    - **Text Data**: Unstructured data from sources like social media posts, emails, and customer reviews. Analyzed using natural language processing (NLP).
    - **Visual Data**: Data in visual formats like images and videos, useful for quality control, machine learning in self-driving cars, and medical research.
  - **Sampled Data**:
    - Used when analyzing the entire population is impractical.
    - Important considerations include sample size, representativeness, and generalizability.
  - **First vs. Third Party Data**:
    - **First Party Data**: Collected directly by a business from its customers or internal sources.
    - **Third Party Data**: Collected by outside parties, such as market research firms or government agencies.

  **Evaluating Data Quality**

  - **Source Credibility**: Check the authorship, credentials, and publication date.
  - **Methodology**: Ensure the sample size is adequate and the sampling method is unbiased.
  - **Objectivity**: Avoid data with biases or conflicts of interest.
  - **Accuracy**: Cross-check data with other reputable sources and look for obvious errors.
  - **Relevance**: Ensure data is relevant to the question at hand and presented in a meaningful context.

  **Checklist Example**

  - **Example**: Evaluating data on climate change effects on polar ice caps.
    - Check source credibility: Verify the credentials and affiliations.
    - Check methodology: Ensure sample size and sampling techniques are clear.
    - Check objectivity: Look for biases or conflicts of interest.
    - Check accuracy: Compare with data from reputable sources.
    - Check relevance: Ensure data fits the research scope and is well-documented.

  **Free Data Sources**

  - **Google Public Dataset Search**: Access to millions of datasets on public websites.
  - **United States Census Bureau**: Data on US population, economy, and geography.
  - **Pew Research Center**: Insights on social, political, and technological issues.
  - **Eurostat**: Economic, social, and environmental data from the European Union.
  - **OECD**: Comparative data on global economic and social matters.
  - **Kaggle Datasets**: Public datasets from various industries.
  - **National Centers for Environmental Information (NCEI)**: Data on climate change and environmental information.
  - **World Bank Open Data**: Global indicators on economic, social, and environmental trends.

  By understanding these aspects of data collection and evaluation, you will be better equipped to obtain and scrub data effectively for your analytics projects.

  **Introduction: Obtaining and Scrubbing Data**

  In this section, we focus on the first two steps of the OSEMN framework: obtaining and scrubbing data.

  **Obtaining Data**

  - **Overview**: The obtain phase involves gathering data from various sources for analysis.
  - **Data Sources**:
    - **Freely Accessible, Open-Source Databases**: Examples include Eurostat, OECD, World Bank, United Nations, and US Census Bureau.
    - **Company-Specific Data**: Internal data such as sales records or data collected using tools like Google Analytics.
    - **Intentionally Collected Data**: Data gathered through surveys, interviews, or observations using tools like SurveyMonkey or Google Forms.
  - **Data Formats**:
    - **Numeric Data**: Quantitative data stored in tables or spreadsheets, often in .CSV format.
    - **Text Data**: Unstructured data from sources like social media posts, emails, and customer reviews.
    - **Visual Data**: Data in visual formats like images and videos, useful for various analyses.
  - **Sampled Data**:
    - Used when analyzing the entire population is impractical.
    - Considerations include sample size, representativeness, and generalizability.
  - **First vs. Third Party Data**:
    - **First Party Data**: Collected directly by a business.
    - **Third Party Data**: Collected by outside parties, such as market research firms or government agencies.

  **Scrubbing Data**

  - **Importance**: Cleaning data is essential to ensure it's usable for analysis and to avoid errors that could lead to incorrect conclusions.
  - **Four Main Tasks**:
    1. **Removing Duplicates**: Identifying and eliminating duplicate records to ensure each record is unique.
    2. **Formatting Records**: Ensuring consistent data formatting to avoid confusion and errors in analysis.
    3. **Solving for Missing Values**: Addressing missing values by either filling them in with placeholders (like "unknown" or "N/A") or removing records with crucial missing information.
    4. **Checking for Mistakes or Wrong Values**: Identifying and correcting obviously wrong values, such as unrealistic numbers or errors.

  **Example: Calla & Ivy's Marketing Campaign**

  - **Business Goal**: Imra, the owner, aims to get 10,000 website visits during May through a social media advertising campaign.
  - **Campaign Plan**: $5,000 budget for Instagram ads, daily Instagram stories, and TikTok videos.
  - **Data Sources**:
    - Google Analytics 360 for website visit data.
    - Meta's Ads Manager for Instagram ad performance.
    - Instagram and TikTok accounts for stories and video performance.

  **Example: Inu and Neko's Subscription Service**

  - **Objective**: Carlos wants to launch a subscription meal service for cats and dogs with a goal of 500 subscribers in the first year.
  - **Data Source**: E-commerce sales data from the company's website.
  - **Scrubbing Process**:
    1. **Remove Duplicates**: No duplicates found in the dataset.
    2. **Format Records**: Standardize ZIP codes and remove unnecessary columns (phone numbers and size).
    3. **Solve for Missing Values**: Fill missing sales totals and remove records with missing customer IDs.
    4. **Check for Mistakes**: Remove entries with obviously incorrect product prices.

  **Conclusion**

  By obtaining and scrubbing data effectively, you ensure that your dataset is clean and ready for analysis. This prepares you for the next steps in the OSEMN framework: exploring, modeling, and interpreting your data.

  **Exploring and Modeling Data**

  In the third week, we focus on the exploring and modeling phases of the OSEMN framework. These stages help you uncover patterns and predict future outcomes using your data.

  **Exploring Data**

  - **Overview**: The explore phase is about getting familiar with the data, identifying patterns using statistics and visualizations.
  - **Iterative Process**: Revisiting earlier stages (obtain and scrub) is normal as you discover more about your data.

  **Steps in Data Exploration**

  1. **Understanding Your Data**:
     - **Data Sources**: Identify and combine data from different sources.
     - **File Sizes**: Note the size of each file to gauge the amount of data.
     - **Data Structure**: Count rows and columns to understand the dataset's structure.
  2. **Data Types**:
     - **Numerical Data**: Quantitative data expressed in numbers.
     - **Categorical Data**: Data that falls into distinct groups.
  3. **Summary Statistics**:
     - **Categorical Data**: Count the frequency of each category.
     - **Numerical Data**: Calculate minimum, maximum, median, mode, mean, and standard deviation.
     - **Raw Data Review**: Look at the first and last few rows or take a random sample.
  4. **Data Visualizations**:
     - **Importance**: Visualizations help understand complex data and communicate insights effectively.
     - **Key Elements**: Titles, axes, legends, labels, and visual attributes.
     - **Types of Visualizations**: Bar charts (compare categories), line charts (show trends over time), scatter plots (show relationships between variables).
  5. **Data Distributions**:
     - **Histograms**: Visualize numerical data by grouping values into bins.
     - **Common Distributions**: Normal, bimodal, log-normal, exponential, and uniform.
     - **Insights**: Identify minimum, maximum, mode, and standard deviation from histograms.
  6. **Variable Relationships**:
     - **Correlation**: Measure the relationship between variables using scatter plots and line charts.
     - **Correlation Coefficients**: Quantify correlation (ranging from -1 to 1).
     - **Correlation vs. Causation**: Distinguish between correlation (relationship) and causation (one variable causes the other).
  7. **Feature Engineering**:
     - **Definition**: Creating new features or modifying existing ones to better understand data.
     - **Techniques**: Extracting information from dates, creating calculated features, and categorizing data.
     - **Domain Expertise**: Understanding your field to identify valuable features.

  **Explore Checklist**

  1. **Inspect Your Data**:
     - Read through your data for interesting information.
     - Use summary statistics to evaluate your data.
     - Inspect a random sample if the dataset is too large.
  2. **Visualizing Data**:
     - Use bar charts, line charts, and scatter plots to examine hidden information.
  3. **Examine Variable Distributions**:
     - Categorize and plot the data.
     - Evaluate minimum, maximum, mode, and standard deviation.
  4. **Examine Variable Relationships**:
     - Visualize variables to understand their correlation.
     - Calculate the correlation coefficient.
  5. **Feature Engineering**:
     - Create new features or modify existing ones for better insights.

  By exploring your data using these steps, you will gain a deeper understanding and be prepared to move on to the modeling phase, where you can make predictions and derive actionable insights.

  **Modeling Data**

  **Make Predictions from Your Data with Modeling**

  So far, we've gone over the first three stages of the awesome cycle. We've obtained the data we needed, scrubbed it, and explored it thoroughly. Now, we're ready for stage 4 "Modeling". This phase is about using our data to make predictions with mathematical models. These models can be anything from simple linear regressions to advanced machine learning algorithms depending on the project. Although there are many different models, they all work by discovering hidden patterns in data and using it to make predictions on any new data we give the model. For example, you might build a model to predict how many conversions you expect a campaign to deliver. You would do that by using data from the past to predict the future. Our discussion of modeling is going to be broken down into the following 3 sections: What are models, how the models work, and the types of models? All of the steps of the OSEMN process are important, but modeling is a central piece of data analysis. This is the stage when we start getting answers to our key questions. So let's get started!

**What Are Models and Why Use Them?**

You might not know it, but we live in a world full of models. In marketing, for instance, models are used for a number of different tasks, including projecting sales, predicting whether an individual is likely to buy a product, splitting audiences into subgroups, and finding new potential customers. In fact, it's hard to find an area where models aren't used. You'll hear about economic predictions and weather forecasts if you listen to the news. If you go on your favorite social media apps, your feed is full of content that you might like, and you're shown advertisements based on the groups you are perceived to be in. These are all examples of models. So what are models? Models are mathematical tools that can be used to recognize hidden patterns in data that often aren't obvious to humans. Models can be used to try and apply those patterns to different data. Their most common use is making predictions about the future based on data from the past. Creating some models can be really simple. For instance, calculating the average of a set of data and then predicting our future data will be average is a simple model. It's often not a very good one, but it is a model. Creating models can also be very complex, like machine learning models that try and predict stock prices, or models that generate amazing text and images. The good news is that you often don't need to know how to build them to use them! Analysts will often work very closely with a team of data scientists or with a specialized firm that builds the models for them. So, we'll focus on understanding how to use them, so that you can pick the right model for the job and use it correctly in your work as a data analyst. Let's look at a type of model you learn to use called a Linear Regression. Linear regression sounds fancy, but it just means a line that can be used to predict the value of one variable using the value of another variable. This particular linear regression predicts the price of a house based on the square footage. The line and shaded region represents our model and its predictions, while the dots are the examples the model examined from the data set we trained it on. In other words, the dots represent the data we gave to the machine to learn more about house prices, and the machine then used that learning to make predictions about other prices. This model gives price predictions for a given amount of square footage. For instance, to get the prediction for a 3000-square-foot house, we find the x-coordinate (Square feet) and move up until we hit that prediction line. Then we take the Y coordinate on the line as the corresponding prediction! We can see that the model predicts that the higher the square footage, the higher the price of the house-- this makes sense. Remember that I mentioned predictions will not be perfectly accurate. This is what the shaded area represents-- the margin of error with 95% confidence, meaning that we're 95% sure that the actual value will land somewhere in the shaded area. The ability to quantify uncertainty like this is what allows us to make smart business decisions about the predictions our models make and how much we should trust a given prediction.

As the statistician, George Box famously said, "All models are wrong, but some are useful." We must understand that the predictions our models make won't be perfectly accurate. However, with the right data, they'll be accurate enough to generate insights that have business value. The predictions that models make are never 100% certain, but importantly, we can quantify the uncertainty for each model. This allows us to understand how far off the average prediction might be.

​
**How Do Models Work?**

Now that we know what models are, let's dive deeper into how they actually work. Remember, models are mathematical tools that can be used to recognize hidden patterns in data that often aren't obvious to humans. But how do we get from a set of raw data to a useful model that can make predictions? It all starts with model training. The basic idea is that we feed in a set of known data and use an algorithm to create a mathematical representation of the relationship between the input and output variables we're trying to predict. You can think of an algorithm as a recipe. It describes a set of procedures to be carried out. The algorithms used by models examine data sets and use math to learn the underlying relationships in the data. In the case of linear regression, an algorithm is used to create the equation for a line that approximates the input data. If you've taken algebra, you might remember the equation y is mx+b. This is the general equation for a line. Y is the output of the equation, x is the input value, and m is the slope of the line, and b is the y intercept. The algorithm creates a linear equation with m and b values that best fit the input data. You can then put any x value into the equation and predict y. Essentially, all models attempt to do something similar. They mathematically encode a relationship. In linear regression, it's the m and b values. In a more complicated model, like a neural network, the model's algorithm might be determining billions of value in a complex network of relationships. In practice, different models will require the data to be in a specific format. For example, some algorithms will require all of your data to be numerical. Other algorithms can't handle missing values. It's up to you to manipulate the data before applying an algorithm. This is one of the reasons why the scrubbing stage of the OSEMN process is so important. Another key feature of models is what type of data they output. There are special names for different models based on the type of data they output. We will discuss this more when we talk about the types of models. Many models also need to go through a training and testing phase. The training phase is what we just discussed, where the model attempts to learn a relationship. The testing phase is where an analyst uses a second dataset that the model was not trained on to check and make sure that the model did not learn something that was only true for the training data. If a model returns good results for another dataset, its results are said to be generalizable to that data. The testing data is fed into the model and the algorithm predicts the output variable for each data point. We can then compare these predictions to the actual output values we know from the testing data. This allows us to measure how well the model can make accurate predictions. If you look at our linear regression model from before, we can see that the model fits very well for the blue data the model was trained on, but does not fit very well with the red data we tested it with. In this case, its because we trained on data from one city and test it on data from another city. Apparently, housing prices can be quite different between cities. This leads us to an important lesson about models. Always make sure that your model is trained on data that is as similar as possible to the business case you care about. In this case, we only care about predicting housing prices in the blue city, so our model will work just fine. It's often best practice to split a single dataset into testing and training data to ensure the data is similar. Sometimes it makes sense to split the data randomly, but often it's best to train on older data and test against the most recent data if the data is time-based.

Generating models can be an art as much as it is a science, so practice is key to learning to make good models. Keep up the great work. I'll see you in the next video.

**Different Types of Models**

Models are fundamental to understanding and interpreting patterns within data, and they can be used across many fields. So far, we've discussed basic models, such as linear regression, but let's look deeper at the types of models you might encounter. A good place to start discussing models is, what types of questions do you want your model to be able to answer? Regression models answer questions that start with how much, or how many. They're used for answering questions that are numerical in nature. As an example in marketing, they're often used for forecasting numerical metrics like sales and click-through rates. As another example, regression models could be used in finance to predict the price of a stock based on various factors like the company's earnings, the state of the economy, and historical trends. Classification models predict the group or "classes" as answers. The data they output is categorical. These classes can be binary, like in "True" or "False". Or they can be one of many in a multiclass problem, like when predicting if an image is of a dog, cat, or human. For instance, in healthcare, classification models can help identify what disease a patient might have based on their symptoms. They can be used in marketing to answer questions like, is this a customer that's going to leave me for a competitor or not? Clustering algorithms split data into groups or segments with similar characteristics. Clustering algorithms are often used to make a customer base and divide them into smaller niche audiences, which can lead to more focused advertising for each of these specialized groups. In sociology, clustering algorithms are used to group people based on common attributes or behaviors. Using a clustering algorithm on customer sales data might reveal that most of the customer base comes from distinct segments that we can translate into marketing insights. For example, a clustering algorithm could show distinct customer segments, like "sports fans aged 18 to 25," or "new mothers in urban communities." Each model is defined by its algorithm, each having its own strengths and weaknesses. Although there are many algorithms, let's focus on three prominent ones, linear regression, decision trees, and neural networks. Each of these models brings a different approach to data analysis, suited for different complexities and types of problems. Linear regression, a form of a regression model, identifies linear relationships between input and output variables. They offer a simple, less data-intensive model ideal for simpler problems. You often don't need a large dataset to perform linear regressions, but the trade-off is they only provide answers to simple questions. Decision trees are models that use a tree-like structure to segment data based on a series of binary decisions. For instance, in credit scoring, decision trees might be used to determine whether an individual is likely to default on a loan based on factors such as income, employment status, and credit history. Individual trees are usually combined into a model known as a random forest. These models are probably best known as classification models, but can also be used to create regression models. Decision trees are well suited to help with a wide range of analysis problems and tend to require less computing power than neural networks. Neural networks use complex networks of "neurons" to make predictions. They're great at learning complex relationships involving numerous variables and patterns. They're used in diverse fields like real estate, autonomous driving, and art generation, and often require substantial data and computing power, but the results can be very useful.

The key to success is selecting the right model for the problem you're trying to solve. Some models are more effective than others when working with certain types of problems. Understanding the strengths and limitations of each type of model will help guide you in making the best choice for your specific project. It's important to understand that models primarily help us to analyze data so that we can interpret it and predict what might happen in the future. We will go into this in more detail later. See you then.

**Example: Exploring and Modeling Data**

**A Real World Example**

In previous videos, we followed Keira, a data analysts working with Carlos, the owner of Inu and Neko, a dog and cat care company. Carlos had approached Keira to help and launch a new subscription meal service for cats and dogs and wanted to select the 10 best products to offer as part of the subscription. Keira started by obtaining the necessary data from their e-commerce software. She downloaded last year's sales data into Google Sheets to analyze which products were most popular and purchased repeatedly. She obtained the data properly by making sure it was credible, collected accurately, objective, accurate, and relevant to Carlos' questions. Keira proceeded to scrub the dataset, checking for duplicates, inconsistent formatting in the zip codes, and missing values like phone numbers and sales totals. She standardized zip codes for consistency and remove the phone number column because it was not crucial for analysis. Keira filled in missing sales stores by calculating product price multiplied by quantity. Some records lacked customer IDs, preventing the assessment of repeat buyers. So Keira deemed them unhelpful and removed them from the dataset. Lastly, she identified inaccurate information such as negative sales amounts and unusually high prices, recognizing them as glitches in Inu+Neko’s system and excluded them from consideration to maintain data accuracy for future analysis. Now that Keira has completed scrubbing the data successfully while addressing duplicates, inconsistencies, proportionate gaps, and inaccuracies, she's ready to dive into our next topic; explore and model. In the explore and model stages of the OSEMN Framework, Kiera is now tasked with uncovering patterns and trends within the data and creating a model to predict subscription bundle preferences for Inu and Neko's customers. During the explorer stage, Keira will perform various exploratory data analysis techniques to gain insights. This might involve using charts and visualizations to identify correlations between different product categories or demographic segments. She'll also dive deeper into customer behavior by analyzing purchase frequency, average basket sizes, and seasonal trends. Once Keira has explored the data thoroughly. She can move on to modeling. Modeling involves using algorithms to create a predictive model based on historical data. Keira will develop models to help Carlos meet his goal of hitting 500 subscriptions. In the upcoming videos, we'll follow Keira through the explore and model stages of the OSEMN Framework! Let's get started!

**Case Study - Exploring Data**

Kira now has a clean set of data that she considers relevant for the question she got from Carlos at Inu+Neko: Which products should he include in the new dog and cat food subscription product to help him reach 500 subscribers by the end of the year? Kira gets started with Step 3 of the OSEMN cycle, she starts to explore the data. She thinks that a good place to start is to see what types of data are in each column. She sees that she has both categorical and numerical data. There's a date column and text columns like order numbers and customer IDs. She notices that the order number and customer ID columns are very long and take up a lot of screen space and are hard to type. So, she decides to map each unique value to a number in a process called encoding. Encoding is the process of turning a string of data into numerical data by mapping each unique string to a unique number. Encoding can be used to reduce the amount of memory needed to work with data, or to make large, complex strings easier to work with. Can also be used to turn text data into numbers when a model needs numerical inputs instead of text inputs. In this example, Kira's using encoding to make the data easier to view and understand. She also notices that there are quite a few columns that contain redundant information. A column is redundant if you could look at one column and always guess what's in the other column. In this case, the customer ID and name are redundant. They might not always be, but for this product she knows it's safe to assume, so she hides customer name. She also notices that SKU and product name are redundant as well, so she hides the SKU. She also decides that it is unlikely that this project will look at data at the street address level, so she only keeps the state and zip code. Great, now she can see all of the useful columns at a glance. And not having a large text columns even helps her computer run more smoothly. Next, she decided to run some summary statistics for the remaining columns. She looks at things like the most common value, how many times it appears, and what percentage of the rows it shows up in. She also looks at the minimum, maximum, range, and mean of the numerical values. From this, she sees that the data spans 899 days from the spring of 2019 through the summer of 2021. She sees that Texas is the most popular state, which makes sense because it is quite populous. There are also some numerical columns that she thinks could come in handy, like price and quantity. Kira decides to add up the total sales for each product and use a bar chart to look at the distribution of these total sales. This gives her exactly what she was looking for, the top selling products. She can also see from this chart that cat products tend to sell more than dog products. She realizes that it's great to know what top selling products are, but it might also be important to know what causes those products to sell more. To answer that question, she creates a few scatter plots to observe the relationships between the quantities sold and different variables. One relationship that stands out to her is quantity sold and price. She notices that if she looks at just cat or dog products in isolation, the quantity sold is low for low and high-priced items, but high for items in the middle. This means that the two variables have a positive correlation for low prices, and a negative correlation for high values. This is great information to pass on to Carlos. Maybe adding more medium-priced items will help him find additional products for his subscription service. Kira now knows the top selling products and a possible reason for why those products are the top sellers. Kira's ready to move on to the next stage in her analysis, the model stage.

**Case Study - Modeling Data**

Kira is making her way through the OSEMN cycle. She has obtained the right data, scrubbed the data, and she learned quite a bit in the exploration stage to help Carlos at Inu+Neko decide on the product mix for the new subscription service. Now, it's time to create a model. Kira wants to create a model so she can use the data from historical purchases to predict the future. Of course, there are a few complexities here. The historical data doesn't show any information on subscriptions, because the subscription model doesn't exist yet. But Kira believes that it is fair to assume that if a subscription model exists, the people who buy almost every month are likely to subscribe. She doesn't know exactly how likely though, so she decides to play it safe and assume that 50% of the people who buy eight or more times a year will subscribe. Kira talks to her friend who is a data scientist. She tells him that, from her exploration of the data, she learned that this is the ranking of the products among repeat buyers. She thinks that the top five cat and dog products in this list should be the ten items included in the package. She refers to this package as "Package A". She also tells him that she thinks there's a 50% chance that a repeat buyer would become a subscriber. Her friend says that using this information he can create a custom algorithm to train a regression model that will predict the number of subscriptions Kira can expect in a year. At this point, she doesn't have to understand the details of the model, but it basically finds every repeat buyer in the database that bought one of the ten products in the proposed package. And then assumes that 50% of them will become subscribers. The model shows that if we use the 10- product mix Kira suggested in Package A, we can expect 475 subscriptions. Kira knows that isn't quite enough to reach Carlos's goal of 500 subscribers. She knows that Carlos told her that he wants to keep the products offered in the subscription limited to 10 products, but she's curious to see what would happen if she put 12 products in the package instead. So she asks her friend to run the model again, but now with package B, which includes the top six dog and cat items purchased by repeat buyers. After they run the model, they find that the estimated number of subscribers for the year with package B is 517. That helps to reach Carlos's goal! Kira is happy with the predictions she has from the model based on the different packages. She isn't sure whether Carlos will be interested in adding two more products to the package, but at least she can now provide him with a convincing argument for why he might want to consider it.

**Weekly Conclusion**

That brings us to the end of another week in this course. You're doing great. We have now covered four of the five steps in the OSEMN framework, and you're well on your way to becoming a data analyst! You know how to explore your data by examining data distributions and relationships. You also know the different types of models and how they can be used to make predictions from your data. And you've seen the explore and model phases applied to a real-world example. Now, we will move on to the final step of the OSEMN framework INterpreting the data. See you there!

**Interpreting Data**
**Introduction: Interpreting Data**

Welcome to the last week of our Intro to Data Analytics course, great work so far. You now know what's involved in obtaining and scrubbing data, as well as exploring and modeling it. So this week, we will focus on the last step in the awesome Framework, interpreting your analysis. Generating insights from your data is very powerful, and giving an interpretation to those findings, is where you can help the data tell a story. I think this is where a lot of the magic happens. Telling a story and convincing other people of your points is great. But if you can do that, backed up with data, it just makes you feel more powerful. This week, we'll highlight some of the ways in which you can do this. In the first lesson, we will take a look at what's involved in this interpretation phase, and how you can approach drawing conclusions from your analysis. Then in the second lesson, we will focus on how you can let your data tell a compelling story, a skill that you'll find very useful as a data analyst. And after that, we'll put all we learned about applying the awesome Framework together, in a real life example. We'll hear from Jules on how he used the awesome Framework, for a research project, to try and influence policymakers. You're in the final stretch for this course, so let's get right to it.

**Answer Your Business Question with Your Data**

The interprets stage of the OSEMN process is where everything comes together. So far, you've seen the first four stages of the OSEMN framework, obtain, scrub, explore and model. Each stage gives you a better understanding of the business problem you're trying to solve. For example, you could be predicting the number of responses for an email campaign or you could be trying to find new potential customers to grow the business. In either case, you start with the obtain stage where you collect the data to answer the question. Then you move on to the scrubs stage where you clean the data. Remember cleaning data includes removing duplicates, handling missing values, and ensuring the data has a consistent format. After scrubbing, you then explore the data. When exploring, you'll apply statistics to find interesting patterns and trends. For example, you might notice that some email campaigns have higher response rates than others, and after exploring, you'll apply models to the data. These models will allow you to generate predictions. For example, based on past email campaigns, how many responses can be we expect for this campaign? Models also give you more insights. A model that predicts email response rates will also tell you what makes it more likely for people to respond to emails. Knowing what influences people to respond can help improve marketing campaigns. The last stage of the OSEMN process is interpret. The interpret stage is where you interpret your analysis. It's arguably the most important. Without it, all we would have is data and statistics. The interprets stage translates your analytical findings back to a business context. After successful modeling stage, you'll have a new tool like a regression model that can be used to generate predictions. The answers generated from these sorts of models are very specific and usually aren't immediately interpretable or understandable by non-technical team members. During the interprets stage, our goal is to close the loop of the OSEMN cycle by using the models and insights we generated during the exploration and modeling phases to try and answer the business question driving the entire project. In other words here you look back at your objective for your analysis. Your goal here is twofold. First and foremost you need to understand the results of your model and all the insights it can provide. These might be the actual predictions the model makes, like forecasting the results of sales from a campaign or they might be information contained within your model like an insight that shows that mailing lists sign-ups are strong predictors of people spending more money with your company. Second, you need to be able to explain your findings to a non-technical audience in a clear concise way. Simply understanding the implications of your model isn't enough. You need to be able to make others understand it and trust your results. Remember, analytics projects are about generating actionable insights or information that can be used to make better decisions that help the company.

**Understand the Results of Your Model**

The first thing you'll do during the interpret stage is to try and answer some basic questions. Some questions to ask yourself in the interpret stage include, what was the objective of this analysis? It's important to go back to your starting point because that will remind you of the questions you set out to answer. It's quite easy to get lost in the data during the model and explore stages and lose sight of your initial question. Then ask yourself, how does the data answer my questions? Maybe the data shows you that the business goal you set is currently unattainable, or maybe it gives you a plan that you could use to move forward. Another question to ask is, what other learnings do I have? In the process of answering one business question, you will often find new pieces of potentially useful information that help solve the problem at hand in a different way. Or maybe they open up new potential business objectives you can address in later analysis. How can I apply this to a business context? Gaining new knowledge is great, but it is important to focus on information that's actionable and moves your business forward in meaningful ways. It will often be someone else that takes action based on your information, so think about how that will happen. Perhaps the most important question to ask is, how confident should I be in my results? If you see an improvement in a business metric, was it due to the changes you made or was it due to random chance? Many data analysts are overconfident in their results, and when they implement what they have learned, they quickly discover that something was wrong with their analysis. That brings us to the topic of, how do you know if you should be confident in the results of your model? Earlier, during the modeling stage, we briefly discussed using a separate set of test data to check your trained model against. The testing process is all about ensuring you have the right amount of confidence in your model. By running your test data through your model, you can answer questions like, how wrong is the model on average? If the model predicts something, how likely is it to be correct or incorrect? Are there particular scenarios that cause the model to be incorrect? Even the best models have limitations, so it's important to know what they are. On top of those basic questions, you can also use a tool called statistical testing to quantify how confident you should be. Statistical tests are mathematical methods of ensuring that differences are not caused by random chance. Sometimes this is called the significance of the results. For example, you're trying to improve an email campaign. Your model recommends a change that you implement, and you see a 5% increase in sales. Great, you just made the company 5% more money, right? Well, not so fast, it's possible that the increase in sales was random or because of some other factor. How can you know? You might run a statistical test and see that you should be 80% confident that the change in revenue was due to your new improved emails. It's then up to you or your organization to decide how confident you need to be to take action. Sometimes organizations want to be between 90 and 95% confident to take action, other times, they're happy with greater than 50%. It often depends on how risky it is for your business to be wrong. This is why statistical tests are so useful to businesses. So how do statistical tests work? To be honest, there's a lot of complicated math involved that we won't get into here. But there are some things they generally measure, including the differences in the averages of the datasets. If the averages are very different, the difference is less likely to be caused by randomness, the size of the dataset. The more data you have, the more confident you should be that the difference in averages isn't random, even if it's small. And the distributions of the datasets, this is often measured using standard deviation. A high standard deviation indicates high variability or that data values on average fall far from the mean. And a low standard deviation would mean that data in general is closer to the mean. If your data sets have high standard deviations, even large differences in their averages can simply be due to randomness. It's important to note here that none of these metrics in isolation provide a quantifiable measure of confidence. Only by combining them using statistical tests can you measure confidence.

The interpret phase of the awesome framework is crucial because it's where data driven insights are evaluated and communicated. You must revisit your initial analysis objectives, understand how the data answers your questions, and uncover any additional findings. Moreover, it's vital to ensure these insights are actionable within your business context. Tools like statistical testing play a key role, helping quantify your confidence in the results. And ultimately, the goal of the interpret phase and data analysis overall is not just to gain insights, but to make informed decisions that propel your business forward.

**Explain Your Findings**
Let's continue our discussion of the interprets stage of the OSEMN process. This is where all the data explorations and number crunching finally pay off. This is also where we explain our findings and generate concrete recommendations for our organizations. Now that we've explored the entire OSEMN process to understand our model, we want to make the hard numbers tell a story. It needs to be a story that anyone in our team, not just the data experts can understand and act upon. This is a crucial step because it's not just about having insights, it's about communicating them effectively. An essential aspect of effective data storytelling is choosing the right medium to communicate your findings, and there are many different mediums you could use. Some examples could be a slide presentation or an interactive notebook, or an in-depth report for an executive review. Each of these comes with its own strengths and challenges, and being able to adapt your story-telling approach to fit each medium is an important skill in data analytics. For the purpose of our discussion today, we will concentrate on slide presentations, but these lessons largely apply to any other medium you use. Slide presentations are universally relevant across industries and job roles and provide a structured, visual and engaging way to take your audience through your findings and recommendations. Your presentation should recap the original goal, review how you went through the steps of the OSEMN cycle, visualize critical data points, and importantly, explain your findings and recommendations. Let's talk about the key components of an effective slide presentation. Start your presentation by taking your audience back to the original problem that initiated your analysis. Re-introduce the issue at hand. What were we trying to solve? Why did we consider it necessary to undertake this analysis? Highlight the potential implications of not addressing this issue, illustrating why it was significant enough to warrant such an in-depth investigation. The purpose of this segment is to establish the context, helping your audience comprehend the relevance of the forthcoming findings and recommendations. Now that you've established the context, take the audience through the method you used. In this example you're going to take the audience through the steps of the OSEMN process that was followed. You want to maintain high-level overview, provide enough context for each step so the audience can understand the methods that lead to the insights. Remember, the goal here isn't to dive into highly technical details. You simply want to build a clear picture of the process leading up to your findings. If you're presenting to a more technical audience though, you might want to include more of the technical details. Now, guide your audience through a visual tour of the data, using aesthetically appealing and easy to understand visuals such as graphs, charts, and tables. You should try to encapsulate the core data points that lead to your findings. Make sure these visuals are accessible to a broad audience and designed in a way that even non data analysts can comprehend. The role of visualizations it's not just to display data, but to make the data speak for itself. Highlighting trends, anomalies, patterns, and correlations that underscore your findings. With visuals presented, it's time to describe them for your audience. This is where you act as the translator, decoding the visuals and turning data into a narrative. Elaborate on the key observations drawn from the data, explaining what they signify in the context of the original problem. Try to link the patterns and trends into visuals to the story you're trying to tell. Make sure your explanations are straightforward and relatable so that the audience can easily understand the story coming from the data. As you approach the end of your presentation, present your recommendations based on the findings, what actions should be taken to address the problem identified at the beginning of the presentation. If your data reveals a potential for improvement or modification in certain areas, outline those recommendations clearly. Discuss why you believe these steps would make a difference. Drawing connections between your findings and the recommended actions.

Let's imagine we've been analyzing a recent decline in web traffic for an e-commerce store. We start our presentation with a recap. We remind the audience about the initial problem; a significant drop in website traffic over the past three months, which we noticed during our routine metrics review. Next, reshare the OSEMN process. We obtained website analytics data, scrubbed it to ensure accuracy, then explore that to identify trends and anomalies. Our exploration led us to the modeling phase where we created a model that identifies potential costs. We then present a visualization, a graph illustrating the downward trend in website traffic alongside the increase in page load time, which we discovered was the primary cause for our model output. The clear inverse correlation visually substantiates our key finding. We can use the graph to explain what happens and when the problem started. As the page load time increased, the web traffic decreased significantly. We're able to tell the story of how our users started leaving our website because of longer low tides causing the website traffic to decline. And finally, we conclude with our recommendation. Given the correlation between page load time and traffic, we suggest optimizing the website's performance in an effort to reduce low time, which should help recover and possibly increase our website traffic. Thus, our data journey comes full circle, offering actionable insights that can be used to improve our business. Interpreting your findings and explaining them effectively to your audience is what turns data into action. It's the crucial final step in the OSEMN cycle, one that bridges the gap between raw data and real-world decisions, and that's the power of data analytics

**Storytelling with Data**

**The Power of Stories**

We can tell stories as a way to entertain, like when we read a mystery novel or go to a movie. But much more than to entertain, we can use stories as a way to teach and share knowledge. Even our novels and movies teaches things. They often inspire us to want to do better in our lives, and they warn us of things like the dangers of war. In this lesson, we'll explore how to use the power of storytelling to make sense of our data and create compelling stories from it. Picture yourself as a master storyteller. As a storyteller, your words create characters, plot twists and intense climaxes. Well, as a data analyst, you're also a storyteller, Adidas storyteller, and you're using data, visuals and a narrative to set up and build your story so that you can inform and persuade your audience. The narrative you create explains the origin of your data and its purpose. It provides a context to your findings and links the data to the answers we need. Using visuals as an aid, your story helps to bring the insights to life and presents a comprehensive picture in a way that's easy for everyone to grasp. Later in this lesson, we'll further explore how to build a narrative from your data and use visuals to craft a compelling and persuasive story for your audience. Let's get started.

**Explain, Enlighten, and Engage**

In previous videos, we talked about the importance of storytelling with data. In this video, we'll talk about the steps in identifying and creating a story from our data. After you've analyzed your data and drawing your conclusions, you will often need to present your findings. You'll want to make sure that you can convey your findings well and persuade people to understand and believe that. In order to tell a persuasive story, we need to focus on the three Es; explain, enlighten, and engage. This can be achieved by a combination of data, narratives and visuals. Both the data and a narrative will serve to explain the situation. The narrative provides context for the data, specifically, where it comes from, why stakeholders should care about it and what was done with it. Data and visualization are part of enlightening your audience, as we've mentioned before, raw data are simply a bunch of values. It's hard for most people to appreciate what the data I have to say in their raw form. By combining the data with good visuals, one can show clearly what the data are and what they mean to the overall big picture. You want to lead your audience to that aha moment to point where everything clicks. Engagement comes from the narrative and visualizations. If the visualizations are good and the narrative is clear and concise, people can internalize what's being said. This internalization gets them invested in the story. As the Venn diagram shows all three parts together, the data visualizations and narrative can lead to audience understanding and persuasion as they're brought together using the three Es of explain, enlighten, and engage. That's frequently the goal of the story.

Suppose you're presenting data on decreasing honeybee populations. You have data on the global decline in honeybee populations over the last decade gathered from various environmental research agencies. This data includes annual honeybee colony counts across several countries. You prepare a story around the importance of honeybees in the equal system, highlighting their role in pollinating a majority of the food crops we consume. You discuss the potential consequences of their declining numbers and why it matters to everyone, not just environmentalists. You create a line graph that vividly illustrates the decrease in honeybee populations over time across different countries. This visual representation allows for a clearer understanding of the magnitude of the problem. The numbers alone would illustrate. Now, how do these elements combine to explain, enlightened, and engage? You use the narrative to clarify the data's origin and its significance. Stating disfigures derived from multiple environmental agencies showed the alarming rate of decline in honeybee populations over the past 10 years. The visual of the line graph is used to illustrate the data. You'd see this graph powerfully depicts the downward trend allowing us to visualize the severity of the issue at hand. Finally, you combine the narrative and visuals saying, imagine a future where many fruits and vegetables becomes scarce due to the lack of pollination. This graph isn't just lines and dips. It represents potential problems to our food supplies that might need to be addressed. This makes the story memorable and drives home the urgency of the issue. In future videos, we'll go deeper into data storytelling and how to tell an effective and compelling story. For now, remember that a good data story persuades your audience by transforming the data into stories that explain, enlighten, and engage.

**Telling a Compelling Story**

So far, we focused quite a bit on data and we also saw how to create compelling visuals. But how about a narrative? How can we build a good narrative to help people understand and be persuaded by the data and the visuals? Every compelling story, from novels to movies to data analysis, typically has four key parts. Setup, buildup, climax, and conclusion. To create an impact, your data story should incorporate these elements. Let's look at them one by one. As with any good story, we should start our story with a hook. Something to get the audience interested in following along. Frequently, these hooks are questions derived from curiosity. Is there a sudden change? Are we missing an opportunity? What should we expect moving forward? What we want to convey in the setup is the theme of the story. It could be that there is an issue or concern that needs to be addressed, or an opportunity to be seized. For our example hook, suppose that we notice a sudden dip in Inu and Neku's sales for the last couple of months. An obvious and compelling hook is why? What could be causing this downturn? After we've set up the story, we want to create a build up. Build up is where the story unfolds. It's here that we describe the steps taken in investigating the hook from the setup. We also want to communicate the findings from our investigations. The actions taken that lead us to the key findings are particularly important. The key finding is the insight from our analysis that has the greatest explanatory power. In our example, after some data exploration, we realized that the sales figures are from multiple channels. They can be broken up into Internet, wholesale, and retail. It looks like the change in sales is stemming from our Internet sales. We then investigated the web data and found that while Internet sales numbers went down, the number of customers visiting the online store did not. Digging further, we discovered that for Internet customers, there was a high abandonment rate of shopping carts, meaning many customers never checked out even though they had items in their cart. This appears to have started at the same time as the downturn started. So that would be our key finding. Online shopping cart abandonment rates increased during the downturn. If this was a mystery story, the climax happens when the villain is unmasked. For us, it's when we explain the hook's root cause with our key finding. Ideally, this is where the audience's light bulb goes off. If you engaged with the audience adequately, they should now understand the dynamic between the hook and the cause and want to act on your insight. In our story, it's strange that 92% of customers abandoned their cards while in the process of making a purchase. This didn't happen in prior months or years. It's at this point that we uncover the fact that customers abandoned their cards due to our lack of inventory for the products that they want to buy. If we don't have the product in stock, of course our customers can't buy it. Consequently, we received no sales for those products. Now we finish the story. If there is action that needs to be taken to remedy the issue, it should be revealed at this point. We should also discuss the cause if we have an idea of what it is. Going back to our example with Inu and Neku, now that we've uncovered the cause for the decline in sales, how do we fix it? Well, if we work to increase our stock of in demand products, we should be able to reverse the downward sales trend. If the products were in stock, our sales would have likely looked much better and the decline wouldn't have happened.

Should be mentioned that you don't always need to tell a nice and neat story when interpreting data. But the most impactful and meaningful discoveries tend to. When the insights are hard to understand or when the impact on your business is large, storytelling with data is essential. Expect an underlying story when data shows something unexpected, unpleasant, complex, costly, or especially surprising. These situations tend to point to a good data story waiting to be told.

**Summary Reading: iNterpreting Data & Storytelling**
**iNterpret Checklist**
Step 1: Understand the results of your analysis
Ask the following questions:

- What was the objective for this analysis?
- How does the data answer my questions?
- What other learnings do I have?
- How can I apply this to a business context?
- How confident should I be?
- How wrong is the model?

How likely is the model to be correct?

What scenarios cause the model to be incorrect?

Step 2: Explain your findings
Build a presentation with these key components:

- Recap
- Method
- Visualization
- Explanation
- Recommendation

**Application: Using the OSEMN framework**

**Introduction**
Hi, my name is Jules Lustig. In the next few videos, I will walk you through my first data analytics project. It is a project I did for school, but I'm hoping my findings will impact how governments make decisions about funding child care. I used the awesome framework to approach my analysis and I thought it was super helpful. But, I'm getting ahead of myself here. Let me first tell you what I tried to do with my analysis. Studies around the world have shown that childcare starting at an early age benefits children's development. However, I believe that access to childcare also has a real impact on the life of moms and their ability to work outside of the house and progress in their careers. Not only that, I believe that access to affordable childcare might be the key to understanding the gender pay gap, or in other words, the difference in pay between men and women. Fixing the gender pay gap is an important topic for policymakers. But, encouraging women to participate in the workforce in the first place is maybe even more important. Given that our population is aging, the older group of the population relies more heavily on younger people for income. To support the older group, it is important that as many of the younger people as possible are employed, so having women as part of the working group is crucial. I believe that these are good reasons for governments to pay attention to childcare. If childcare costs are lower, more women will participate in the workforce. And I also believe that if employers know that women have access to childcare when they need it, the wage gap will shrink. Now, that is of course what I think, but in my study, I wanted to prove that this is actually true. So here are the hypotheses I wrote. First, the percentage of women who work outside the home will increase if childcare costs decrease. And second, with decreasing childcare costs, the gender pay gap will also decrease. In the videos that follow, I will walk you through how I use the awesome framework to evaluate these hypotheses.

**Obtaining Data**

I've been using the Osemn framework to guide me through my analysis, and as a first step in the framework, I needed to figure out how I'm going to obtain my data. I decided to limit my research to only Europe, I'd read a lot about the efforts of the European Union to support childcare. So I felt that I would be able to see the effect of these efforts in data for different European countries over the past decade or so. I am sure data from other countries could have been interesting as well, but I felt I had to narrow down my analysis.
For my first hypothesis, the percentage of women who work outside the home will increase if childcare costs decrease. I needed the following data, data on the percentage of women who work outside the home, and data on the cost of childcare. My second hypothesis was this, as childcare costs decrease, the gender pay gap also decreases. Now, to evaluate this, I again needed data on the cost of childcare, as well as data on the gender pay gap. I also needed data on the time children spend in childcare, because that would help me evaluate the effect of the cost on the gender pay gap. Fortunately for me, Europe keeps lots of data like demographics, income, childcare costs, etc, for its different countries. For my analysis, I decided on two different data sources. The first source I used is eurostat, the official statistical office of the European Union. It is a large database with lots of statistics, and it comes with a website where you can download and filter all the data you need, all free of charge. The data goes back many years, and it covers information around women's participation in the labor force, hours children spend in childcare, and the wage gap between men and women. So many of the data points I needed are here. I also use data from the Organization for Economic Cooperation and Development, also known as the OECD, an international organization with the goal of building better policies for better living. The OECD makes this data available to the public for free. The data covers more countries, but I only use the data related to the European countries I was studying. In particular, the OECD has data on the cost of childcare, so that gave me more of what I needed. Of course, there are other sources where you might find some of these data, but I selected these two data sets because I could be confident that they would provide me with valid and reliable data. And I know this because I use the data validity checklist.
Their source is credible, they come from reputable organizations, and the data is recent. The methodology these organizations use to collect their data is sound. They use large samples that represent the population well, and they clearly describe how the data is collected on their website. These organizations also provide objective data, they use this data for their own research and to help them in policy making. The data is also accurate, often, eurostat and OECD report on similar things, and their data align pretty well. The other thing I found encouraging is that eurostat lists a lot of research that is done using their data. And they publish the commentary of other researchers about the data, which helps to understand whether there might be inaccuracies. But in any case, I did not see any red flags for the data I used. Finally, the data in these data sets is relevant to my research questions. So, having gone through this checklist, I felt confident with the data sets I chose, and I proceeded to download the relevant data. I downloaded the data to Google Sheets because that is where I plan to do my analysis. Here is an example of a piece of a data set I downloaded from eurostat. Next, I had to focus on cleaning my data, I will explain what I did in that phase in the next video.

**Scrubbing Data**

After I'd obtained the data I needed for my analysis, it was time to scrub the data. Let me take you through what that involved in this video. First, I checked for duplicate values in my data. In my case, that was pretty straightforward, I could just check the countries listed in the country row of my data set and the years listed in the year rows to make sure there were no duplicates. I didn't expect any duplicate values because I downloaded this data straight from official databases that had been vetted by many researchers already. And indeed, I did not find any duplicates to remove. But I still had to remove some data, you might remember that I only wanted to focus on data from 2010 until 2021. But when I downloaded the data set, it came with more data than I needed. So I removed the columns with the years of data that I did not need. I did that for all the data sets I had downloaded. Next, I had to format records, this was pretty straightforward for my data as well, because after downloading the data, it came through nicely formatted already. I just double checked that the format was good. I made sure, for instance, that the numbers were indeed formatted as numbers in the spreadsheet, but it was all good, so I did not have anything to do here. Next, I checked for missing values, and as you might note, there were some missing values in my data set. First, I went back into the source data in Eurostat to double check. these values were indeed missing in the database, and that this was not some sort of glitch from downloading the data. But indeed, these data points were not available there can be different reasons for that, like a country not having reported the data, but there was really no way for me to replace them with actual data. In the data set, the missing data points were reflected by a colon or two vertical dots. That is okay, but I like to use letters instead, just to make sure there is no confusion when I start to use formulas later on. So I replaced these missing value indicators with NA. I'm showing you a piece of one of my data spreadsheets here, but I did the same for all of the data sets I used. Note that the data for 2021 had quite a few missing values. That is because it takes some countries longer to report their data than others, and the data for these countries was not available yet when I did my analysis. So I decided to take out all the data for 2021 from my analysis. As a final step in cleaning my data, I had to check for wrong values. Given that my data came from a trustworthy source, I didn't expect any wrong values, but I checked anyway to make sure and could not find anything. There was, however, one issue with my data I was collecting information about childcare, and it was quite obvious in my data that the year 2020 was a bit of an outlier. I had expected this to happen, 2020 was the year of the COVID-19 pandemic, and childcare options were often not available, even in countries where childcare was usually very accessible. I thought that this would affect my analysis, and because I had data for many years prior to 2020, I decided not to include the 2020 data at all. They weren't wrong, they were reported correctly, but 2020 was such an outlier when it came to childcare that I decided not to use the data in my analysis. So I took out all the data related to that year. Now my data sets are ready for further exploration, I'll focus on that in the next video.

**Exploring Data**

Now that I had obtained the data and scrubbed it, it was time to explore the data. This is the phase where I just try to get a sense of what the data was telling me and what trends I could observe. Remember that I was studying the relationship between access to affordable childcare and women's labor force participation, as well as a gender pay gap. So I had a few variables that were important for me to understand. Exploring the data thread in these variables would be a good start. I focused on four variables and they were women's labor force participation, the gender pay gap, the cost of childcare, and the time children spent in formal childcare. I thought that these last two variables combined cost of care and time spent, would be a good indicator for the accessibility of childcare. For each of these variables, I explored the data trend, or in other words, I looked at how the data changed over time. This chart shows the trend in women's labor force participation. I used Google Sheets to create this chart. Later you'll learn more about how to create charts from your data. Exploring my data told me that 67.7% of women in Europe were employed in 2021, and this percentage increased slowly over the past years from 60.7% in 2010. However, women's participation in the labor force is still considerably lower than that of men, 78.5% of men were employed in 2021. The data you see in this chart is an aggregate for Europe, but it was clear from my data that the percentage of women in paid jobs differs between European countries. So I decided to create a chart for that as well. You can see that chart here. Greece and Italy have the lowest percentage of women in the labor force, but in Sweden, 78% of women were employed in 2021, which is quite a bit higher than the European average and is the red bar you are seeing in this chart. I also took a look at the difference between the percentage of men and the percentage of women employed in different countries, which is what the green line in this chart shows. Romania has the largest discrepancy between men and women in the labor force, followed closely by Greece and Italy. In Finland and Lithuania, there is a much more balance between the number of men and women who are employed. I had read another research that one of the main drivers of gender inequalities in the labor market is the difference in time spent on unpaid activities between men and women. Women spend much more time on unpaid activities like household work and childcare, which leaves them less time for them to do paid work. So I was interested in exploring further whether the countries that have big differences in percentage of women versus men working outside the house maybe had less accessible childcare. I'll come back to that later. Next, I looked at the trend in the gender pay gap or the differences in average wages between men and women. In Europe, the average gender pay gap was 13% in 2019. In other words, women were on average making 13% less than men. The gap has decreased a little bit compared to 2010, when it was 15.8%, as shown in this trend chart. The pay gap varies across European countries, with countries like Luxembourg, Romania, Italy and Belgium having considerably lower gap between the average wages of men and women than Germany, Austria, Latvia and Estonia. Then I looked at the data related to childcare. First, I explore the cost of childcare. For this, I use my data from the OECD and I found that over the past years, the average net childcare cost as a percent of household income decreased, In Europe. In 2004, people were paying on average 10.33% of their income for childcare, and that decreased to 8.19% in 2021. As with other variables, the cost of childcare is different between European countries. Malta and Italy provide free childcare, but the cost is as high as 20% or more of the household income in Czechia, Cyprus and Ireland, with the cost of childcare decreasing, we are also seeing an increase in the time preschool age children spend in childcare. In 2019, children under the age of 3 spent, on average 10.8 hours per week in formal childcare, a 22.7% increase compared to an average of 8.8 hours in 2010. Again, we see large differences in different European countries, with children under the age of three in Norway spending 23.3 hours on average per week in childcare. And children under three in Switzerland, Bulgaria and Italy spending less than half that time in childcare.

There was one last thing I wanted to explore in my data. The European Union has invested a lot in decreasing the cost of childcare, but I wanted to check whether lower cost actually means that people will use childcare more. People might have other reasons to opt out of childcare, and maybe lowering the cost doesn't change that. So I checked out the relationship between the cost of childcare and time spent in childcare. Here's what I found, the cost of childcare accounts for 84% of the variation in the time children under 3 spend in childcare. In other words, it looks like cost is a big factor influencing the time children have been spending in childcare. As this graph shows, when the cost of childcare goes down, time spent in childcare goes up. I have to add a caveat here though, given that I only had data from six different years to do this analysis. That is not that many data points, but I think the correlation is strong enough, which makes me feel more confident. As my statistics course taught me. This didn't quite prove that lowering the cost is the reason for increased time spent. It just shows that there is a relationship between the two. You'll learn all about this later. I felt that exploring the data confirmed many assumptions I had and I thought this was a very interesting and fun phase of the analysis. With all I had learned so far, I continued to do the modeling phase. I will tell you more about that in the next video.

**Modeling Data**

I explored the data and found a few things to take a closer look at. Although I didn't exactly create models that can help me predict the future, I focused on further analyzing the relationship between childcare on the one hand, and women's labor force participation, as well as the gender pay gap on the other hand. Even though my models are not complex at all, I classified this step under the modeling phase. Here's what I did. I read a research paper from the OECD about unpaid care work, in which they showed that around the world, women spend 2-10 times more on unpaid care work than men. For Europe specifically, it was estimated that women spend about double the time on unpaid care work than men. Earlier, I had read and other research by Gimenez-Nadal and Molina that this discrepancy in unpaid care work is responsible for the lower participation of women in the workforce. I thought that if we reduce the care load for women, we would see an increase in women's participation in the labor force. Childcare is, of course, a big part of the unpaid care women take on. I expected that if the time children spend in daycare increases, we would also see the increase in the participation of women in the labor force. This chart shows the result of my analysis. Basically, it shows that there is a significant correlation of 87.2% between women's labor force participation and the average number of hours children under three spend in formal daycare. It shows that a one-hour increase in childcare increases women's participation by 2.46% points. Another topic I was really interested in exploring was ways to close the wage gap between men and women. I had read a study by Claudia Goldin , a Stanford professor, in which she pointed out that for the gender pay gap to close, it is important that firms don't disproportionately reward individuals who work long and specific hours. I believe that if women are spending more time on unpaid care, the people who hire them might assume that they won't be as flexible and thus they will not be spending more hours at work. That could affect how they are rewarded. I think that if childcare is more accessible, then that assumption might go away and over time, companies will expect that women and men will be equally flexible when it comes to spending an extra hour at work. With that in mind, I expect a relationship between childcare and the gender pay gap. My analysis confirms that the pay gap decreases as the average weekly time spent in formal daycare for children under the age of three increases. There is a significant correlation of 65% between the difference in pay between men and women, and the time children spend in formal childcare. We found that when the average time spent in childcare goes up by one hour, the average pay gap decreases by 0.64 percentage points. These analyses were in line with what I had expected or the hypothesis I formulated for my research. Now, I had what I needed to interpret my results and draw my conclusions. I will tell you more about that in the next video.

**Interpreting Data**

Now it is time to interpret the results of the analysis and draw some conclusions. I did this research for a school project, but I would like to send my findings to the European Union because I'm hoping it might inform their policymaking. To do that, it is a good idea to summarize the main results. Here's what I concluded. Women's labor force participation in European countries has increased slowly over the past decade, from 60.7% in 2010 to 67.7% in 2021, but it is still significantly lower than the participation of men, which was 78.5% in 2021. The average gender pay gap in EU countries decreased from 15.8% in 2010 to 13% in 2019, but a gap of 13% is still pretty high. Of course, it is important to note that the gap differs substantially between countries. The European Commission highlights and it studies that one reason for these differences between men and women's status in the workforce is the unequal share of paid and unpaid work between men and women. More specifically, women spend much more time than men on unpaid child care. This is why I expected the lowering the cost of childcare will make it easier for families to put their child in childcare and will allow women more time for paperwork leading to more women joining the workforce. I also expected that increased access to childcare will increase equality between men and women regarding salaries, availability of childcare might change the employer's expectations in terms of how much unpaid work women will take on compared to men and that could influence the gender pay gap. My expectations were confirmed by my analysis. The cost of childcare decrease by 2.19% points since 2004. As the cost of childcare decreases, we have seen a 22.7% increase in the number of hours children spend in formal childcare. The number of hours children under three spending formal childcare accounts for 87.2% of the variation in the amount of women's participation in the labor force. This correlation suggests that improving the accessibility and quality of former child care within a country, could incentivize more women to join the workforce. I found a similar relationship when I compare the amount of time spent in form of childcare to the average gender pay gap. I expected that as the amount of time and childcare increase, the pay gap would decrease. In my analysis confirms that in fact, time spent in childcare accounts for about 65% of the variation in the average gender pay gap. To conclude, my research suggests that access to formal childcare for children under the age of three, plays an important role in achieving gender equality in the workforce, both when it comes to increasing women's labor participation, as well as closing the gender pay gap. Given that achieving economic gender equality is crucial for economic growth in the EU, focusing on policies that improve access and affordability of childcare is important. That gives you a full overview of the research I conduct it. Of course, there are many more analyses that would have been interesting and that would give us further insight into my research questions, but that's for the future. I found that using the awesome framework to help keep me focused and guided me from data gathering to conclusion. I hope that the framework can do the same for you.

**Weekly Review: Interpreting Data**

That brings us to the end of another week in this course. You're almost there, this week you learned about the last step in the awesome process interpreting your data. You also know how to craft a story from your data and insights. And you heard from Jules and saw how he used the awesome framework to approach his data analytics challenge. And you had a chance to apply the framework yourself, too. Now it's time for you to put all you know into practice. Let's explore tips on how to chart your career path next. Good luck.

**GenAI in Data Analytics**

**What Is Generative AI?**

**Understanding Generative AI: An Overview**

Almost every day, I read news stories or hear my friends and colleagues talk about new and great things you can do with artificial intelligence or AI. You've probably seen the headlines too. We hear about generative AI and tools like ChatGPT, MetaAI, Gemini, and so on. We're told how AI will change how we work and interact with the digital world. I find it fascinating, and these developments have been incredibly helpful in making me more efficient in my day-to-day job. We'll look at some of these developments and their applications, but before we do that, I want to take a step back and walk through the rapid developments we've seen over the past few months, and what it all means. While AI is the news of the day, AI isn't new. AI, or artificial intelligence, stands for the ability of machines to perform tasks that are typically performed by humans. Yes, we distinguish ourselves from other species because of our intelligence. We can think, reason, connect relevant facts together, and so on. Over the past few decades, engineers and data scientists have worked on different ways to make computers do similar tasks. And because computers are very good at connecting different data points and storing information in memory, they've succeeded at making machines do very complex tasks, like super-complex math, data analysis, image recognition, and so on. You may have heard people talk about machine learning in the context of artificial intelligence. In machine learning, scientists feed a machine a lot of information and patterns, or algorithms, to teach it how it should react in certain situations. With that information, the machine can build models, and these models help it predict new outcomes based on what it learned. So before a machine is intelligent, it needs to absorb a lot of information to start to recognize patterns and solve new problems. An example of where artificial intelligence and machine learning are used is in advertising personalization. Platforms like Meta learn from the behavior of their users, and based on the machine's understanding of patterns in behavior, it can tailor ads to align better with your preferences and needs. While we've seen great developments and ever-more-powerful applications of AI, it's already been used for a few decades. So what's the hype now? The new developments center around a specific form of AI, generative AI. Most AIs operate on predefined rules and memory. But generative AI can create learning from data to generate new content that's never been seen before, whether it's text, images, or even music. To do this, generative AI uses deep learning, an advanced form of machine learning that's great at making connections between pieces of information, something we humans do all the time when we create new things. Here's how it works. First, a machine, think of it as a big computer, is fed vast amounts of information. And through machine learning, it learns patterns and features from this data. Then the machine can be prompted with a question. And it answers it by generating new, original material that mimics the input data in a helpful way. Feels abstract? Well, let's look at an example. I will use Meta.ai, which I can access at meta.ai, it's free. It gives me a few examples here of what I can ask Meta.ai, but I will use it to create an itinerary for a trip I'm planning through Europe with four friends One friend of mine uses a wheelchair, and sometimes that poses extra challenges. Let's see what generative AI can do for me. I'm typing in my question in the search box. I'm giving Meta.ai information that will help it answer my question here. I'm doing this in natural language. In other words, I am talking to Meta.ai just as I would to a friend, or in this case, a travel agent. Let's click the arrow to generate the answer. Nice, I'm getting a great itinerary here, with highlights, ways to travel, and hotel suggestions. Oh, amazing, everything is wheelchair accessible. So one less thing for me to figure out. This saved me hours of research. You can have a conversation with Meta.ai and ask it to dig deeper into the information it already gave you, or just continue asking for what you need. So I'm asking, I heard people talk about a great bistro in Paris that's really old and traditional, but I forgot the name. I only remember it has wood paneling inside and they serve a great crêpe Suzette. Any chance you would know bistros like that? And maybe you have other suggestions for traditional Parisian restaurants. That's what I'm getting as an answer. Several suggestions for bistros. These are all fantastic suggestions. My friends will be impressed. If you're like me, you probably already have a thousand questions in mind that you would like to ask Meta.ai. I'd say, go ahead, try it out. Now let's think about what happened in the background here. Meta first trained machines with vast amounts of information and algorithms or models. Based on that, the machines learn to make new connections when given information. These machines I'm referring to are lots of connected servers that live in what we call the cloud. This apparatus of information and models is called a large language model or LLM. In this case, the model Meta built is called LLAMA, the large language model Meta.ai. In order for us to interact with this large language model, we need a tool or interface to prompt it with questions. In our example, we used Meta.ai. Other companies have built their own LLMs or large language models. OpenAI is one of them. Their well-known model, GPT, can be prompted using chatGPT. Similar to Meta.ai, chatGPT lets you interact with GPT and ask it questions through a conversation. Similarly, you can interact with Google's LLM, Gemini, using the Gemini interface. All the examples I've shown you are great for natural language conversations. But generative AI can do more than that. Not only can it generate new language-based information, GenAI can also create images, music, code, and even video. How? Well, if the underlying model is trained with tons of pictures, videos, and so on, and taught to recognize patterns and associate these with meaning, it can, based on the same principles we just discussed, generate new images, videos, music, and so on. Clearly, the applications of generative AI are vast and varied. From chatbots that can mimic human conversations, like this one we saw, to sophisticated programs that can create stunning visual art and music, generative AI extends the boundary of human creativity and efficiency and is rapidly changing the way we work and tackle daily tasks. As businesses integrate AI, understanding and utilizing generative AI will be crucial. It's not just about the technology itself, but how we apply it thoughtfully and creatively to real-world problems. If you haven't tried these tools yet, do it now. All the tools we discussed are free, and to really grasp their power, you have to experience them.

**Exploring Different GenAI Technologies**

In this video. Well take a closer look at the different Genai technologies that give us new ways to partner with machines to create anything from text content to images, audio and video. You know how Meta AI, ChatGPT and Gemini let you have natural language conversations with AI to help you research and generate new content in text format. The models used for text generation use natural language processing or NLP. NLP enables computers to understand and generate human language, whether its chatbots on websites or content generators that draft articles. NLP is behind the magic. It is transforming how we create content. It can help you write emails for instance, but also changes how we do customer service. Chatbots have become very good at solving problems similar to how humans approach tasks, which is of course essential to customer service. There are many more examples of where NLP based GenAi is changing how we work like in writing copy for ads, generating social media posts, recognizing patterns in research data, and much more. Aside from text, generative AI can also generate images. Image generation tools understand images and their patterns and can generate new images from text descriptions. OpenArt.ai is such a tool. You can also generate images in OpenAI's ChatGPT four which uses the underlying DALL-E, a model trained on images. Other tools such as Artbreeder, DaVinci or Hotspot also let you create images from text. Apart from creating art with the help of AI, you can also use image generating tools for prototyping. AI can help you generate illustrations of prototypes so you don't need a graphic designers help for every draft. You can also use it to generate images for ads or social media posts. People also use image generators to help them build slides for presentations. There are many more applications. Lets look at an example. Meta AI has a built in image generation tool. You can find it under the link imagine. Lets take a look at what image generation can do for us. I would like to create an invitation for my upcoming trip to Europe with my friends. Sending them the information ive gathered in an email would be cool and I want to catch their attention with an image. I asked Meta AI to create an image for me with some iconic landmarks of the places we want to visit, some traditional food, and something that illustrates the fun well have. Lets check out how this worked. What do you think? Did it capture the fun adventure? Next up lets look at audio generating tools. With GenAi you can now generate music tracks or voiceovers without a studio, perfect for creating custom audio for videos or automated customer support lines. Some examples of tools focused on audio generation are WaveNet from Google, Jukebox from OpenAI, Audiocraft from Meta, ElevenLabs and Stable Audio. Theres a wealth of applications for these audio tools. We already mentioned customer service. Now customer service bots can talk to people in human voices and have a real conversation. These tools also let you create music and sound effects with a few prompts, making a musician out of all of us. They can also create audiobooks in a flash without the need for someone to read and record the book. The tools to do all this are easy to use. Let me show you how I used eleven labs to dub a video from this course in German. My German is rusty, so I used AI to create this version of me speaking German by uploading my English spoken video. I uploaded my english language video, told eleven labs that I wanted to dub this in German, and voila. Now I speak German. [FOREIGN] You can play around with this too. Most tools offer the option to get started for free, so absolutely worth exploring. Generative AI goes beyond images and audio. You can use it to generate video as well. You can create videos without touching a camera by simply writing what you want a video to show. A few examples of tools that help you do that are Invideo.io, Veed.io and Fliki.ai. These tools use stock footage and images combined with audio to create videos. Some tools like synthesia IO let you use avatars to create videos of people who speak the text you type into the tool. Let's look at a video I created with synthesia using one of their avatars. Basically footage of a person and a fragment of text I gave them. As promised, I am sending you a summary of the key points of the proposal we discussed today. I will put you in touch with your pedagogy team right away. This person doesnt look like me, but I could change that. I can provide synthesia with some footage of me making it possible to generate this video message using my voice and likeness. Im sure you can already think of many applications for this technology. It lets you create videos. You can post on social media, create video explainers for your website, or create educational videos. You can also add video content to film documentaries, news reporting and so on. As amazing as this technology is, there are also ethical considerations we should always keep in mind. When AI is used to generate audio and video using a persons likeness, we sometimes also refer to the result as deepfakes. Deepfakes are images or recordings that have been convincingly altered and manipulated to misrepresent someone as doing or saying something that was not actually done or said. As generative AI tools become increasingly sophisticated its also easier to create audio or video content that lets a person say or do something on screen they never really did. Fake news or bullying often result. So its extremely important that people and businesses set boundaries around how this technology is used. So far we've seen AI models trained with text, images or video that can generate new content in these different formats. There is another type of GenAi technology that's popular and has many interesting applications in business, code generation tools. Code generation tools assist in writing and completing blocks of code. They can write code or translate code from one programming language to another. They can also help generate documentation about the code and find snippets of code you can use to execute certain tasks. There are several popular code generation tools. OpenAI's Codex is one of them. The tool lets you translate natural language into code. The model behind it is trained on both natural language and millions of lines of code in a dozen different coding languages. The tool can be used by developers. Codex is also the tool that powers GitHub Copilot. This tool lets users access publicly available code from GitHub repositories and can detect errors in code and recommends changes to it. When you use Copilot, you can use natural language prompts to help check and polish your code. Other GenAi coding tools are AlphaCode, Tabnine and CodeT5, and there are many more, but you don't have to use a specific coding tool to help you with your programming. GPT-4, OpenAi's latest AI model is a multi model tool and can also handle programming tasks using ChatGPT. You can also polish your code or even write new code with text prompts. The use of generative AI in coding is quickly changing the software development landscape. AI powered coding tools often outperform human programmers in complex tasks, and developers can use the tools to speed up their programming tasks considerably. Moreover, for people without programming skills, new possibilities are opening up, helping them to create applications by using text commands. There are still some limitations though, and for now we see the best performance when these tools are used by seasoned programmers. They help with code generation, faster testing and debugging code, writing documentation, explaining the code, translation from one language to another, etcetera. One last technology I would like to touch on are data generation tools. These are tools that enhance or generate new data from existing datasets without manually collecting new data, sometimes also referring to as synthetic data generation. Tools like Gretel.ai generate synthetic datasets based on a sample of real data with the same characteristics the real data. Other tools are Mostly.ai and K2View. You can also use ChatGPT 4 to generate datasets from text or data prompts. Generating synthetic datasets can be very helpful in cases where its hard to collect data because it may be expensive or it may have privacy implications. Think about market research for instance where collecting user information isnt cheap and can be complex. Being able to enhance the data you collected with synthetic data that mimic the patterns of your original data can help you get the data you need to conduct marketing analysis tasks. Synthetic datasets are also used to train the models that power generative AI. For these models to become as smart as they are, they need to be trained on fast sets of data and this is a way to quickly supply the model with more data to learn from. Generative AI is quickly changing the way we do business. Different technologies focus on generating different formats from text to images, video, code and data, all of which have their own powerful applications. Embracing these technologies can elevate your work and can help you be competitive and cutting edge. Next, lets take a look at some examples of how different industries have made use of generative AI.

**Applications of Generative AI in Business**

As the digital world evolves, so does the role of AI in business. Generative AI is not just a futuristic concept. It's here, revolutionizing how companies operate, create, and connect with customers. From text and image creation to innovative uses in video, data analysis, and coding, applications of GenAI are reaching new levels of creativity and innovation. In this video, we'll explore some real-world examples. Before I do that, there's one important topic I want to point out. While there are many off-the-shelf GenAI tools like Meta AI, ChatGPT, Gemini and so on, businesses often build their own adaptations of these tools. This means that a business will use the underlying GenAI model, but customize it for its own environment and make it proprietary or only accessible to people in the company. They do that for several reasons. First, for privacy concerns. Businesses don't always like to feed their information into the GenAI model. By asking these GenAI tools questions, the data you provide will be used to further adapt the model. Obviously, companies don't like to feed their company's secrets to the system. So many GenAI tools offer the option for enterprises to use their model privately. Second, businesses often have their own proprietary way of operating, and they may also have important private information. They can decide to feed this information to the private model, so the model uses this information for its training. That will then allow businesses to create a version of the GenAI tool that provides results that are more in tune with how the organization operates. We refer to that as tuning GenAI. The examples we will discuss in this video can be achieved by using public tools or by adapting GenAI to suit a business' needs. Generative AI excels in creating compelling and personalized text, transforming how companies engage with their audiences. Take, for instance, BuzzFeed, the digital media giant, harnesses natural language generation to create diverse content ranging from quizzes to articles. By using tools like OpenAI's GPT, BuzzFeed automates the generation of content ideas and drafts, which are then fine-tuned by human editors. This integration of AI not only speeds up content creation, but also helps in tailoring unique content that engages different audience segments. Another application of text generation with GenAI can be found in customer service, where AI chat bots provide 24/7 support, handling inquiries and resolving issues faster than ever. This enhances customer satisfaction and frees up human agents to tackle more complex tasks. Take ZARA, for instance, their AI chat bot handles thousands of queries daily from tracking orders to product recommendations, ensuring that customer needs are met promptly and accurately. Images are crucial in marketing and advertising, and small and large advertisers alike are using generative AI to create visually captivating content in their ads. Sometimes GenAI is used to enhance images, but in some cases, images are created with text prompts only. In a campaign to encourage people to help protect wildlife and the environment, World Wide Fund for Nature used GenAI to create images to feature on billboards and in social media. In the #WorldWithoutNature campaign, WWF asked creators to use GenAI to imagine a world without nature. The bleak futuristic images were used to help generate awareness for the cause. In marketing, GenAI tools like image generating and natural language processing tools are game changers, and so are video generating tools. Virgin Voyages combined the power of video generating tools with GenAI's capability to create highly targeted content that resonates with audiences, ensuring that messages are both personalized and powerful. They created an advertising campaign in which they encouraged people to create a personalized invitation for a Virgin Voyage cruise, using video footage of Jennifer Lopez. Let's take a look at the ad that announced this GenAI campaign. Hi. It's me. Jennifer Lopez, Chief Celebration Officer of Virgin Voyages, here to invite you to celebrate. So come. Celebrate your anniversary, your birthday, or just generally being fabulous. Really, Kyle? It's good, right? GenAI is supposed to be inviting people to Virgin Voyages, not doing whatever that was. Just give me that. As I was saying, why not celebrate on an award-winning voyage with Michelin starred, chef-curated menus. World-class destinations. Code generating tools are already embedded in many developer teams. Tools like GitHub Copilot are used to streamline the software development processes. GitHub Copilot assists developers by suggesting whole lines or blocks of code as they type. This AI tool helps developers write code more efficiently by automatically suggesting completions and entire functions based on the context of the code they are writing. One application that has saved developers countless hours is using GenAI for bug fixing. Coding is complex and, in some cases, the codes developers write may not execute the tasks it's intended to complete. The code doesn't run properly and, in those cases, we talk about a code bug. Fixing bugs can take a lot of time, as it often involves reading and rereading code to find which lines cause the problem. ChatGPT, for instance, can help find and correct bugs and explain why the original code ran into problems. In this example, the original code did not work and ChatGPT suggested new code and explained where the original code was problematic. Finally, let's touch on the use of data generating tools, which enhance, expand, and mimic real data. One powerful application of this technology is in healthcare. Synthetic data generation has the potential to revolutionize the way healthcare organizations handle sensitive patient data. Synthetic data generation involves creating artificial yet realistic datasets that mimic the statistical properties of real patient data. This approach offers several benefits, such as protecting patient privacy, since the data contains no real patient identifiers, and it allows for extensive data sharing and analysis without the risk of breaching confidentiality. This can lead to better disease modeling, more robust research studies, and enhanced healthcare services, as developers and researchers can work with large volumes of data without compromising individual privacy. These are just a handful of applications of generative AI in business. Of course, there are many more. Keep an eye on new applications and think about how you can use GenAI in your work environment.

**Navigating the Challenging: Concerns with AI**

You may have heard about concerns about the ethical use of AI in recent news. There have been instances where AI systems have misused data or acted in ways that raise ethical concerns. These stories underscore the need for robust ethical standards and privacy safeguards in AI development and deployment. So what does this all mean and how do you ensure accuracy and ethical use of GenAI? This video will focus on these questions. Generative AI processes vast amounts of data. Deep learning algorithms recognize patterns in the data, and large models are built that allow machines to make new connections and generate new content based on natural language prompts. The new content can be text, images, videos, music, or code. GenAI is powerful, and it provides us with tools that can save a lot of time and resources. But with these powerful applications come some challenges. First, there are challenges related to the data input. GenAI's ability to create new content is dependent on the data it was trained with. When that data includes biases, the results will be subject to those same biases, and sometimes AI may even amplify these biases in its outputs. For instance, AI may be trained with historical hiring data for people in engineering roles. The data may show more men than women in these roles. When the AI is asked to generate an image of an engineer, it may favor men over women in its depiction of people in this role. To avoid this problem, researchers are focusing on improving data diversity, implementing fairness-aware algorithms, and regularly auditing AI models for bias to mitigate this issue. Another challenge related to the data input concerns intellectual property and copyright infringement. By relying on existing data, generative AI can create content that may inadvertently infringe on intellectual property rights, including images, text, and code. An AI-generated artwork, for instance, might create pieces that are derivatives of a protected work, leading to copyright infringement claims. Companies are working to address this concern by implementing content filters and developing technologies that can recognize and avoid using copyrighted material as data input. Another way companies are working through this issue is by licensing the content to allow for its use in their models. Another set of concerns when working with AI have to do with processing of the data and the deep learning algorithms. Many AI models, especially deep learning models, are often considered black boxes because their decision-making processes are not easily interpretable by humans. We get an answer in response to our prompt, but we don't really understand how GenAI came up with that answer. A medical diagnosis AI, for instance, might provide a treatment recommendation without a clear explanation, making it difficult for doctors to trust the suggestion. Companies developing AI models realize that it's important to make their model's decision-making traceable or transparent, and they're focused on providing more elaborate documentation to help with that. Another concern related to the advances in GenAI is the resources required for training state of the art AI models. A lot of computational power is necessary, and all these computers and servers churning the data require a lot of energy, making it very costly in terms of both financial resources and environmental impact to develop and use GenAI. Training models like OpenAI's GPT-4, for instance, can cost billions of dollars and consume energy equivalent to that of a small town. Companies and researchers in the field are working on more efficient model architectures, improving algorithmic efficiency and using techniques like transfer learning to reduce the need for training large models from scratch, to help lessen the impact on the environment and reduce the costs. Finally, there's a group of challenges related to the output of the GenAI models or the new content they generate. Sometimes AI can mimic language patterns and facts from its training data without genuine understanding or contextual awareness for its output. AI might generate text, for instance, that is factually incorrect or inappropriate if it's based on misleading or incomplete input data. We often refer to this as mindless parroting. Basically, AI just repeating something it learned without really understanding what it means. Developers working on AI models are including training models to help AI recognize when it's generating nonsensical or irrelevant outputs, and they are including mechanisms for feedback and correction. But it's good to know that these systems aren't fully reliable yet. For that reason, it's a good idea to use a human-in-the-loop approach when working with GenAI, where the GenAI output is critically evaluated by users and corrected when necessary. Another related problem is hallucination, which we can encounter when using GenAI. AI models can hallucinate details, meaning they generate plausible but false or nonexistent information. For instance, a text generating AI might invent details about the public figures that are not based on real events, but that could be considered plausible. Techniques to reduce hallucinations include refining training data, using stricter validation during the training process, and implementing cross-checks within the model's outputs. But just as with parroting, these techniques aren't yet an accuracy guarantee, so it's a good idea to keep a human in the loop. A final and one of the biggest challenges is ensuring AI systems act in ways that align with human ethical standards and values. GenAI is programmed to generate new content and provide answers to our questions, and it may not deploy the same value system that is inherent to our society. It may provide us with answers and information that's inappropriate or unethical. Ethical guidelines and value alignment frameworks are being developed to help with this challenge. Regulators and governments are working on establishing ethical guidelines and standards to help ensure the ethical use of AI. But as AI evolves, we will need to stay vigilant and continue to improve the models to impress human values on the machines. Regular audits, updates to AI protocols, and active engagement with ethical discussions are indispensable in this task. We all have a role to play in shaping the use of AI. By staying informed and proactive, you can help ensure that generative AI serves the greater good, enhancing both industry standards and consumer trust.

**GenAI in Data Analytics**

**Integrating GenAI with Data Analytics**

Generative AI is transforming the data analytics landscape from obtaining and scrubbing data to exploring, modeling, and interpreting. GenAI is proving to be extremely helpful in the data analysis process. In this video, I'll give you an overview of these developments. First, let's go over the definition of generative AI. GenAI is a form of artificial intelligence that uses machine learning to generate new content based on a variety of inputs. The inputs can be text, images, sounds, animation, audio, video, or code. The process looks something like this, a machine, think of it as a big computer, is fed vast amounts of information, and through machine learning, it learns patterns and features from this data. Then the machine can be prompted with a question, and it answers it by generating new original material that mimics the input data in a helpful way. Because GenAI can process large amounts of data quickly, it can be tremendously helpful in data analysis. Let's examine each part of the data analysis process and how GenAI can influence our approach. I'll highlight each part using the OSEMN framework for data analysis. OSEMN stands for obtaining, scrubbing, exploring, modeling, and interpreting data. Let's start with obtaining. GenAI can greatly enhance data acquisition by generating synthetic data that mimics the existing data when real data is insufficient or unavailable. This is particularly useful in scenarios requiring large dataset or when dealing with privacy-sensitive sectors like healthcare. For instance, in the healthcare sector, GenAI has been used to create synthetic patient datasets that mimic real world complexities and variabilities. It allows researchers to conduct robust analyses, relying on smaller sets of real data and adding synthetic data to it that mimic patient profiles without compromising patient privacy. GenAI can also automate and refine the data cleaning or scrubbing process by identifying and correcting errors or inconsistencies in the data. These errors can be outliers or missing values, for instance, and GenAI can help correct them based on learned patterns and dependencies. An example of this could be the use of GenAI to scrub scanner data in supermarkets. GenAI can help automatically clean customer transaction data, filling in missing values and correcting anomalies in purchase patterns like double scanned products or missing parts of loyalty data. This helps ensure the data's reliability for further analysis. GenAI really shines when it comes to data exploration. Through advanced pattern recognition and generative models. GenAI can facilitate deeper data exploration, helping uncover complex relationships and trends that may not be visible through traditional analytical methods. It can deal with large amounts of data in different formats and structures and analyze that data very quickly. It can also support the creation of graphs to help you better understand the patterns in your data. There are many ways in which you can use this capability of GenAI to help with data exploration, and data analytics tools are integrating GenAI benefit from this. As an example, GenAI tools can help marketing analysts explore large amounts of consumer behavior data, generating insights into purchasing trends and preferences across different demographics that were not previously apparent. GenAI is also very powerful when it comes to building models. GenAI can automate the feature engineering process by identifying variables in the data and assist in developing predictive models by suggesting or even creating new model architectures based on the data's inherent characteristics. Financial analysts, for instance, use GenAI to construct and refine credit scoring models. GenAI not only suggests new features like spending patterns but also tests different model architectures to improve prediction accuracy. Finally, GenAI can also help us interpret data and models. GenAI can generate explanations and visualizations for complex models that are easier to understand, making the results of data analysis more accessible to decision makers. An interesting example of this is the use of GenAI in urban planning. GenAI can interpret traffic flow simulations, providing planners or people who may be less familiar with these types of simulations with intuitive visualizations and scenario analysis that help them make informed decisions about infrastructure developments. GenAI is quickly transforming each step of the awesome framework, making the data analytics process more efficient and insightful. You'll find that different data analysis tools are adding GenAI capabilities to help you benefit from this. As you approach your data analysis roles, I encourage you to think about where AI can help you and take your analysis to the next level.

**Improving Data Quality and Generation with GenAI**

In the world of data analytics, the quality of your insights depends heavily on the quality of your data. Therefore, data collection and cleaning are critical steps as they form the foundation on which all good analysis is built. Even the most advanced analytics techniques cannot compensate for poor data quality. Data collection ensures we obtain the necessary information to address specific questions, while data cleaning or scrubbing involves refining this data by correcting inaccuracies and filling in missing pieces, ensuring that the data is not only accurate, but also formatted correctly and consistently for analysis. In other words, its a step in which we prepare our data for analysis. Generative AI is shifting how we handle data. GenAI doesnt just work with data, it can actually generate data and enhance its quality. This capability is a game changer. It means that GenAI can synthesize completely new datasets and mimic the statistical properties of real data, which is invaluable when the available data is limited or sensitive. GenAI can also automate the tedious aspects of data cleaning, like identifying and fixing errors or filling in gaps. Let's first take a closer look at the role of GenAI in obtaining data. Data synthesis is the process of creating artificial data that closely mirrors the statistical properties of real world data. This doesn't mean simply duplicating existing data. Instead, it involves generating new data points that are statistically consistent with an original data set, but do not replicate any real individual entries. This is crucial in maintaining privacy and adding robustness to data driven models. Lets consider a practical example. Imagine a startup that develops data analytics tools specifically tailored for the retail industry. To test and refine its tools, it needs access to extensive customer transaction data. However, acquiring such data can be problematic due to privacy concerns and regulatory constraints. Heres where GenAI comes in, the startup uses a GenAI model trained on a dataset obtained from a retail partner under strict privacy agreements. The model learns the statistical properties of this dataset and then synthesizes a new artificial dataset. This synthetic data retains the essential characteristics necessary for meaningful analytics, but does not contain any real customer information, thereby not violating privacy laws. This approach allows the startup to test and improve their products extensively before going to market, ensure their tools are capable of handling real world data variations, maintain compliance with data protection regulations. Data synthesis with GenAI enhances the depth and breadth of data available for analysis and safeguards privacy and compliance, opening up new possibilities for data utilization without the associated risks of handling sensitive information. Next, lets examine the role GenAI can play in ensuring data quality. Poor data quality can derail the most sophisticated analytics projects, leading to misleading results and faulty decisions. Common data quality issues include missing values, data entries that are absent can skew analysis and lead to biased outcomes. There can also be inconsistencies in the data, conflicting or duplicate data entries that can confuse analysis algorithms. Another common issue is noise, random errors or variances in data that obscure the true signal. These issues are addressed during the data cleaning or scrubbing phase. It ensures that the data is usable and accurately reflects the real world scenarios its supposed to model. GenAI can automate much of the data cleaning process using intelligent algorithms to identify and rectify these issues. GenAI models can be trained to recognize patterns indicating errors or inconsistencies. And then automatically suggest or apply corrections. These corrections often involve estimating missing values. GenAI can use more sophisticated techniques to estimate missing values based on the likelihood inferred from the observed data, which helps maintain the integrity and distribution of the data. Lets consider a practical scenario involving a retail company. This company has extensive sales data collected from various sources, but the data often contains pricing errors and missing inventory records, common issues in dynamic retail environments. Using a GenAI tool, the company can automate the correction of these pricing errors. The tool scans through the sales records, identifies prices that deviate from expected patterns based on product type, location, and historical data, and corrects them. Moreover, for missing inventory data, the GenAI system uses data from similar products and stores to intelligently fill in the gaps. The result? The quality of the sales data is significantly improved, enabling more accurate demand forecasting. Accurate forecasts are crucial for inventory management, pricing strategies, and overall business planning. With enhanced data quality, the company can better predict demand periods, optimize stock levels, and avoid overstocking or stock outs. By automating and improving both the data collection and cleaning stages, GenAI enables businesses to unlock new insights and drive innovative solutions. We saw how GenAI can generate synthetic data that replicates real world data's statistical properties. This capability is crucial for training robust models and conducting analysis in environments where data privacy must be preserved or real data is scarce. We also saw that GenAI can greatly enhance data quality by identifying and correcting data errors and intelligently filling missing values. These steps can be tremendously helpful to ensure you can proceed with datasets that are clean, consistent, and reliable for analysis. As GenAI evolves, you'll find that more of these capabilities will be built into the data analysis tools you use, making it easier for data analysts to leverage the power of GenAI.

**GenAI in Predictive Analytics**

GenAI can profoundly impact the way we approach data analytics projects. If we consider the awesome framework for data analytics, splitting up data analysis projects into five phases, obtain, scrub, explore, model, and interpret, we find that GenAI disrupts each phase. In this video, we'll focus on how generative AI is changing, how we approach the exploration and modeling phase. In fact, GenAI is particularly powerful when it comes to predictive analytics, or in other words, predicting events based on historical data. Let's start with an overview of predictive analytics. Predictive analytics involves using historical data, statistics, and machine learning techniques to identify the likelihood of future outcomes based on historical data. It's a powerful tool used across many industries to drive decision making and strategic planning. For instance, in the finance industry, predictive analytics is used to forecast market trends and manage risk. Financial institutions rely on predictive models to guide investment strategies and anticipate market movements. In healthcare, predictive analytics helps forecast patient admission rates, predict disease outbreaks, and improve patient outcomes by identifying risk factors and suggesting preventative measures. The retail sector uses predictive analytics to forecast demand, manage inventory, and personalize marketing strategies. By predicting customer behavior, retailers can optimize stock levels, reduce waste, and improve customer satisfaction. Now, let's consider generative AI. Generative AI is a form of artificial intelligence focused on generating new content based on a variety of inputs. In that new content, it mimics the properties of the real world data it was given as an input. Let's illustrate this process. Real world data is used as an input to train a machine to learn to recognize patterns in the data. Based on this recognition of patterns, it can then generate new data when giving a question or a prompt. You may recognize that this sounds somewhat similar to what I just talked about when defining predictive analytics. In predictive analytics, you also use real world data, and based on that data, you try to find patterns that help you predict events in the future. You're right. In fact, you could say that GenAI is a very sophisticated form of predictive analytics. It's no wonder that the advancements in GenAI are having a big effect on creating predictive models in data analytics. Generative AI contributes to predictive analytics in several significant ways. First, GenAI can help even before the modeling starts by generating synthetic data to add to the real world data set you already have. This is particularly useful when real data is limited or sensitive. This helps in building more accurate and robust predictive models. In the modeling phase, GenAI can help with feature engineering. GenAI can automatically create new features from raw data that are more predictive of future outcomes, saving data scientists significant time and effort in the feature engineering process. GenAI can also help with model improvement. It can assist in designing and tuning predictive models, helping to create more accurate and reliable forecasts. We already looked at data synthesis before. Now let's take a closer look at GenAI's role in feature engineering. First, let's clarify what feature engineering is. Imagine you're trying to predict how well a student will do on a test. You might look at various pieces of information about a student, like their study habits, attendance records, and previous test scores. These pieces of information are what we call features. In data analytics, feature engineering is the process of selecting, modifying and creating new features from raw data to improve the performance of a predictive model. Good features are crucial because they help the model make more accurate predictions. Now, feature engineering can be quite complex and time consuming. It often requires deep domain knowledge and lots of experimentation. Generative AI makes the process much simpler and faster. How does GenAI help? First, GenAI can automatically create new features from the raw data. For example, if you have data on customers' shopping habits, GenAI might create new features, like average purchase value or most frequent purchase day, which are more predictive of future buying behavior than the raw data alone. GenAI can also improve data quality. GenAI can identify and correct errors or inconsistencies in the data, which helps ensure that the features used in the model are accurate and reliable. Finally, using GenAI in feature engineering saves time and effort. Instead of manually analyzing the data and creating features, GenAI automates this process, allowing data analysts to focus on interpreting results and making decisions. Let's consider a practical example. Suppose you're a data analyst at a retail company, and you want to predict future product demand. You have raw data on past sales, customer demographics, and marketing campaigns. Without GenAI, you might spend days or even weeks manually creating features, like average monthly sales or customer age group preferences. With GenAI, the system can analyze the raw data and automatically generate these features for you and even suggest additional features that you might not have thought of such as seasonal sales strengths or impact of specific marketing campaigns on sales. Feature engineering is an important part of predictive modeling, and the features will become crucial in the actual model that's developed. Building that predictive model involves selecting an algorithm, training it on historical data, and then fine tuning it to make the most accurate predictions possible. GenAI can help in this process by enhancing these steps to get better performance from your models. We refer to that as model improvement. I won't go into detail on how this happens exactly, as that is more the domain of data scientists. But it's good to realize that GenAI plays a crucial role in improving predictive models by assisting in both their design and tuning. This leads to more accurate and reliable forecasts, enabling better decision making and strategic planning. Imagine you're a financial analyst, tasked with predicting stock prices. You have historical stock data and want to build a model that can accurately forecast future prices. Without generative AI, you might spend weeks testing different algorithms, manually adjusting the model architecture and tuning its parameters to find the best combination. With GenAI, this process becomes much more efficient. GenAI analyzes the historical stock data and suggests what algorithm your model should be based on. GenAI can then help design and optimize the model, helping you generate the most accurate predictions possible. Not only will they be more accurate, but you will also be saving a lot of time by using GenAI to automate this process. Generative AI is revolutionizing the field of predictive modeling by enhancing data synthesis, automating feature engineering, and optimizing model design and tuning. These advancements allow for the creation of more accurate and reliable predictive models, making sophisticated analytics accessible to a broader audience. As you embark on your data analytics career, it's important to leverage GenAI where you can, as it will help you gain deeper insights, forecast trends more precisely, and make informed decisions that drive innovation and success.

**Case Studies: GenAI in Data Analytics**

In this video, we'll explore how leading companies are leveraging GenAI to solve complex problems and enhance their data processes. This will help you understand the far-reaching applications of GenAI and data analytics. More specifically, we'll look at the use of GenAI for data synthesis, data cleaning, and predictive modeling. Before we discuss the case studies, let's review the key concepts of GenAI's influence on data analytics. First, data synthesis. This refers to using GenAI to generate synthetic data that mimics real-world data, which is useful when actual data is limited or sensitive. Next, data cleaning or automating the identification and correction of errors in data and intelligently filling in missing values. Finally, predictive modeling, enhancing the design and tuning of predictive models to create more accurate and reliable forecasts. Let's start with our first case study on data synthesis. American Express is a global financial services corporation, renowned for its credit card, charge card, and travel-related services. American Express needed to develop models for detecting fraudulent transactions. However, they faced a significant challenge. Their transaction data was both limited and sensitive. Posing privacy concerns. To overcome this challenge, American Express turned to a solution using GenAI that allowed them to create synthetic transaction data that closely resembled real transaction patterns without using any actual customer data. This synthetic data enabled American Express to train robust fraud detection models. As a result, they improved their ability to identify fraudulent transactions, reducing financial losses, and enhancing customer trust. GenAI can generate high-quality synthetic data, allowing companies to develop effective models while preserving data privacy. Now, let's move on to our second example, which focuses on data cleaning. Airbnb is a popular online marketplace for lodging, primarily homestays for vacation rentals and tourism activities. They want to ensure that listing descriptions, prices, and availability information are accurate and up to date. However, with millions of listings across the globe, Airbnb faces the challenge of maintaining high-quality data. Airbnb uses GenAI to automate data cleaning process. Their models identify and correct inconsistencies in listing descriptions, adjust prices to reflect local currency and market rates accurately, and updates availability information. This considerably improves data quality and leads to a better user experience on the platform with more accurate search results and reliable booking information. This also helps Airbnb optimize pricing strategies and availability management. GenAI can significantly improve data quality by automating error correction, and intelligent imputation leading to more reliable analytics and better user experiences. Finally, let's look at our third example, focused on predictive modeling. Netflix is a leading global streaming service that offers a vast library of video content on demand. To stay ahead of the competition, Netflix wants to provide its users with personalized recommendations, so they will choose to watch more of their programming. To do that, Netflix needs to accurately predict user preferences and viewing behaviors, so it can provide them with recommendations that work for them. Netflix employs GenAI to enhance its recommendation algorithms. GenAI automates the creation of new features from raw viewing data and continuously tunes the predictive models for improved accuracy. This approach has led to highly personalized content recommendations, which increases user engagement and satisfaction and reduces churn rates. GenAI helps in creating and tuning sophisticated predictive models, resulting in highly accurate forecasts that drive user engagement and business success. Generative AI is transforming data analytics in the real world. GenAI enables businesses to unlock deeper insights and make more informed decisions by automating complex tasks and improving data quality and model accuracy. The potential of GenAI in data analytics is fast, and its applications are continuously expanding. GenAI can enhance every stage of the data analytics process, making it more efficient and effective. I encourage you to think about how you can apply these concepts in your own projects or workplace.
