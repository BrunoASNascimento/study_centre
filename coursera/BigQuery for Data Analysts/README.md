
#TODO: Finish the README.md file
# BigQuery for Data Analysts

## Course Introduction

### Course Introduction

Welcome to the course BigQuery for data analysts. This course is designed for data analysts who want to learn about using BigQuery for their data analysis needs. Through a combination of videos, labs, and demos, we cover various topics that discuss how to ingest, transform, and query your data in BigQuery to derive insights that can help in business decision making. Let's take a look. In the first module, we look at analytics challenges faced by data analysts and compare big data on premises versus on the Cloud. We then introduce big query, which is Google Cloud's enterprise data warehouse and review its features that make big query a great option for your data analytics needs. Finally, we'll learn from real world use cases of companies transformed through analytics on the Cloud. The second module is all about exploring your data with SQL or structured query language. We go from very simple select statements to more complex queries that explore various data sets. Of course, not all data is in the form that we need it to be, which is why in Module 3, we will discuss principles about data integrity and then we look at how to use SQL to clean, prepare, and transform your data. The last section of this module also briefly introduces other products like Dataprep, Cloud Data Fusion, Dataflow, Dataproc, and Dataform that can help with data preparation and transformation. Module 4 talks about ingesting and storing data into big query native storage. We discuss when to use extract and load versus extract, load and transform versus extract, transform and load approaches for loading data into BigQuery. We also cover external data sources where you can run your query in BigQuery, but the data is hosted outside of BigQuery. Module 5 is where all that hard work around ingesting, cleaning, preparing, and transforming your data comes to fruition as you get to visualize insights from your data by building insightful dashboards and reports. We start off with a little visualization theory and some best practices and then look at tools like Looker Studio and connected sheets that can connect to BigQuery and help create impactful visualizations to capture and convey your insights. Although SQL is a powerful query language, programming languages such as Python, Java or R provide syntaxes and a large array of built in statistical functions that data analysts might find more expressive and easier to manipulate for certain types of data analysis. Such tools include open source web based applications like Jupyter Notebooks, and so we discuss these as well. Creating, maintaining, and versioning SQL pipelines is a lot of hard work and many times data analysts have to use multiple tools to achieve this. In the next module, we've reduced data to form a new product that offers a unified end-to-end experience to develop version control and orchestrate SQL pipelines in BigQuery. Finally, we introduce BigQuery Studio. BigQuery Studio brings an end-to-end analytics experience in a single purpose built platform. In the module, we start off by discussing the motivation behind building it, then we discuss its features and capabilities in more detail, ending with a demo and a hands on lab. We are excited for you to learn about BigQuery and what it can do for your data analytical workloads. Let's jump into the first module.

## BigQuery for Data Analysts

### Module overview

Welcome to module 1, BigQuery for Data Analysts. In this module, we will highlight analytics challenges faced by data analysts and compare big data on premises versus on the cloud. Next, we discuss BigQuery, which is Google Cloud enterprise data warehouse, and review its features that make BigQuery a great option for your data analytics needs. And finally, we'll learn from real-world use cases of companies transformed through analytics on the cloud.

### Data analytics on Google Cloud

Let's start by looking at common analytics challenges faced by you, the data analyst. Data analysts, data engineers, and data scientists all analyze datasets to gain insights. However, they have different roles. Data analysts primarily gather and interpret data to solve problems and help companies make decisions. They use SQL and and static modeling techniques to summarize data, build reports, and visualizations and derive insights. Data engineers build systems for collecting, validating and preparing data. They develop and maintain data pipelines. Data scientists use dynamic techniques like machine learning to gain insights about the future. It is important to understand that they may or may not work in silos. For instance, a data analyst might partner up with a data engineer to work on sourcing the data and ingesting it into the system where analysis will take place. And it is not uncommon that these roles are sometimes performed by the same person. Which is why, although the focus of this course is on data analysts, we will cover topics like ingestion and storage and give a little preview of doing some machine learning in BigQuery. Two of the most common barriers a data analyst will run into are either too much data or data that is not connected together. Three themes are usually common querying, where the queries take too long or have very complicated logic. Infrastructure where you see issues with scalability, maintenance, etc. Storage where capacity may be a concern or pricing could be an issue. Or maybe not all data is centralized. Again, it is imperative to note that data engineers and data scientists also face similar challenges when it comes to storage, infrastructure and working with the data. Well, the good news is Google Cloud can help you with storage and infrastructure. It has also been designed with scalability in mind an actual global elastic cloud so that you can focus on query writing and data analysis to unlock those insights even faster. We will discuss that briefly in the rest of this lesson as we compare on premises servers and infrastructure with Google Cloud. Let's start with storage. In December 1981, the cost of a ten megabyte hard drive was $300,000. In 2023, a 1 TB hard drive is less than $20. Storage has become so much cheaper. Although hard drives are cheap, they're not the only thing you need to have to query big data. You will also need computing power, networking, admin and hardware teams to maintain and upgrade your infrastructure, and not to mention software and software license costs. But even after managing all that, how do you find the needle in the haystack of data? Traditional databases are not up to the challenge of data of this scale. If you look at most big data projects and really look at where time and money is being spent, you see that most of the time isn't spent getting insights from the data. It's spent on the care and feeding of the machinery, managing infrastructure, manipulating data, monitoring and performance. Whereas what you really want to spend your time on is data analysis. We realized this ourselves at Google several years ago, and so we started developing systems that one scale with your data growth even as it explodes. Two, are managed so that you aren't wasting time on dealing with all of the underlying complexities and three are just generally magically awesome so you can get back to data insights and not data management. The message we are trying to convey is don't build it yourself when you can leverage Google's massive infrastructure. And speaking of infrastructure, here is a map of Google Cloud's network infrastructure. The number of regions and zones is continually increasing. An up to date view can be found at Cloud.Google.com slash about slash locations. The network and points of presence are available in the Network tab at that URL. The edge points of presence are the locations where Google networks are connected with Internet service providers to allow users to connect. The Google Cloud network strongly distinguishes Google from other cloud service providers. The points of presence allow Google to provide very low latency network performance. The other big benefit is of course scalability. If you manage your own infrastructure, you still pay for power for unused CPUs on premises, especially if you have storage disks and CPUs on the same servers. But with Cloud, you can use the same processing power as having 120 machines to process 1 TB in 1 second, just as if you had one machine process the query in 120 seconds. And the scaling up and down is automatic, so there is no guesswork for demand. If you need more, the Cloud scales up elastically and then goes away when not in use. One of the key features of Google Cloud is the decoupling of storage and compute power. With an on premises setup typically both storage and compute are co located right on that server, which means that you pay for the ability to use processing power even when processing is not happening. With Google Cloud, the separation of compute and storage ensures that you pay only for the resources you use. So if you are only using storage space, well, that's what you get billed on. BigQuery combines all these great features that we just discussed and powerful analytic tools to help data analysts focus on data instead of resource management. BigQuery is Google Cloud's fully managed enterprise data warehouse that helps you manage and analyze your data. BigQuery's serverless architecture lets you use SQL queries to answer your organization's biggest questions with zero infrastructure management. BigQuery's scalable Distributed Analysis Engine lets you query terabytes in seconds and petabytes in minutes. It BigQuery maximizes flexibility by separating the Compute Engine that analyzes your data from your storage choices.

### From data to insights with BigQuery

In this section, we will highlight the five common tasks of any data analyst and discuss how BigQuery can help with these tasks. In an earlier section, we briefly described the role of a data analyst and compared it with other data roles like the data engineer and data scientist. Ideally, the data analyst wants to be spending more time doing the last two tasks shown here, analyzing and visualizing data and we will spend a considerable amount of time discussing these topics. But the data analyst may also have to deal with ingesting the right data and that data may need some cleaning and transforming before you can analyze it. You may want to store it in a certain way for faster retrieval and so we will spend some time discussing these themes as well. Now, each of these tasks has its own challenges. For instance, analysis may get harder because queries are running slow and that could be because of the huge volume of data you were working with. Or the logic may be rather complicated, resulting in a lot of transformations that are done on the fly. All these make it harder for data analysts to get useful, actionable, timely insights from their data. In this course, we look at how BigQuery can help you with each of these data analyst tasks. In the next few slides, we highlight key features of BigQuery that not only make it a great solution for your analytical workloads, but also do it in a scalable, economical, and impactful way. Let's take a look with BigQuery, you get the benefit of Google Data Center backed infrastructure that is fully managed. The best part is that you don't need to spend your time optimizing the specific hardware and networking. Your job as a data analyst is to focus on asking great questions of your data and hunt down interesting insights. Now let's expand on specific features of BigQuery. BigQuery is a fully managed enterprise data warehouse which provides near real time interactive analysis of massive datasets. It runs on Google's fully managed, secure, high performance infrastructure. By no ops we mean no administration for performance and scale. The data is replicated across multiple data centers, giving you peace of mind. You only pay for the storage and processing you use. Secure your data through access control lists, ACL's, and identity and access management, IAM and rest assured that data is encrypted in transport and rest. Auditing is important for many users and stakeholders. Google Cloud audit logs track admin activity, data access, and system events. You can also know who did what, where, and when in BigQuery. When it comes to scalability, BigQuery's virtually unlimited data storage and processing power leads to a highly parallel and distributed process model. Which means very fast queries flexible with streaming, ingestion of real time data BigQuery makes the data available for analysis even more quickly without you having to wait for all the data to be available and you have the flexibility to mash up your data across diverse datasets and projects, including a huge collection of public datasets that BigQuery makes available to you, easy to use. Data is stored in denormalized tables, simple schemas. There is columnar storage for high performance. It requires no indexes, keys, or partitions. It has a familiar SQL interface and intuitive UI. There is nested and repeated field support for schema flexibility. It supports open standards, so analysts can use their preferred tools. There are four ways to interact with BigQuery, the BigQuery Web UI, the command line interface, CLI, the REST API, and third party GUI SQL editors. Since this course focuses on using BigQuery for data analysis, you spend most of the time using the BigQuery Web UI. In the hands on labs, you learn how to examine tables, quickly, build queries with a few simple mouse clicks, determine how much the query will process and so on, all through the BigQuery Web UI. You can also use the CLI to execute queries and explore big query features. The CLI contains a robust set of commands that provide you the flexibility to run commands and queries interactively. A third option is the REST API, which is the programmatic interface that programming languages like Java and Python use to communicate with BigQuery. The service receives HTDP requests and returns Json responses. Both the Web UI and the CLI use this API to communicate with BigQuery. Finally, there's many GUI SQL editors like Looker, Appsmith, retool, cogniti, super query, PopSQL, Zing data, and many more that can interface with BigQuery. Note that the REST API and the GUI SQL editor tools are beyond the scope of this course. If you had a chance to complete the recommended prerequisite course, Introduction to Data Analytics on Google Cloud, you'll remember that BigQuery is a two in one service. The storage layer provides an economical and efficient option for your data storage needs and has tools to help with the ingestion process. With support for many data formats from a variety of data sources, the analysis engine is optimized to run analytic queries on large datasets, including terabytes of data in seconds and petabytes in minutes. Understanding its capabilities and how it processes queries can help you maximize your data analysis investments. It is worth mentioning that BigQuery also supports querying data that resides outside of BigQuery storage, and we will talk about this in a later module. Here's a quick recap from the Introduction to Data Analytics course on how resources are organized in BigQuery storage. Remember, tables reside in a dataset and datasets exist within a project. Of course, you can store other resources like views, routines, et cetera in the dataset, which we will discuss in a later module. You can think of a dataset as a collection of tables, views, and so on.
Let's log into the BigQuery user interface to demonstrate how you can query terabytes of data in seconds.

### BigQuery demo

First things first, let's open BigQuery, which can be found in the navigation menu. Now, this is what the BigQuery user interface looks like. Every resource in Google Cloud exists inside a project, which you can view here. In addition to security and access control, a project is what links usage of a resource to a credit card. It makes a resource billable. BigQuery has numerous public data sets that anyone can query. One of these is all public Wikipedia page metadata. Next, let's simply paste and run a SQL query to see how fast BigQuery can scan and process 10 billion rows looking for the word Google in the Wikipedia page title? BigQuery's Scalable Distributed Analysis Engine lets you query terabytes in seconds and petabytes in minutes. The query includes filtering, grouping, and ordering on a data set that contains 10 billion records and runs quickly and efficiently in BigQuery. In BigQuery, data is stored inside datasets. Datasets contain tables and tables contain columns. Access to data in BigQuery is controlled at many levels, from the project and data set to the individual table, column, and row. A BigQuery slot is a virtual CPU used by BigQuery to execute SQL queries. BigQuery automatically calculates how many slots each query requires, depending on query size and complexity.

### Real-world use cases of companies transformed through analytics on Google Cloud

In this lesson, we will examine real-world use cases of companies where moving compute and storage to the cloud has enabled them to scale their data analysis. While these customer success stories involve multiple Google Cloud products, we will focus on how their use of BigQuery transformed their operations through faster analysis and useful insights. After a technical evaluation in 2018, MLB Major League Baseball decided to migrate their enterprise data warehouse from Teradata to Google Cloud's BigQuery. They successfully completed a proof of concept in early 2019 and ran a project to fully migrate from Teradata to BigQuery from May 2019 through November 2019. With the migration complete, MLB has realized numerous benefits in migrating to a modern cloud-first data warehouse platform. BigQuery made it easy to securely share datasets with any G suite user or group with the click of a button. Users can access BigQuery's web console to immediately review and run SQL queries on data that is shared with them. They can also use connected sheets to analyze large datasets with pivot tables in a familiar interface. In addition, they can import data from files and other databases and join those private datasets with data shared centrally by the data engineering team. With BigQuery's on demand pricing model, MLB was able to run side-by-side performance tests with minimal cost and no commitment. These tests involved taking copies of some of their largest and most diverse datasets and running real-world SQL queries to compare execution time. As MLB underwent the migration effort, BigQuery cost increased linearly with the number of workloads migrated. By switching from on demand to flat rate pricing using Bitquery reservations, they were able to fix their costs and avoid surprise overages and share unused capacity with other departments in their organization, including their data science and analytics teams. Customer first service means shipping direct, having the right tool in store, teaching new skills, and anticipating customer needs everywhere. The Home Depot empowers its associates with Google Cloud's BigQuery by providing timely data to help keep 50,000 plus items stocked at over 2000 locations to ensure website availability and provide relevant information through the call center. Migrating to Google Cloud the Home Depot's engineers built one of the industry's most efficient replenishment systems, then figured out how to get more done using BigQuery for streaming application performance monitoring. The Home Depot runs more than 600 projects in Google Cloud and analyzes 15 petabytes of data in BigQuery. Let's summarize the key things we looked at in this module. We started by highlighting analytics challenges faced by data analysts and compared Big data on-premises versus on the cloud. Next, we talked about BigQuery, which is Google Cloud enterprise data warehouse, and reviewed the features that make BigQuery a great option for your data analytics needs. And finally, we looked at some real-world use cases of companies transformed through analytics on the cloud.

## Exploring and Preparing your Data with BigQuery

### Module overview

Welcome to Module 2, Exploring and Preparing Your Data with BigQuery. In this module, we will start by looking at common data exploration techniques when working with BigQuery datasets. Next, we focus on writing SQL or structured query language in BigQuery. Both the examples in the lessons as well as the practice through the Hands-On labs in this module use the BigQuery public datasets and the course dataset. If you have experience in SQL, that is great, and some of the queries we cover here will be rather familiar. But if you are new to SQL, don't worry, because we start right from the basics and move quickly to more advanced query constructs with aggregates, functions, unions, and joins in later lessons.

### Common data exploration techniques

Let's talk about your different options that are available for exploring your datasets. There are three primary options for exploring a dataset, write some SQL and use it in a SQL editor like the BigQuery UI, use a Data Preparation or BI tool. Examples include Data Prep, SQL Runner and Looker Studio, Cloud Data Fusion's Wrangler Tool and and so on. Visualize your raw data with many other third party tools, SQL syntax hasn't changed all that significantly since the 1980s. And out of the three options listed, is the most core to your skill set as a data analyst. But that said, as a data analyst, you're not necessarily limited to just using SQL and the BigQuery web UI. There are data preparation and visualization tools that you can use as well, and we will briefly cover them later in this course. In the rest of this module, we will focus on using the SQL approach in the BigQuery web UI, why? Because SQL is a very good skill to have, almost an imperative skill to have as a data analyst. And it's one of the fastest ways you can interact with data inside of BigQuery, and of course, because it's fun. Exploring a dataset through SQL is more than just writing good queries. You need to know what destination you're heading towards and the general layout of your data. Good data analysts will explore how the data set is structured, even before writing a single line of code, ask yourself these questions what type of data am I interested in? For example, financial, nonprofit organizations, etc, what specifically about that data am I interested in? It could be organization revenue, perhaps, how do I want to receive the results? Say you want it sorted by the highest revenue first. Next, think of the data set, what datasets do I have available to me? Do I need to find and upload my own? How is the data structured? Are there multiple tables? Important fields? How much data is there to explore? And finally, think of the SQL query, how do I translate my question into a SQL query? Is my data clean, what fields should I focus on? Do I need to perform any aggregations? Do I need data from multiple tables? These steps are essential to writing good queries that will look through data in the right places and give you the insights you need in a fast, efficient manner. And we will go through all these steps in the rest of the module as we write queries to get insights from our data sets.

### Analyze large datasets with BigQuery

Let's quickly talk about some of the datasets we will run our SQL queries on. A public dataset is any dataset that is stored in BigQuery, and made available to the general public through the Google Cloud Public Dataset program. The public datasets are datasets that BigQuery hosts for you to access and integrate into your applications. We will use a few of these datasets in this course as we show you how to build and run SQL queries in the BigQuery UI. One of the hardest parts about making a course like this is finding a practice dataset that any analyst can relate to, and want to query for insights. In this course, we're excited to bring you a real products and e-commerce dataset with over a million site hits in a year of transaction records from Google's own online store, where you can buy Google branded merchandise. These transactions have been recorded through Google Analytics and made publicly available through BigQuery. With the knowledge of BigQuery and SQL that you'll practice throughout this course, you'll be able to answer real world e-commerce questions like what is the total number of transactions generated per device browser? Which customers have added items to their cards but then abandoned them? How do I set up cohorts, or a segment of my customers, based on their behavior, so I can target each one of them in a more personalized way? Or even what keywords and referring partner sites have led our visitors to our e-commerce site, and what pages did they visit while they were there? The great part about working with this dataset is that all the queries and the insights that you derive here, are going to be directly applicable to your own Google Analytics datasets. Now if you're not a fan of e-commerce, the analytical tips and techniques that we'll cover here are sure to be useful in your own queries.

### Query basics

For the most part, queries in BigQuery are the same as when using other databases, there is an international standard for SQL with each version referred to with the term NC and the year it was adopted. BigQuery SQL conforms to the NC 2011 version of the standards, there are some minor differences specific to the platform as is common with most SQL based databases. All the usual SQL constructs like Select, Wear, Join, Brow, Pi, Order Bi, etc are supported. You will find the aggregation functions typical of other databases, count, some, mean, max, etc, some functionality has been added to support features unique to BigQuery arrays instructs, geography, and BigQuery ML, let's quickly review some SQL query basics. The basic construct of a SQL query starts with the select keyword followed by one or more columns that you want to display in your result, in this select statement, we want to look at the name and revenue columns. Of course, you have to specify which table the data resides in using the From keyword and in this example, it is the Sales table, you can also filter your rows by using a war clause, like in this example, you are only interested in seeing product names whose revenues are above $100,000. Take note that you can filter on multiple columns too, so you can say where revenue is greater than 100,000 and the category equals toys, and now you will see only rows that match these two conditions. Optionally, you can order or sort your results based on a certain column, like in this case, you order it by revenue. The default is to use the ascending order, which goes from the smallest to the biggest value, but you can use the DESC suffix to specify descending order, and when you use the wear clause and the order by clause together in the original query, you get rows of data where the revenue is above $100,000 and the results are sorted from the highest revenue to the lowest revenue. Take note that you can also use order by on more than one column, so in this example, rows that have the same revenue amount will be further sorted using that category column. Sometimes you only want to see a subset of rows and you don't really have any matching conditions yet for your where clause, maybe you are just exploring the data for now, like in this example, say you only want to see ten rows from the table. You add the limit keyword followed by the number 10, which will get that number of rows and they will be the first 10 rows based on the order the results are sorted in. Also, it's good practice to comment your SQL queries because after a while you will become an expert and start writing long complex queries, so it's always a good idea to have an explanation about what the query does. Now let's use that SQL knowledge and run some queries on the E-commerce data set in BigQuery, first, do you know what's missing in this query? Remember that tables and BigQuery exist within a data set, so it's important to specify the data set name. Tables exist in data-sets and data-sets exist within projects. If you omit the project ID, BigQuery will assume the project is your current one, this could be an issue when, for example, you are trying to access public data sets as they would be hosted in the BigQuery public data project, so it's a good idea to use the fully qualified name using the notation project ID data set table name. Use back ticks to get a suggestions list as you type out the fully qualified name of the table, starting with the project ID. Do pay attention to the error messages as they provide good guidance and hints to resolve issues with your queries. Be careful when using the Star Wildcard, which will select all columns of your table. If the intention is to explore the schema of a new table, you can use the schema tab. Running Select Star without a limit or were clause on a large table will cost a lot as the entire table, all rows, all fields are read and returned. You can also explore by looking at all column data for a subset of the rows by using the Preview tab. Sometimes there may be a genuine need to present all columns it in a report, so if you must use Select Star, be sure to include a limit or a where clause if it applies, take note that using Preview does not incur any query costs. Here is a high level overview of how big query charges for usage, for more details on the pricing numbers, you can go to cloud.google.com/ BigQuery/ pricing. Jobs like load batch data, copy, and extract are free. Loading streaming data, however, does incur a cost, and storing data in BigQuery is a separate cost. The BigQuery WebUI also has a query validator on the top right of your SQL query editor window, which provides an estimate of the number of bytes required to process the query. Take note that you get one terabyte per month of query processing at no cost. You can also use the Google Cloud Pricing calculator to get estimates for your storage and processing costs. For queries, BigQuery charges based on the number of bytes processed, which is basically the bytes read to complete the query. Recall that traditional data warehouse costs include hardware, licensing, and maintenance. BigQuery is a serverless data analytics platform. You don't need to provision individual instances or virtual machines to use BigQuery. Instead, BigQuery automatically allocates computing resources as you need them. Let's do a quick recap. List the specific columns in your select statement that you want to see in your results. Be specific about the table name, which means you may want to include the project ID and the dataset as a prefix and use a limit and or a where clause when possible. If you run the exact same query twice, you will get the advantage of query cache. The second attempt of the query will try to use results from the previous run of this query, as long as the reference tables are unmodified. If cached results are returned, you will not be billed for any usage. Results are cached for approximately 24 hours. The default setting in the BigQuery UI is to use cached results. However, caching cannot be enabled when a destination table is selected. We will talk more about destination tables in a later module. BigQuery will also not use cache with a slight change in query. For example, if a space or a blank line space are added, it will not use cache even though it is essentially the same query. Of course, the query cache also won't make sense if you have non deterministic elements in your select list. In this example, you see the use of the current time stamp function, which returns the time at which the query runs, along with some of the column data. This is also a good example of how you would use a function in your query. We will talk more about functions in a while. Now we did see examples of using the order by, and limit clauses in previous SQL queries. Combining them in the same query helps you when you need to do top end analysis. For example, the top 10 products by revenue or top five most popular patents, etc. In this example, you are ordering the rows by the time on site column, which shows the amount of time the website visitors spent on the site. You have limited the query to show only 10 rows, in other words, the top 10 results. If your resulting rows have duplicates, like what you see in the result on the left here, then use Select Distinct to remove duplicates. Another thing you may have noticed is that the select distinct column shows durations in seconds. You can use a simple mathematical formula to change that into minutes. Since you are modifying the original column value into a different denomination, why not call it something else? In this example, you can use an alias with the As keyword. Notice though that the results show a lot of decimal places for the time on site duration in minutes. How can we clean that up a little? Well, you can use the round function that will round up the value to two decimal places. Functions are a great tool to use in your SQL queries. They can help perform certain actions on your column data, like calculations or transforms before displaying them in the output. In the query you just looked at, we use the round function to take two inputs. The first input is the value of the time on site duration in minutes. The second input is the number of decimal places you want to round up the value to. The result is the duration in minutes, but rounded up to two decimal places, as you would have expected. There is a long list of functions that you can use and we will discuss them later. Take a look at this query. Do you have any idea why the query fails? Well, newly defined aliases in the select statement cannot be used yet for filtering rows in your where claws. The reason for that is that when the query reads the table from disc, it filters for the columns you want returned, which is basically your where clause. At the time of pulling the data from disc, your alias columns in the select statement are not interpreted yet and are thus not available. The way to get around this is to use the same calculation, or formula, or function you had used in the select list again, in your where clause, like what you see in this example. If you not want to repeat the calculation in the where clause, consider starting with a sub query and then filtering, and we will discuss this later. Take note that you can use alias fields in the order by clause though, like what you see here. You can also use aliases in the group by and having clauses which we cover later.

### Lab intro: Exploring an Ecommerce Dataset using SQL in Google BigQuery

In this lab, you explore the E-commerce dataset using SQL in Google BigQuery. This dataset has millions of Google Analytics records for the Google merchandise store loaded into a table in BigQuery. More specifically, you will start by accessing the dataset in the BigQuery UI and explore the table schema and metadata. The lab then walks you through a number of interesting queries, like total unique visitors, listing top five most viewed products, etc, and also look at ways to identify and remove duplicate information.

### Working with functions

We will be introducing a number of functions to you in context over the rest of this module. String manipulation functions are great for changing the string's format or doing simple transforms like making every letter uppercase or pulling the left five characters. The CONCAT function, for example, concatenate one or more string or bytes values into a single result. You can add a FORMAT function to format the results in a certain way, for example displaying revenue column values with commas so it reads much better. Here are a few common string functions, the CONCAT function concatenates two or more strings together. The ENDS-WITH function returns true if the second string is a suffix of the first. Easily convert your strings to lowercase with the LOWER function. The last function here returns true if the value is a partial match for the regular expression. You can find a lot more interesting string and regular expression functions with examples on our documentation site. If you are doing very simple pattern matching for strings, then you can use the LIKE operator in the WHERE clause. This example uses a string function on the name to render the product name in lowercase and then finds the word shirt anywhere in the name. The use of the LOWER function here is important because we're unsure whether the product name field is uppercase, lowercase or a combination of the two. Hence we convert it before comparing it. It is also worth mentioning that while this is a valid approach, it is also a very high overhead approach as all values of v2ProductName need to be read and converted to lowercase before the LIKE comparison can be processed. Aggregation functions can help perform calculations over a set of values like the sum or count or minimum or maximum. You can see the COUNT function here giving the total number of unique users as it aggregates the total number of distinct visitor IDs. This kind of information is great when looking at website traffic. Can we do more though? We know that there is country information captured in the table as well. Can we do a total count by country? The answer is yes, and this is where you get to use the GROUP BY clause. So essentially you are getting the aggregate total. But after grouping the visitors based on a certain column, which in this case is the country, now the output is more exciting and insightful as we can see the top five countries in terms of unique visitors to the website. Sometimes we want to filter results after some sort of aggregation. The way to do it is using the HAVING clause. In this example we want to look at visits per website visitor, but only for those who have visited more than once. And so we use the total count of visits, which is aliased as the records column in the HAVING clause. You can think of the HAVING clause as the where clause for aggregated columns. Let's look at a few more functions, for data type conversion, use the CAST function. Maybe you want to display a number column as a string, for example. Or maybe you need to cast a string into a date data type so that you can pass it to the extract function to get the month from the date string. Here are some of the common BigQuery data types. We have seen most of them in the queries and results we have discussed so far. Take note that this is not the complete list. To learn more, you can go to cloud.google.com/bigquery/docs/reference/ standard-sql /data-types. Casting allows you to treat one data type as another. As mentioned earlier, this is particularly useful if a function expects to see an input in a specific data type, like in the case of the EXTRACT function where you want to display only the month part of a date value. Now, sometimes CAST may not work because you are converting a string column into an integer, and maybe some of the values in the column are not valid. If you tried the CAST function, normally this would error out, but you can use SAFE_CAST instead. Now it just produces a null for that value instead of throwing an error. And so the question is, what is a NULL value? Well, just like black holes are the absence of light, NULLs are the absence of data. NULLs may or may not be included in aggregations by default, NULLs are respected. You cannot simply do if value equals NULL, since NULLs can never be equivalent to anything. NULL is not even equal to NULL. So if you want to filter out NULL values, you can specify is not NULL in the WHERE clause. This example will display only rows that have a valid transaction ID and discard the ones where the transaction ID value is NULL. Here is a scenario where you have multiple rows with identical data for multiple columns, like the visitor ID, transaction ID, date, and so forth, and only one column where the data differs, which is the product column. Is there another way to represent this? One solution is to aggregate the various product names into a single cell in the final result, note the use of the STRING_AGG function that concatenates strings from different rows into a single string. And so here you see one row per transaction ID because it was one of the grouping columns and the list of products all combined together into a single string. Another possible solution is to use the ARRAY_AGG function. The function works in the same way by combining all the products together, but presentationwise it's a little different. As you can see here, instead of a really long string of product names mashed up together, here you see an array of product names which is a lot more readable. The same grouping applies so each transaction ID is only displayed once. This example uses the array aggregation function on the product name and the product quantity, so you can see the amount bought per product and a nice summary of the number of distinct products per transaction. There are many other different kinds of functions which are not discussed here. Feel free to refer to our documentation site. Let's wrap up this lesson by talking about the WITH clause. A WITH clause contains one or more common table expressions, or CTE. A CTE acts like a temporary table that you can reference within a single query expression. Each CTE binds the results of a subquery to a table name, which can be used elsewhere in the same query expression. Let's summarize what you've learned so far in this module. As you have seen, exploring massive datasets with SQL in the BigQuery UI is what the tool does best and what you will need to master as a data analyst. Use comments to improve the readability of your queries and make best friends with the query validator. It is also worth mentioning that there is an interactive BigQuery SQL translation tool in the BigQuery UI that provides translations between various supported SQL dialects. So for instance, if you have a workload in Amazon Redshift SQL, this tool can help translate it for BigQuery. Lastly, if you're looking for other datasets to explore, complete with example, SQL queries, search for BigQuery public datasets to see the full list. Let's test your knowledge with an interactive lab where you get to run queries on the ecommerce dataset.

### Enrich your queries with UNIONs and JOINs

In this lesson, we will talk about using unions and joins to enrich your queries. For this lesson, we will use the temperature readings and the weather station data available through big queries, noaa public data set. Since we will use these two table types in our examples, let's describe them in a little more detail. We have a separate table for all daily weather temperatures since 1929. There is one table for daily temperature readings in the year 1929, one for 1930 and so on. That's a lot of tables for us to query and combine. But don't worry, it won't be so bad. Our weather station location details like latitude, longitude, state, station name, etc, are stored in a single look up table. Important fields like country and state are not present in the daily temperature table because of a concept called normalization that we will come to later. But we can look these fields up by joining the tables together. But before we can link and join the two tables together, we need to first figure out what linking field they have in common. What is our unique identifier for weather stations? Is it USAF, US Air Force Station ID. Or WBAN, Weather Bureau Army Navy. Well, let's investigate. In the rest of this lesson, you will learn how to join and union the data across these many tables for enriched insights. Let's start with union. As mentioned earlier, unions are for appending data from various tables. For example, you want to see temperature recordings for the years 1929 and 1930. A simple way is to select rows from the 1929 temperature table, then select rows from the 1930 temperature table and union the results. Unions append more data to your table. Vertically union requires the tables or select statements to have identical schema, and the result two will have the same number, type, and order of columns. Here is what the query would look like. Notice the use of union distinct, which removes any duplicates. You can use union all if you don't have or care about duplicates. Note that when using union, the number of columns and the data types of each column must match respectively. Now what if you want to union tables for 1929 all the way to 2023? Typing all those unions by hand seems tedious. Well, the good news is a wild card table represents a union of all the tables that match the wild card expression. In this example, The FROM clause uses the wild card expression to match all tables in the Noah underscored Sad dataset that begins with the string, Sad and magically unions all of the rows for you. That sounds great, but what if you want to union from 1929 all the way to 2023, but skip some of the years in between? Well, not to fear there is a trick for that too. Include a collection of tables with the table wildcard. Then filter them with underscore table underscore suffix as shown in the example here. Think of this as a special hidden metadata column that you can use in your WHERE clause for filtering. Unions in SQL require careful handling of schemas between tables because things may change over time. If the number of columns or the data types do not match, the query may fail. Combining all temperature records over the years is great, but we are missing the weather station details which are captured in a different table. How do we enrich our temperature data with station details? For starters, it may be important to see the weather station data in the results. Beyond that, you can think of doing really insightful aggregations and categorize results by groups to assist in decision making. Essentially joins enricher dataset by potentially adding fields from two or more tables. Let's take a look. Join requires the same field in each table to connect on. Here is a good place to introduce the concepts of a primary key and a foreign key. Let's use a simpler example to illustrate this, and we can go back to the weather data example right after this. Say we have an employee table with employee information and a department table with department information. The EID column in the employee table uniquely identifies every employee. That means there cannot be duplicates or null values. This is what we call a primary key. No two employees can have the same EID, in the same way the DID in the department table is the primary key because each ID is unique and not null. Each ID uniquely identifies a department. But the DID in the employee table is a foreign key. Which means that the value has to match one of the department I's in the department table, but the same value may appear a number of times in the employee's table. Because there could be many employees per department. Let's go back to the weather data example and see how we can join the two tables. Before we can link the two tables together, we need to find our unique row identifier. What is our unique identifier for weather stations? Is it the USAF number? Not really. As you can see from the above query USAF is not unique. One station could possibly have re used this ID over time or one station could have multiple recording devices. The WUBAN column also doesn't look unique by itself. But what about the combination of the two WUBAN and USAF? Yes, if we can catenate the two fields, we get a combined unique key showing 30,016 stations. When you join two tables, you basically combine data from separate tables that share a common field into one table. In this example, for each row in the temperature table, you try to join with a row from the weather station table. If the station number matches the USAF number and if the WUBAN numbers match, that's called the join condition. Aliases are optional in the select statement if the field names are unique between the tables. Joins can have multiple linking fields to establish the join on logical keys like the one shown here. The default join is an inner join, which means the records must exist in both tables for results to be shown. Let's cover the basic join types now. Inner join is what we have just discussed. This is a join condition based on which the rows are matched and returned. There is also a right outer join that returns all rows from the table on the right side of the join and all matched rows from the table on the left side of the join. The opposite of that is the left outer join. The full outer join returns all rows from both sides even if the join condition is not met. Please note that table on the left and right of the join are based on the order of the tables are listed following the from and join keywords in your select statement. There is one more type of join, the cross join, which is essentially a Cartesian product from both sides. Meaning that the rows from both tables are combined and returned. In your result set. Take note that it is normal for the join key to be unique only on one side of the join. That was the whole point behind the primary key and foreign key discussion earlier. However, sometimes when the data relationships are not fully explored or understood, you may end up missing some join conditions that could explode your resulting rows. Let's take a look at an example. The query on the left is what we have seen before. The join is based on two conditions, because the combination of the two columns is unique as we pointed out earlier, The query on the right, however, removes one of the two conditions. Notice how the number of resulting rows just explodes. There are 128 times more rows coming out of the query on the right. Doing a join using a non unique join key can result in the same output and overhead as explicitly doing a cross join. That's exactly what you see here. While the effect is like a cross join, this is a standard join with a poorly chosen key. With this dataset, it results in the same output as if the join were a cross join. Try to get a good understanding of your data model and relationships. Correctly identifying your primary and foreign key fields is crucial to get the desired results and joins. You can always use multiple join conditions if there is no single unique column like what we saw in the weather tables example. Let's summarize what you've learned in this lesson. Understanding when and how to use joins and unions in SQL is a concept that is easy to pick up, but takes a while to truly master. The best advice I can give you when starting is to really understand how your data tables are supposed to be related to each other, Customer to orders, supplier to inventory, and being able to verify if that is actually true those equal, remember all data is sturdy and it's your job to investigate and interrogate it before potentially polluting your larger dataset with joins and unions. Once you understand the relationships between your tables, use unions to append records to a consolidated table and joins to enrich your results with data from multiple sources. Let's practice these concepts in our next lab.

## Cleaning and Transforming your Data

### Module overview

Welcome to module three, Cleaning and Transforming Your Data.

One of the most critical parts of data analysis happens way before you build your first visualization. And while not necessarily the most glamorous, data preparation and transformation can't be ignored. Mostly you will use SQL and data preparation tools for this crucial step, and we will discuss that in detail in this module. But visualization tools can be a great aid in identifying the data that needs work to make it usable, and we will cover that in a later module. In this module, we're going to cover the elements of what makes a good data set and then look at two different ways to process, clean, and prepare your data. The first one, as you might expect, is through SQL queries in the BigQuery UI, but we will also briefly introduce other options that are available for preparing, cleaning, and transforming data and when to use what.

### 5 principles of dataset integrity

High quality datasets will yield high quality insights. Let's spend sometime in this lesson to talk about the five principles of dataset integrity. The majority of data quality lectures will have one central theme, garbage in garbage out. If you want to build amazing machine learning models, you can't feed them with garbage. You've got to feed them with really good data to begin with. The same goes with data analysis. You have to get extremely good, clean data in order to get good, useful, actionable insights. Let's talk about what makes quality data and some of the things that we can do to bring data to that level. High quality data sets follow these strict rules, and there are five of them. They're valid, they conform to your business rules. They're accurate because they conform to some true objective value. They're complete, meaning that you can see the whole picture and you're not just getting a subset of the data and no data elements should be filtered, truncated, or lost. They're consistent, meaning that there is harmony across the many data systems and no contradictory facts and they're uniform too. Next up, we'll look at an example of each of these. When modern database technology is used to design data capture systems, validity is fairly easy to ensure. Invalid data arises mainly in legacy contexts where constraints were not implemented in software or where inappropriate data capture technology was used. For example, spreadsheets where it is very hard to limit what a user chooses to enter into a cell if cell validation is not used. Data constraints fall into the following categories. Data type constraints, range constraints, mandatory constraints, set membership constraints, Foreign key constraints, regular expression patterns, cross field validation, and unique constraints. Speaking of uniqueness, what do all these images represent or have in common? Each is a unique means of identification. You have the phone number on the left, car, license plate number in the middle, and mailbox or address. Why would it be terrible to have duplicative license plate numbers? Speeding tickets from one person's car could be fine to someone else and for that same reason, having multiple physical addresses be the same, or people sharing phone numbers presents a massive problem. It's the same with data. Having duplicate records or having stale records like an old address is not ideal, like we saw in the example of an outer join, where the results could unnecessarily multiply with duplicate rows. Now, valid data corresponds to range constraints too. Let's use an example. Say you've got some dice on the left, and you've captured the roles of the dice in the table on the right, which value looks out of place in the table? If you answered seven, you could be right. If you answered one, you could be right too. It really depends on how many dice you rolled. If you rolled one die, then obviously seven is out of range. But if you rolled two dice, then the total could be seven, which then means that one is out of range because the lowest total you can roll with two dice is two. Range and data type are just two kinds of constraints we discussed. There's a bunch more that can affect the validity of your data. So you need to ensure that those constraints are met too. Moving on data has to match a source of truth, right? That's what makes it accurate. Say you're in the United States and you're presented with this list. Washington, Alaska, California, hot dog, Florida and Texas. And you're like, wait a minute, I don't remember a US state called hot dog because hot dog is a string. So it's a valid data type, but it won't match any value in a known source of truth. There's a set amount of known US States 50 and hot dog is not in that list. Even a good source of truth could have different values in it. For example, some would choose to include overseas territories like Guam or Puerto Rico, while others might list only states. And so it depends on your source of truth, your standards of accuracy, and even your definitions of terms. Accuracy is very hard to achieve through data cleansing in general because it requires accessing an external source of data that contains only true values and that kind of gold standard data is often unavailable. Now take a look at these two images. Unless you're an expert traveler or know the area, it may be hard to tell where we're looking at from just these two images. Visually, it's pretty easy to tell when we aren't seeing the whole picture. Sometimes you need to question the source of the data to get the big picture and once you get a complete picture of the data, you can easily determine that this is in London. With incomplete data, it is hard to get a real sense of what is going on. We inherently trust data because if it comes to us in row and column format, we somehow believe that everything we need is in there if we just query it, sometimes that's not true. Consequences of incomplete data can include insights that are over-dependent on the small segment of data, like the out-of-context lamp and clock land from the previous frame, or biased results, or interpreting patterns that are not there, which we call overfitting in machine learning models. Next, we have consistency. The example here introduces a dilemma where there are potentially two owners for a house. Inconsistency occurs when two data items in the dataset contradict each other. For example, a house address is recorded in two different systems as having two different owners, and only one of them can be correct. In the first table, the house at 123 ABC Street has an owner ID of 12. But in the second table, owner ID 12 is associated with a different address, and 123 ABC Street is associated with an owner ID of 15. In database 101, we call this a referential integrity issue.

Consistent data ensures harmony across many different systems so you don't end up in situations like this. Fixing inconsistency is not always possible. It requires a variety of strategies. For example, deciding which data was recorded more recently, which data source is likely to be most reliable, or simply trying to find the truth by testing both data items, for example, calling up the customer. Last but not least, there has to be uniformity in data, which means using the same units of measure in all systems. To illustrate just how important that is, listen to this. On November 10th, 1999, NASA unfortunately lost its $125,000,000 Mars climate orbiter as it burned into the surface of Mars. This was because of an issue with one team using the imperial measurement system of feet and yards. And another team that was working on a different part of this system using the metric system. Getting your data to that caliber of quality is important for the reasons we just discussed. The examples we used also highlighted some of the reasons and challenges affecting your data quality. For example, your data could have competing or out-of-range data types or there could be duplicates to deal with. In the next lesson, we focus on how to clean, prepare, and transform data with SQL to get a high-quality dataset. But before we get into that, let's talk a little about dataset shape and skew. Datasets usually come from different sources and can be in a variety of different forms or shapes. Sometimes you can have a lot of columns, but not that many rows to analyze, we call that a wide but short dataset. In some cases this may work, in other cases, maybe you could try asking for more data or asking for data at a different level of detail. Just to the left of that, we see a dataset that's taller than wide, meaning that you have a lot of observations but only a few columns even as few as two or three. Sometimes that may be enough and sometimes you need more than just those columns. Again, it depends on your analytical need. Sometimes you just have a small dataset like you see on the very left here. You have an average number of columns and an average number of rows, and you know that's all you've got. Of course, the perfect case scenario is you have whatever the right amount of columns is for you and enough records to make judgments and inferences from your data and insights. Again, there's no magic number. You need to make sure that there are actually enough observations to make a justifiable conclusion about your records. This is especially true if you plan to use the data for machine learning, But the same is true for data analysis. The other important attribute of the dataset is the skew or the distribution of values. For example, you may have an employee table, and notice that there are way more people in the California office compared to every other office location. Or maybe the dataset is centered around one value. For example, you have an international business, but the data for sales orders is only showing one country. Then you start to question the validity of the data. Did we collect everything or did we miss some rows? Does it correspond to what you were expecting? Are there clear outliers or anomalies in your data and how would you analyze this in SQL. Group and order by results will be rather skewed to so skew is something that you want to watch out for from an exploration standpoint. But skewed data is not necessarily a bad thing. The important thing with skew is considering if the skew is representative of the objective facts or is an artifact of bad data, like missing the sales data outside of a given area.

### Clean and transform data using SQL

In this lesson, we revisit each of the five data integrity rules we covered in the previous lesson and show how you can use SQL to clean, prepare and transform your dataset to achieve a high level of quality. Let's start with validity, meaning that your data needs to conform to business rules and constraints. You can do a lot of things directly within SQL and BigQuery to make this happen. You can specify that a column is allowed to be null, or maybe it's a required column like name or ID fields that are unique identifiers for rows and so they should not be null. This can be done when you create new tables so you can ensure that incoming rows from upstream systems will be ingested if they have valid data. Alternatively, you can proactively check whether or not those values are valid after the fact. There's a lot of conditional logic, like the if then else and the case when flows that you can apply in SQL to test for data validation. Now, even when your data matches the data types and it meets all of your specifications about whether it's allowed to be null or it's required, you still want to make sure if your data is actually accurate and makes sense. Sometimes you can use calculated fields to set up a test case to check for values, or potentially you can look it up against an objective reference data set, like in the case of the 50 states we discussed in the previous lesson. Joins can be one way of setting this up. Another powerful SQL construct that you can explore is using subqueries or CTEs, and testing the occurrence of values with the in operator. Completeness can be a hard one. Completeness is exploring the data to see if something's missing. This means checking for the SKU, as we mentioned before, and looking for those null values and handling those null values. And saying if it's null, do this, or if this value isn't present, roll back on this value, which is what the coalesce function can help with. You can further enrich your data set with other datasets and mash them up together to form a more complete picture. We saw some examples of this when we talked about unions and joins, where we combined multiyear data by doing unions on the various temperature reading tables by year. And adding weather station information using joins, thus getting a bigger and better picture of what the reading says and where it was collected, all available in one result. Depending upon what kind of dirty data or inconsistency you're dealing with, there's a whole host of functions to choose from. Parsing out dates, cleaning up a lot of dates is a common step many data analysts end up doing to clean their data and make it consistent. String functions like substring and replace can be very helpful and very easily, efficiently resolve inconsistencies when sourcing from a number of tables. And it doesn't necessarily mean you have to replace your raw data table. All these steps are going to be parts of a recipe that continue on after or at downstream. So this logic is repeatable when new dirty data comes into your system. Last but not least, uniformity. You want to make sure that all your data types are similar and that you're comparing apples to apples, especially when it comes to visualizing your results. So always document and come at your code. For example, highlight whether you are using the metric or imperial system for measurements. Functions like format and cast that we discussed earlier can be very useful here to standardize formats across the board. Different time zones are another common issue in imported data. Functions like date time and date time all accept a time zone parameter, so you can easily convert to a standard time zone. And lastly, there are functions available to convert Unix APOC time values to a timestamp format for uniformity. Okay, here's an interesting example. We've got some weather data that we're pulling from Noa, and we're basically saying give us all the weather data where the US state is not null, meaning that it is present and limit the top ten results. So the state has to be present. But now the question is why does the query still show these blank state values when it's clearly filtered on is not null? And the correct answer is it's because it's not null. It's not null because it's blank string. You can't see it normally. Remember, an empty string or blank string is a valid data value, whereas a null is the absence of any data. A valid null value in BigQuery looks exactly like what these latitude and longitude columns look like. It will say nul there. Well then how could we adjust this query to filter out blanks as well, you would have to update your where clause, like how it is shown in the slide, where you also filter out blank strings in addition to NULL values.

### Clean and transform data : other options

In this lesson, we will highlight other options besides SQL that are available for you to use to clean, prepare, and transform your data. More specifically, we will talk about Cloud data fusion and its Wrangler tool that helps to create many useful transformations. Next, we will briefly introduce Dataflow where you can code powerful transformations to be applied to your data. Dataprep is yet another UI based tool that data analysts use to do data cleansing and preparation. Then we briefly look at Dataproc, which could be an option for your ETL workloads. There is also the recently launched data form, which is available through the big query console, which we will cover in a later module. We conclude this lesson with recommendations on when to use which product. Cloud Data Fusion is a fully managed Cloud native enterprise data integration service for quickly building and managing data pipelines. It is essentially a graphical no code tool to help business users, developers, data analysts, and scientists to quickly and easily build, deploy, and manage data integration pipelines. It is a great UI based tool to consider when not all data can be exported to Excel and sheets or you have inconsistent metadata and no business glossary or operationalizing the Data preparation is a challenge. Wrangler is a powerful tool in Cloud data fusion that helps you view, explore, and create data transformations on your data. It allows you to transform a small sample, 10 megabytes of your data in one place before running the logic on the entire dataset. This means you can quickly apply transformations to gain an understanding of how they will affect the entire dataset. Wrangler provides an easy and interactive way to visualize, transform, and cleanse data that allows the user to view data from multiple locations, such as Cloud storage or Kafka, as well as from databases and derive new schemas and operationalize the Data preparation with a few clicks. Wrangler allows you to connect to your data wherever it resides and transform it using simple point and click transformation steps, you can create multiple transformations and add them to a recipe. When you are satisfied with the results of your recipe, you can create a data pipeline that includes the source and the Wrangler transformation. In the UI, you can add more plug ins to continue transforming your data and at a sink to write the transformed data to a target location, like Bakery. Dataflow is a Google Cloud service that provides unified stream and batch data processing at scale. In this case, processing refers to the steps to extract, transform, and load data, or ETO. Dataflow uses a data pipeline model where data moves through a series of stages. Stages can include reading data from a source, transforming and aggregating the data, and writing the results to a destination. Pipelines can range from very simple to more complex processing. For example, a pipeline might move data as is to a destination. Transform data to be more usable by the target system. Aggregate, process, and enrich data for analysis. Joining data with other data, Dataflow is based on the open source Apache Beam Project. Apache Beam is an open source unified model for defining both batch and streaming pipelines. The Apache Beam Programming Model simplifies the mechanics of large scale Dataprocessing. Using one of the Apache Beam SDKs, you build a program that defines the pipeline. Then you execute the pipeline on a specific platform such as Dataflow. This model let you concentrate on the logical composition of your Dataprocessing job rather than managing the orchestration of parallel processing. Apache Beam insulates you from the low level details of distributed processing, such as coordinating individual workers, charting datasets, and other such tasks. Dataflow fully manages these low level details. A pipeline can be run locally on your computer or remotely on a virtual machine, or by using another service in the Cloud. The engine that will be powering your pipeline is specified by the runner. Each runner has its own configuration and an associated back-end service. Dataflow is one of the runners available in Apache Beam. Dataprep is an integrated partner service that is operated by another company, Trifacta. Dataprep is built on Dataflow, which means it is auto scalable to any size and can easily process massive datasets. This also means it is fully integrated with Google Cloud. There is no infrastructure to deploy or manage. They can access raw data from big query Cloud storage or local files. The data can be in CSV, JSON, or a relational table format. Refined data can be directly ported into BigQuery, Looker Studio, Cloud Storage, or Vertex AI for further analysis and storage. When it comes down to executing the transformations, Dataprep easily scales to handle any data big or small by using the power of Cloud Dataflow to execute efficiently and in any region supported by Google Cloud. With your refined data, you can load it into BigQuery and perform interactive queries. Visualize it with Looker Studio or load it into Cloud storage to train machine learning models with Google Cloud ML service. Here are common use cases where customers have found success using Cloud Dataprep. Data onboarding. Analysts need to integrate external or third party information into their work, but need to ensure that it conforms to internal structures and formats so that it is compatible with existing systems. Data science and machine learning. Preparing data is not only a challenge for analytics, but also for data science and machine learning projects. You need clean, well structured data to effectively train models, marketing analytics. As Google Cloud users have access to tons of marketing data, gaining insight often involves combining and enriching the data with a range of datasets from cross functional sources like sales, finance, or support. This can involve manipulating those datasets into forms that can be joined, aggregated, and cleansed. Dataprep is a graphical user interface, gooey for creating data transformation steps. You can pick from predefined wranglers to process your data. You can find more information at cloud.google.com/dataprep. Review the final Dataflow, and then run the job, which will then kick off a new Dataflow job behind the scenes. Use dashboards to monitor your jobs and dig into the metrics for your transformed data sets. Finally, let's briefly talk about Dataproc. Dataproc is a fully managed and highly scalable service for running Apache Hadoop, Apache Spark, Apache Flank, Presto, and 30 plus open source tools and frameworks. Use Dataproc for data lake modernization, ETL, and secure data science at scale. Integrated with Google Cloud at a fraction of the cost. With Dataproc, enterprises get a fully managed purpose built cluster that can auto scale to support any data or analytics processing job Dataproc has built in integration with other Google Cloud Platform services such as BigQuery Cloud Storage, Cloud Big table Cloud logging, and Cloud monitoring. So you have more than just a Spark or Hadoop cluster, you have a complete data platform. For example, you can use Dataproc to effortlessly ETL terabytes of raw log data directly into BigQuery for business reporting. Now that we have discussed a little bit about each of these products, here's a summary Cloud Data Fusion and its Wrangler tool and Dataprep come with a graphical user interface that can help with the wrangling activities. With Dataflow, you have more flexibility because you can code all your transformations, but that means that you will need to do quite a bit of coding. Dataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Data form helps you develop and operationalize scalable data transformation pipelines and SQL, and we will cover that in a later module. The intention in this lesson was to highlight other options besides SQL that are available for you to use to clean, prepare, and transform your data. For more details on using each of the products, feel free to refer to our product documentation site.

## Ingesting and Storing New BigQuery Datasets

### Module overview

Welcome to Module 4, ingesting and storing new BigQuery datasets. One of the building blocks of data analysis is creating SQL queries on raw data sources, and then saving your results into new reporting tables that you can then query from. In the first lesson, we'll cover the difference between permanent and temporary data tables, and how to store the results of your queries. So far, all the queries we have executed have been on tables or sources that have been previously created and shared via the course dataset or via BigQuery public datasets. In the second lesson, we will look at how to create new datasets in tables, and ingest data into them. Finally, we also cover external data sources where you can run your query in BigQuery, but the data is hosted elsewhere.

### Permanent versus temporary data tables

Let's start with permanent versus temporary tables and BigQuery. When you run queries in BigQuery, the query results are always saved either to a permanent table, which you can specify or a temporary table, even if you don't explicitly create one. Many times data analysts run SQL queries on raw data sources and then save the results into new reporting tables for further analysis in the future. Before you run the query, you can specify destination table and choose whether to append or overwrite data in an existing table or to create a new table if none exists by that name. Now, if you forget to specify a destination table before running your query, the results get copied into a temporary table that is available for 24 hours. This also means that you can also copy the temporary table to a permanent table by clicking the "Save Results" button in the results window and choose BigQuery Table. Recall the discussion on query cache from an earlier module, that is basically your temporary table. Same queries in the near future will use cached results when available assuming. The underlying table has not been modified and no current date or similar non deterministic function is used in the query. A view is a virtual table that's defined by a SQL query. Think of it as a save SQL query. When you select from a view, you are essentially selecting from a select statement, which in turn selects from a table. This means it does not store the results. Later we'll talk about materialized views where the results can be stored. You can query views in big query using the web UI, the command line tool, or the API. You can also use a view as a data source for a visualization tool such as Looker or Looker Studio. Essentially, it is pretty much like a table because views are not materialized, the query that defines the view is run each time the view is queried, queries are billed according to the total amount of data in all table fields that are referenced directly or indirectly by the top level query. You can also create the view by using the create statement, which is a DDL or data definition language. The create view statement specifies that this query is to be saved as a view. You can use the OR REPLACE keyword phrase to overwrite the view query if one already exists with the name you specified. Alternatively, you can use the IF NOT EXISTS keyword phrase to create the view only if that name is not already used. So no overwriting any existing view with that name. This discussion would not be complete without showing you the DDL syntax to create a new table schema. This will create an empty table in the specified dataset using the specified name with the corresponding columns and data types and in the future, you can use the Insert statement, which is a DML or data manipulation language to insert rows into the table. You can also use a low job from the command line to populate the table with data coming from a CSV file. For instance, in BigQuery, materialized views are pre-computed views that periodically cache the results of a query for increased performance and efficiency. BigQuery leverages pre-computed results from materialized views and whenever possible reads only data changes from the base table to compute up to date results. Materialized views can be queried directly or can be used by the BigQuery optimizer to process queries to the base tables. Queries that use materialized views are generally faster and consume fewer resources than queries that retrieve the same data only from the base table. Materialized views can significantly improve the performance of workloads that have the characteristic of common and repeated queries. In the example on the slide, step 1 shows how a materialized view is created and automatically kept fresh. Step 2 shows queries still going to the base table. Note, you can explicitly query the materialized view as well. Step 3 shows queries being executed against the materialized view for faster and more efficient execution. Here are a few things to know about materialized views. You can grant access to a materialized view at the dataset level, the view level or the column level. You can also set access at a higher level in the IAM resource hierarchy. You can manually refresh a materialized view at any time. You can partition a materialized view if the base table is partitioned as well. By default, materialized views are automatically refreshed within five minutes of a change to the base table, but no more frequently than every 30 minutes.
